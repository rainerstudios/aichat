Directory structure:
â””â”€â”€ langchain-ai-chat-langchain/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ CONCEPTS.md
    â”œâ”€â”€ DEPLOYMENT.md
    â”œâ”€â”€ langgraph.json
    â”œâ”€â”€ LANGSMITH.md
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ Makefile
    â”œâ”€â”€ MODIFY.md
    â”œâ”€â”€ Procfile
    â”œâ”€â”€ PRODUCTION.md
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ .dockerignore
    â”œâ”€â”€ .env.gcp.yaml.example
    â”œâ”€â”€ _scripts/
    â”‚   â”œâ”€â”€ clear_index.py
    â”‚   â”œâ”€â”€ evaluate_chains.py
    â”‚   â”œâ”€â”€ evaluate_chains_agent.py
    â”‚   â”œâ”€â”€ evaluate_chains_improved_chain.py
    â”‚   â””â”€â”€ evaluate_chat_langchain.py
    â”œâ”€â”€ backend/
    â”‚   â”œâ”€â”€ configuration.py
    â”‚   â”œâ”€â”€ constants.py
    â”‚   â”œâ”€â”€ embeddings.py
    â”‚   â”œâ”€â”€ ingest.py
    â”‚   â”œâ”€â”€ parser.py
    â”‚   â”œâ”€â”€ retrieval.py
    â”‚   â”œâ”€â”€ utils.py
    â”‚   â”œâ”€â”€ retrieval_graph/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ configuration.py
    â”‚   â”‚   â”œâ”€â”€ graph.py
    â”‚   â”‚   â”œâ”€â”€ prompts.py
    â”‚   â”‚   â”œâ”€â”€ state.py
    â”‚   â”‚   â””â”€â”€ researcher_graph/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”œâ”€â”€ graph.py
    â”‚   â”‚       â””â”€â”€ state.py
    â”‚   â””â”€â”€ tests/
    â”‚       â””â”€â”€ evals/
    â”‚           â””â”€â”€ test_e2e.py
    â”œâ”€â”€ frontend/
    â”‚   â”œâ”€â”€ components.json
    â”‚   â”œâ”€â”€ next.config.js
    â”‚   â”œâ”€â”€ package.json
    â”‚   â”œâ”€â”€ postcss.config.js
    â”‚   â”œâ”€â”€ tailwind.config.ts
    â”‚   â”œâ”€â”€ tsconfig.json
    â”‚   â”œâ”€â”€ vercel.json
    â”‚   â”œâ”€â”€ .env.example
    â”‚   â”œâ”€â”€ .eslintrc.json
    â”‚   â”œâ”€â”€ .prettierrc
    â”‚   â”œâ”€â”€ .yarnrc.yml
    â”‚   â””â”€â”€ app/
    â”‚       â”œâ”€â”€ globals.css
    â”‚       â”œâ”€â”€ layout.tsx
    â”‚       â”œâ”€â”€ page.tsx
    â”‚       â”œâ”€â”€ types.ts
    â”‚       â”œâ”€â”€ api/
    â”‚       â”‚   â”œâ”€â”€ [..._path]/
    â”‚       â”‚   â”‚   â””â”€â”€ route.ts
    â”‚       â”‚   â””â”€â”€ runs/
    â”‚       â”‚       â”œâ”€â”€ feedback/
    â”‚       â”‚       â”‚   â””â”€â”€ route.ts
    â”‚       â”‚       â””â”€â”€ share/
    â”‚       â”‚           â””â”€â”€ route.ts
    â”‚       â”œâ”€â”€ components/
    â”‚       â”‚   â”œâ”€â”€ AnswerHeaderToolUI.tsx
    â”‚       â”‚   â”œâ”€â”€ ChatLangChain.tsx
    â”‚       â”‚   â”œâ”€â”€ DocumentCard.tsx
    â”‚       â”‚   â”œâ”€â”€ DocumentDialog.tsx
    â”‚       â”‚   â”œâ”€â”€ GeneratingQuestionsToolUI.tsx
    â”‚       â”‚   â”œâ”€â”€ LangSmithLinkToolUI.tsx
    â”‚       â”‚   â”œâ”€â”€ ProgressToolUI.tsx
    â”‚       â”‚   â”œâ”€â”€ RouterLogicToolUI.tsx
    â”‚       â”‚   â”œâ”€â”€ SelectedDocumentsToolUI.tsx
    â”‚       â”‚   â”œâ”€â”€ SelectModel.tsx
    â”‚       â”‚   â”œâ”€â”€ SuggestedQuestions.tsx
    â”‚       â”‚   â”œâ”€â”€ chat-interface/
    â”‚       â”‚   â”‚   â”œâ”€â”€ chat-composer.tsx
    â”‚       â”‚   â”‚   â”œâ”€â”€ index.tsx
    â”‚       â”‚   â”‚   â””â”€â”€ messages.tsx
    â”‚       â”‚   â”œâ”€â”€ icons/
    â”‚       â”‚   â”‚   â””â”€â”€ langsmith.tsx
    â”‚       â”‚   â”œâ”€â”€ thread-history/
    â”‚       â”‚   â”‚   â”œâ”€â”€ index.tsx
    â”‚       â”‚   â”‚   â”œâ”€â”€ thread-item.tsx
    â”‚       â”‚   â”‚   â”œâ”€â”€ thread-list.tsx
    â”‚       â”‚   â”‚   â””â”€â”€ utils.ts
    â”‚       â”‚   â””â”€â”€ ui/
    â”‚       â”‚       â”œâ”€â”€ avatar.tsx
    â”‚       â”‚       â”œâ”€â”€ button.tsx
    â”‚       â”‚       â”œâ”€â”€ card.tsx
    â”‚       â”‚       â”œâ”€â”€ carousel.tsx
    â”‚       â”‚       â”œâ”€â”€ collapsible.tsx
    â”‚       â”‚       â”œâ”€â”€ dialog.tsx
    â”‚       â”‚       â”œâ”€â”€ dropdown-menu.tsx
    â”‚       â”‚       â”œâ”€â”€ input.tsx
    â”‚       â”‚       â”œâ”€â”€ progress.tsx
    â”‚       â”‚       â”œâ”€â”€ select.tsx
    â”‚       â”‚       â”œâ”€â”€ sheet.tsx
    â”‚       â”‚       â”œâ”€â”€ skeleton.tsx
    â”‚       â”‚       â”œâ”€â”€ textarea.tsx
    â”‚       â”‚       â”œâ”€â”€ toast.tsx
    â”‚       â”‚       â”œâ”€â”€ toaster.tsx
    â”‚       â”‚       â”œâ”€â”€ tooltip.tsx
    â”‚       â”‚       â””â”€â”€ assistant-ui/
    â”‚       â”‚           â”œâ”€â”€ markdown-text.tsx
    â”‚       â”‚           â”œâ”€â”€ syntax-highlighter.tsx
    â”‚       â”‚           â””â”€â”€ tooltip-icon-button.tsx
    â”‚       â”œâ”€â”€ contexts/
    â”‚       â”‚   â”œâ”€â”€ GraphContext.tsx
    â”‚       â”‚   â””â”€â”€ utils.ts
    â”‚       â”œâ”€â”€ hooks/
    â”‚       â”‚   â”œâ”€â”€ use-toast.ts
    â”‚       â”‚   â”œâ”€â”€ useRuns.tsx
    â”‚       â”‚   â”œâ”€â”€ useThreads.tsx
    â”‚       â”‚   â””â”€â”€ useUser.tsx
    â”‚       â””â”€â”€ utils/
    â”‚           â”œâ”€â”€ cn.ts
    â”‚           â”œâ”€â”€ constants.tsx
    â”‚           â”œâ”€â”€ convert_messages.ts
    â”‚           â”œâ”€â”€ cookies.ts
    â”‚           â””â”€â”€ dummy.ts
    â”œâ”€â”€ terraform/
    â”‚   â”œâ”€â”€ backend.tf
    â”‚   â”œâ”€â”€ main.tf
    â”‚   â””â”€â”€ modules/
    â”‚       â””â”€â”€ chat_langchain_backend/
    â”‚           â”œâ”€â”€ main.tf
    â”‚           â””â”€â”€ variables.tf
    â””â”€â”€ .github/
        â”œâ”€â”€ dependabot.yml
        â”œâ”€â”€ actions/
        â”‚   â””â”€â”€ poetry_setup/
        â”‚       â””â”€â”€ action.yml
        â””â”€â”€ workflows/
            â”œâ”€â”€ clear-and-update-index.yml
            â”œâ”€â”€ deploy-cloud-run.yaml
            â”œâ”€â”€ eval.yml
            â”œâ”€â”€ lint.yml
            â””â”€â”€ update-index.yml

================================================
FILE: README.md
================================================
# ðŸ¦œï¸ðŸ”— Chat LangChain

This repo is an implementation of a chatbot specifically focused on question answering over the [LangChain documentation](https://python.langchain.com/).
Built with [LangChain](https://github.com/langchain-ai/langchain/), [LangGraph](https://github.com/langchain-ai/langgraph/), and [Next.js](https://nextjs.org).

Deployed version: [chat.langchain.com](https://chat.langchain.com)

> Looking for the JS version? Click [here](https://github.com/langchain-ai/chat-langchainjs).

The app leverages LangChain and LangGraph's streaming support and async API to update the page in real time for multiple users.

## Running locally

This project is now deployed using [LangGraph Cloud](https://langchain-ai.github.io/langgraph/cloud/), which means you won't be able to run it locally (or without a LangGraph Cloud account). If you want to run it WITHOUT LangGraph Cloud, please use the code and documentation from this [branch](https://github.com/langchain-ai/chat-langchain/tree/langserve).

> [!NOTE]
> This [branch](https://github.com/langchain-ai/chat-langchain/tree/langserve) **does not** have the same set of features.

## ðŸ“š Technical description

There are two components: ingestion and question-answering.

Ingestion has the following steps:

1. Pull html from documentation site as well as the Github Codebase
2. Load html with LangChain's [RecursiveURLLoader](https://python.langchain.com/docs/integrations/document_loaders/recursive_url) and [SitemapLoader](https://python.langchain.com/docs/integrations/document_loaders/sitemap)
3. Split documents with LangChain's [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)
4. Create a vectorstore of embeddings, using LangChain's [Weaviate vectorstore wrapper](https://python.langchain.com/docs/integrations/vectorstores/weaviate) (with OpenAI's embeddings).

Question-Answering has the following steps:

1. Given the chat history and new user input, determine what a standalone question would be using an LLM.
2. Given that standalone question, look up relevant documents from the vectorstore.
3. Pass the standalone question and relevant documents to the model to generate and stream the final answer.
4. Generate a trace URL for the current chat session, as well as the endpoint to collect feedback.

## Documentation

Looking to use or modify this Use Case Accelerant for your own needs? We've added a few docs to aid with this:

- **[Concepts](./CONCEPTS.md)**: A conceptual overview of the different components of Chat LangChain. Goes over features like ingestion, vector stores, query analysis, etc.
- **[Modify](./MODIFY.md)**: A guide on how to modify Chat LangChain for your own needs. Covers the frontend, backend and everything in between.
- **[LangSmith](./LANGSMITH.md)**: A guide on adding robustness to your application using LangSmith. Covers observability, evaluations, and feedback.
- **[Production](./PRODUCTION.md)**: Documentation on preparing your application for production usage. Explains different security considerations, and more.
- **[Deployment](./DEPLOYMENT.md)**: How to deploy your application to production. Covers setting up production databases, deploying the frontend, and more.



================================================
FILE: CONCEPTS.md
================================================
# Concepts

In this doc we'll go over the different concepts that are implemented in Chat LangChain.
By the end, you'll have a conceptual understanding of how Chat LangChain works, and it's different architectural components.
We'll start with the vector store, the basis of the entire system.

## Vector Store

Vector stores, fundamentally, are specialized databases designed to efficiently store and manage vectors, which are high-dimensional arrays of numbers. These vectors are not arbitrary; they are the product of sophisticated text embedding models, such as those provided by [OpenAI's `text-embedding`](https://python.langchain.com/docs/integrations/text_embedding) API.

In the context of our application, vector stores play a pivotal role in enhancing the capabilities of our language model. Here's a deeper dive into the process:

1. **Vector Generation**: Whenever new content related to LangChain is introduced or existing content is updated, we use text embedding models to convert this textual information into vectors. Each vector acts as a unique fingerprint of its corresponding text, encapsulating its meaning in a high-dimensional space.

2. **Similarity Searches**: The core utility of storing these vectors comes into play when we need to find information relevant to a user's query. By converting the user's question into a vector using the same embedding model, we can perform a similarity search across our vector store. This search identifies vectors (and thus, documents) whose meanings are closest to the query, based on the distance between vectors in the embedding space.

3. **Context Retrieval and Enhancement**: The documents retrieved through similarity searches are relevant pieces of information that aid the language model in generating relevant answers. By providing this context, we enable the language model to generate responses that are not only accurate but also informed by the most relevant and up-to-date information available in our database.

## Indexing

Indexing your documents is a vital part of any production RAG application. In short, indexing allows for your documents to be stored, and searchable to prevent duplicate documents from being stored. This is important for a few reasons:

1. **Duplicate Results**: Say you update your vector store without using an Indexing API. Now you may have two identical documents in your store. Then, when you perform a semantic search, instead of getting K number of different results, you'll get duplicates as the semantic search only returns documents which are semantically close to the query.

2. **Performance**: Indexing your documents allows for faster ingestion. With indexing you don't have to generate embeddings for every document on ingestion, and instead only need to generate embeddings for new documents.

In order to help with indexing we use the LangChain indexing API. This API contains all the features required for robust indexing in your application. Indexing is done in two main steps:

1. **Ingestion**: Ingestion is where you pull in all the documents you want to add to your vector store. This could be all of the documents available to you, or just a couple new documents.
2. **Hashing**: Once the ingestion API is passed your documents, it creates a unique hash for each, containing some metadata like the date it was ingested. This allows for the indexing API to only ingest new documents, and not duplicate documents. These hashes are stored in what we call the "Record Manager".
3. **Insertion**: Finally, once the documents are hashed, and confirmed to not already exist through the Record Manager, they are inserted into the vector store.

The indexing API also uses a Record Manager to store the records of previously indexed documents in between ingestion. This manager stores the hashed values of the documents, and the time they were ingested. This allows for the indexing API to only ingest new documents, and not duplicate documents.

### Record Manager

The LangChain Record Manager API provides an interface for managing records in a database that tracks upserted documents before they are ingested into a vector store for LLM usage.
It allows you to efficiently insert, update, delete, and query records.

**Key Concepts**
- **Namespace**: Each Record Manager is associated with a namespace. This allows logically separating records for different use cases.
- **Keys**: Each record is uniquely identified by a key within the namespace.
- **Group IDs**: Records can optionally be associated with group IDs to allow filtering and batch operations on related records.
- **Timestamps**: Each record has an updated_at timestamp tracking the last time it was upserted. This enables querying records within time ranges.

Using the LangChain Record Manager API allows efficient tracking of which documents need to be added to or updated in the vector store, making the ingestion process more robust and avoiding unnecessary duplication work.

## Query Analysis

Finally, we perform query analysis on followup chat conversations. It is important to note that we only do this for followups, and not initial questions. Let's break down the reasoning here:

Users are not always the best prompters, and can very easily miss some context or phrase their question poorly. We can be confident that the LLM will not make this mistake.
Additionally, given a chat history (which is always passed in context to a model) you may not need to include certain parts of the question, or the reverse, where you do need to clarify additional information.

Doing all this helps make better formed questions for the model, without having to rely on the user to do so.

Lastly, we don't perform this on the initial question for two main reasons:

1. **Speed**: Although models are getting faster and faster, they still take longer than we'd like to return a response. This is even more important for the first question, as the chat bot hasn't proved its usefulness to the user yet, and you don't want to lose them due to speed before they've even started.
2. **Context**: Without a chat history, the model is lacking some important context around the users question.

Most users won't format their queries perfectly for LLMs, and that's okay!
To account for this, we have an extra step before final generation which takes the users query and rephrase it to be more suitable for the LLM.

The prompt is quite simple:
```python
REPHRASE_TEMPLATE = """\
Given the following conversation and a follow up question, rephrase the follow up \
question to be a standalone question.

Chat History:
{chat_history}
Follow Up Input: {question}
Standalone Question:"""
```

In doing this, the language model is able to take the users question, and the full chat history which contains other questions, answers and context, and generate a more well formed response. Now using this rephrased question, we can perform a similarity search on the vector store using this question, and often times get back better results as the question is semantically more similar to the previous questions/answers (content the database).


================================================
FILE: DEPLOYMENT.md
================================================
# Deployment

We recommend when deploying Chat LangChain, you use Vercel for the frontend, [LangGraph Cloud](https://langchain-ai.github.io/langgraph/cloud/) for the backend API, and GitHub action for the recurring ingestion tasks. This setup provides a simple and effective way to deploy and manage your application.

## Prerequisites

First, fork [chat-langchain](https://github.com/langchain-ai/chat-langchain) to your GitHub account.

## Weaviate (Vector Store)

We'll use Weaviate for our vector store. You can sign up for an account [here](https://console.weaviate.cloud/).

After creating an account click "Create Cluster". Follow the steps to create a new cluster. Once finished wait for the cluster to create, this may take a few minutes.

Once your cluster has been created you should see a few sections on the page. The first is the cluster URL. Save this as your `WEAVIATE_URL` environment variable.

Next, click "API Keys" and save the API key in the environment variable `WEAVIATE_API_KEY`.

The final Weaviate environment variable is "WEAVIATE_INDEX_NAME". This is the name of the index you want to use. You can name it whatever you want, but for this example, we'll use "langchain".

After this your vector store will be setup. We can now move onto the record manager.

## Supabase (Record Manager)

Visit Supabase to create an account [here](https://supabase.com/dashboard).

Once you've created an account, click "New project" on the dashboard page.
Follow the steps, saving the database password after creating it, we'll need this later.

Once your project is setup (this also takes a few minutes), navigate to the "Settings" tab, then select "Database" under "Configuration".

Here, you should see a "Connection string" section. Copy this string, and insert your database password you saved earlier. This is your `RECORD_MANAGER_DB_URL` environment variable.

That's all you need to do for the record manager. The LangChain RecordManager API will handle creating tables for you.

## Vercel (Frontend)

First, build the frontend and confirm it's working locally:

```shell
cd frontend
yarn
yarn build
```

Then, create a Vercel account for hosting [here](https://vercel.com/signup).

Once you've created your Vercel account, navigate to [your dashboard](https://vercel.com/) and click the button "Add New..." in the top right.
This will open a dropdown. From there select "Project".

On the next screen, search for "chat-langchain" (if you did not modify the repo name when forking). Once shown, click "Import".

Finally, click "Deploy" and your frontend will be deployed!

## GitHub Action (Recurring Ingestion)

Now, in order for your vector store to be updated with new data, you'll need to setup a recurring ingestion task (this will also populate the vector store for the first time).

Go to your forked repository, and navigate to the "Settings" tab.

Select "Environments" from the left-hand menu, and click "New environment". Enter the name "Indexing" and click "Configure environment".

When configuring, click "Add secret" and add the following secrets:

```
OPENAI_API_KEY=
RECORD_MANAGER_DB_URL=
WEAVIATE_API_KEY=
WEAVIATE_INDEX_NAME=langchain
WEAVIATE_URL=
```

These should be the same secrets as were added to Vercel.

Next, navigate to the "Actions" tab and confirm you understand your workflows, and enable them.

Then, click on the "Update index" workflow, and click "Enable workflow". Finally, click on the "Run workflow" dropdown and click "Run workflow".

Once this has finished you can visit your production URL from Vercel, and start using the app!

## Run and deploy backend API server

If you have a valid LangGraph Cloud [license key](https://langchain-ai.github.io/langgraph/cloud/deployment/self_hosted/), you can run a fully functional LangGraph server locally with a `langgraph up` command. Otherwise, you can use `langgraph test` to test that the API server is functional.

> [!NOTE]
> When running `langgraph test`, you will only be able to [create stateless runs](https://langchain-ai.github.io/langgraph/cloud/how-tos/cloud_examples/stateless_runs/), and the previous chats functionality will not be available.

Once you confirm that the server is working locally, you can deploy your app with [LangGraph Cloud](https://langchain-ai.github.io/langgraph/cloud/).

## Connect to the backend API (LangGraph Cloud)

In Vercel add the following environment variables:
- `API_BASE_URL` that matches your LangGraph Cloud deployment API URL
- `NEXT_PUBLIC_API_URL` - API URL that LangGraph Cloud deployment is proxied to, e.g. "https://chat.langchain.com/api"
- `LANGCHAIN_API_KEY` - LangSmith API key


================================================
FILE: langgraph.json
================================================
{
  "dependencies": ["."],
  "graphs": {
    "chat": "./backend/retrieval_graph/graph.py:graph"
  },
  "env": ".env",
  "image_distro": "wolfi"
}



================================================
FILE: LANGSMITH.md
================================================
# LangSmith

Observability and evaluations are pivotal to any LLM application looking to be productionized, and improve beyond initial deployment.
For this, we use LangSmith, a tool that encapsulates all the necessary components to monitor and improve your LLM applications.
In addition to these two development tools, LangSmith also offers a feature for managing feedback from users.
Getting real user feedback can be invaluable for improving your LLM application, based on facts from actual users, and not just assumptions/theories.

## Observability

Observability is simple when using LangChain as your LLM framework. In its simplest form, all you need is to set two environment variables:

```shell
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=...
```

LangSmith tracing is already setup in an optimized way for Chat LangChain, and only needs extra configuration if you're extending the application in a way that's not covered by the default tracing.

You may see this further customization throughout the repo, mainly in the form of adding config names to runs:

```python
.with_config(
    run_name="CondenseQuestion",
)
```

You can call `.with_config` on any [LangChain Runnable](https://python.langchain.com/docs/expression_language/) and apply things like a `run_name` as seen above.

When running queries through Chat LangChain, you can expect to see LangSmith traces like this show up on your project:

![LangSmith Traces](./assets/images/langsmith_trace.png)

For more detailed information on LangSmith traces, visit the [LangSmith documentation](https://docs.smith.langchain.com/tracing/).

## Evaluations

Evals are a great way to discover issues with your LLM app, areas where it does not perform well, and track regression. LangSmith has a whole suite of tools to aid you with this.

For in depth walkthroughs and explanations of LangSmith evaluations, visit the [LangSmith documentation](https://docs.smith.langchain.com/evaluation). This doc will only go over setting up and running evals on Chat LangChain.

### Datasets

For Chat LangChain, the team at LangChain has already put together a dataset for evaluating the app.

You can find the dataset [here](https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d).

The first step is to download the LangSmith node SDK:

```shell
pip install langsmith
```

Then, you'll want to define some custom criteria to evaluate your dataset on. Some examples are:

- **Semantic similarity**: How similar your generated response is to the ground truth (dataset answers).
- **LLM as a judge**: Use an LLM to judge and assign a score to your generated response.

Finally, configure your evaluation criteria and use the [`run_on_dataset`](https://api.python.langchain.com/en/latest/smith/langchain.smith.evaluation.runner_utils.run_on_dataset.html#langchain.smith.evaluation.runner_utils.run_on_dataset) function to evaluate your dataset.

Once completed, you'll be able to view the results of your evaluation in the LangSmith dashboard. Using these results, you can improve and tweak your LLM.

## Feedback

Gathering feedback from users is a great way to gather human curated data on what works, what doesn't and how you can improve your LLM application. LangSmith makes tracking and gathering feedback as easy as pie.

Currently, Chat LangChain supports gathering a simple ðŸ‘ or ðŸ‘Ž, which is then translated into a binary score, and saved to each run in LangSmith. This feedback is then stored in the --you guessed it-- feedback tab of the LangSmith trace:

![LangSmith Feedback](./assets/images/langsmith_feedback.png)

Then, inside LangSmith you can efficiently use this data to visualize and understand your user's feedback, as well as curate datasets by feedback for evaluations.

### Go further

In addition to binary scores for feedback, LangSmith also allows for assigning comments to feedback. This can allow for you to gather more detailed and complex feedback from users, further fueling your human curated dataset for improving your LLM application.


================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2023 Harrison Chase

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: Makefile
================================================
.PHONY: start, format, lint

format:
	poetry run ruff format .
	poetry run ruff --select I --fix .

lint:
	poetry run ruff .
	poetry run ruff format . --diff
	poetry run ruff --select I .




================================================
FILE: MODIFY.md
================================================
# Modifying

The Chat LangChain repo was built to serve two use cases.
The first being question answering over the LangChain documentation.
The second is to offer a production ready chat bot which you can easily customize for your specific use case.
In this doc we'll go over each step you need to take to customize the repo for your need.

## Vector Store

One of the simplest ways to modify Chat LangChain and get a feel for the codebase is to modify the vector store.
All of the operations in Chat LangChain are largely based around the vector store:

- ingestion
- retrieval
- context
- etc

There are two places the vector store is used:
- **Ingestion**: The vector store is used to store the embeddings of every document used as context. Located in [`./backend/ingest.py`](./backend/ingest.py) you can easily modify the provider to use a different vector store.
- **Retrieval**: The vector store is used to retrieve documents based on a user's query. Located at [`./backend/chain.py`](./backend/chain.py) you can easily modify the provider to use a different vector store.

### Steps

For backend ingestion, locate the `ingest_docs` function. You'll want to modify where `client` and `vectorstore` are instantiated. Here's an example of the Weaviate instantiation:

```python
client = weaviate.Client(
    url=WEAVIATE_URL,
    auth_client_secret=weaviate.AuthApiKey(api_key=WEAVIATE_API_KEY),
)
vectorstore = Weaviate(
    client=client,
    index_name=WEAVIATE_DOCS_INDEX_NAME,
    text_key="text",
    embedding=embedding,
    by_text=False,
    attributes=["source", "title"],
)
```

To make transitioning as easy as possible, all you should do is:

1. Delete the weaviate client instantiation.
2. Replace the vector store instantiation with the new provider's instantiation. Remember to keep the variable name (`vectorstore`) the same. Since all LangChain vector stores are built on top of the same API, no other modifications should be necessary.

Finally, perform these same steps inside the [`./backend/chain.py`](./backend/chain.py) (inside the `get_retriever` function) file, and you're done!

## Record Manager

Continuing with the database, we also employ a record manager for ingesting docs.
Currently, we use a `SQLRecordManager`, however you may also swap that out in favor of a `MongoDocumentManager`:

```python
from langchain_community.indexes import MongoDocumentManager

record_manager = MongoDocumentManager(
    namespace="kittens",
    mongodb_url="mongodb://langchain:langchain@localhost:6022/",
    db_name="test_db",
    collection_name="test_collection",
)
record_manager.create_schema()
```

For more conceptual information on Record Managers with LangChain, see the [concepts](./CONCEPTS.md) doc.

## LLM

The LLM is used inside the `/chat` endpoint for generating the final answer, and performing query analysis on followup questions.

> Want to learn more about query analysis? See our comprehensive set of use case docs [here](https://python.langchain.com/docs/use_cases/query_analysis/).

Without any modification, we offer a few LLM providers out of the box:

- `gpt-4o-mini-2024-07-18` by OpenAI
- `claude-3-haiku-20240307` by Anthropic
- `mixtral-8x7b` by Fireworks
- `gemini-pro` by Google
- `llama3-70b-8192` by Groq
- `command` by Cohere

These are all located at the bottom of the [`./backend/chain.py`](./backend/chain.py) file. You have a few options for modifying this:

- Replace all options with a single provider
- Add more providers

First, I'll demonstrate how to replace all options with a single provider, as it's the simplest:

1. Find the LLM variable declaration at the bottom of the file, it looks something like this:

```python
llm = ChatOpenAI(
    model="gpt-4o-mini-2024-07-18",
    streaming=True,
    temperature=0,
).configurable_alternatives(
    # This gives this field an id
    # When configuring the end runnable, we can then use this id to configure this field
    ConfigurableField(id="llm"),
    default_key="openai_gpt_3_5_turbo",
    anthropic_claude_3_haiku=ChatAnthropic(
        model="claude-3-haiku-20240307",
        max_tokens=16384,
        temperature=0,
        anthropic_api_key=os.environ.get("ANTHROPIC_API_KEY", "not_provided"),
    ),
    ...
)
```

You should then remove it, and replace with your LLM class of choice, imported from LangChain. Remember to keep the variable name the same so nothing else in the endpoint breaks:

```python
llm = ChatYourLLM(
    model="model-name",
    streaming=True,
    temperature=0,
).configurable_alternatives(
    # This gives this field an id
    ConfigurableField(id="llm")
)
```

Adding alternatives is also quite simple. Just add another class declaration inside the `configurable_alternatives` method. Here's an example:

```python
.configurable_alternatives(
    local_ollama=ChatCohere(
        model="llama2",
        temperature=0,
    ),
)
```

Next, scroll up to find the `response_synthesizer` variable, and add an entry for `local_ollama` like so:

```python
response_synthesizer = (
    default_response_synthesizer.configurable_alternatives(
        ConfigurableField("llm"),
        default_key="openai_gpt_3_5_turbo",
        anthropic_claude_3_haiku=default_response_synthesizer,
        ...
        local_ollama=default_response_synthesizer,
    )
    | StrOutputParser()
).with_config(run_name="GenerateResponse")
```

That't it!

## Embeddings

Chat LangChain uses embeddings inside the ingestion script when storing documents in the vector store.
Without modification, it defaults to use [OpenAI's embeddings model](https://python.langchain.com/docs/integrations/text_embedding/openai).

Changing this to the vector store of your choice is simple. First, find the `get_embeddings_model` function inside the [`./backend/ingest.py`](./backend/ingest.py) file. It looks something like this:

```python
def get_embeddings_model() -> Embeddings:
    return OpenAIEmbeddings(model="text-embedding-3-small", chunk_size=200)
```

Then, simply swap out the `OpenAIEmbeddings` class for the model of your choice!

Here's an example of what that would look like if you wanted to use Mistral's embeddings model:

```python
from langchain_mistralai import MistralAIEmbeddings

def get_embeddings_model() -> Embeddings:
    return MistralAIEmbeddings(mistral_api_key="your-api-key")
```

## Prompts

### Answer Generation Prompt

The prompt used for answer generation is one of the most important parts of this RAG pipeline. Without a good prompt, the LLM will be unable (or severely limited) to generate good answers.

The prompt is defined in the `RESPONSE_TEMPLATE` variable.

You should modify the parts of this which are LangChain specific to instead fit your needs. If possible, and if your use case does not differ too much from the Chat LangChain use case, you should do your best to keep the same structure as the original prompt. Although there are likely some improvements to be made, we've refined this prompt over many months and lots of user feedback to what we believe to be a well formed prompt.

### Question Rephrasing Prompt

Finally, you can (but not necessary required) modify the `REPHRASE_TEMPLATE` variable to contain more domain specific content about, for example, the types of followup questions you expect to receive. Having a good rephrasing prompt will help the LLM to better understand the user's question and generate a better prompt which will have compounding effects downstream.

## Retrieval

### Ingestion Script

At a high level, the only LangChain specific part of the ingestion script are the three webpages which is scrapes for documents to add to the vector store. These links are:
- LangSmith Documentation
- LangChain API references
- LangChain Documentation

If all you would like to update is update which website(s) to scrape and ingest, you only need to modify/remove these functions:

- `load_langchain_docs`
- `load_langsmith_docs`
- `load_api_docs`

Other than this, the core functionality of the retrieval system is not LangChain specific.

### Retrieval Methods

You can however, easily add or remove parts to increase/fit your needs better.
Some ideas of what can be done:

- Re-ranking document results
- Parent document retrieval (also would require modifications to the ingestion script)
- Document verification via LLM

## Frontend

The frontend doesn't have much LangChain specific code that would need modification.
The main parts are the LangChain UI branding and question suggestions.

To modify the main LangChain branding visit the [`ChatWindow.tsx`](frontend/app/components/ChatWindow.tsx) file to modify/remove.
Next, to update the question suggestions, visit the [`EmptyState.tsx`](frontend/app/components/EmptyState.tsx) file.
Finally, update the "View Source" button on the bottom of the page by going back to the [`ChatWindow.tsx`](frontend/app/components/ChatWindow.tsx) file.


================================================
FILE: Procfile
================================================
# Modify this Procfile to fit your needs
web: uvicorn backend.main:app --host 0.0.0.0 --port 8080


================================================
FILE: PRODUCTION.md
================================================
# Productionizing

This doc will outline steps which are recommended to bring this LLM application to production. 

Bringing LLM applications to production mainly involve preventing/mitigating abuse and guardrails around prompt injection. We'll go over some of your options for these, and suggestions on how to implement them.

> ### We also have in depth security documentation on the main LangChain documentation site. To visit, click [here](https://python.langchain.com/docs/security).

## Abuse

Abuse is a big concern when deploying LLMs to production, mainly due to the cost associated with running the model (or in our case, calling an LLM API). The first and simplest step is to require accounts with some sort of "identity verification". Commonly this is done with an email verification, or for slightly more security, a phone number verification. Adding an account creation step will defer some abuse, and confirming email/phone numbers will make it even harder for users to abuse the system.

However, accounts are not enough to prevent all abuse. The next step is to add rate limiting to your API. This will prevent users from making too many requests in a short period of time. This can be done in a few ways, but the most common is to use a middleware which checks the number of requests a user has made in the last X minutes, often times linked to their account credentials, or if no account is required, then their IP address. If they've made too many, the system can handle it in a variety of ways, often times putting users into a "timeout" for a period of time, or even banning their account/IP if the abuse is severe.

## Prompt Injection

Prompt injection is when a malicious user injects a prompt which tries to make the LLM answer or act in a way you did not originally intend for it to. This sometimes occurs because users want to use the LLM for other reasons than you intended, without having to pay for it themselves. Or the user might be trying to extract information/data they should not have access to, but the LLM does.

The first step you should always do is scoping any permissions your LLM has (eg, database permissions) as tightly as possible. For example, if your LLM only has to answer questions about a database, it should most likely only have read access. Scoping permissions is a good practice in general, but especially important when deploying LLMs.

For other prompt injection prevention methods, the water is a little more murky. We recommend you research your specific use case, and see what the latest security research says about it as the field is changing at a very rapid pace.



================================================
FILE: pyproject.toml
================================================
[tool.poetry]
name = "chat-langchain"
version = "0.1.1"
description = ""
authors = ["SN <6432132+samnoyes@users.noreply.github.com>"]
readme = "README.md"
packages = [{ include = "backend" }]

[tool.poetry.dependencies]
python = "^3.11"
langchain = ">=0.3.0,<0.4.0"
langsmith = ">=0.3.42" 
langchain-google-genai = ">=2.1.2,<3.0.0"
langchain-core = ">=0.3.10,<0.4.0"
langchain-community = ">=0.3.0,<0.4.0"
langchain-openai = ">=0.3.12,<0.4.0"
langchain-anthropic = ">=0.3.10,<0.4.0"
langchain-weaviate = ">=0.0.3,<0.1.0"
langgraph = ">=0.4.5"
beautifulsoup4 = "^4.12.2"
weaviate-client = "^4.0.0"
lxml = "^4.9.3"
voyageai = "^0.1.4"
pillow = "^10.2.0"
# do not remove this, it's used for indexing
psycopg2-binary = "^2.9.9"

[tool.poetry.group.dev.dependencies]
langgraph-sdk = ">=0.1.61,<0.2.0"
pytest = "^7.3.0"
ruff = "^0.2.2"
pandas = "^2.2.2"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"



================================================
FILE: .dockerignore
================================================
frontend/
assets/


================================================
FILE: .env.gcp.yaml.example
================================================
OPENAI_API_KEY: your_secret_key_here
LANGCHAIN_TRACING_V2: "true"
LANGCHAIN_PROJECT: langserve-launch-example
LANGCHAIN_API_KEY: your_secret_key_here
FIREWORKS_API_KEY: your_secret_here
WEAVIATE_API_KEY: your_secret_key_here
WEAVIATE_URL: https://your-weaviate-instance.com
WEAVIATE_INDEX_NAME: your_index_name
RECORD_MANAGER_DB_URL: your_db_url


================================================
FILE: _scripts/clear_index.py
================================================
"""Clear Weaviate index."""
import logging
import os

import weaviate
from langchain.embeddings import OpenAIEmbeddings
from langchain.indexes import SQLRecordManager, index
from langchain.vectorstores import Weaviate

logger = logging.getLogger(__name__)

WEAVIATE_URL = os.environ["WEAVIATE_URL"]
WEAVIATE_API_KEY = os.environ["WEAVIATE_API_KEY"]
RECORD_MANAGER_DB_URL = os.environ["RECORD_MANAGER_DB_URL"]
WEAVIATE_DOCS_INDEX_NAME = "LangChain_Combined_Docs_OpenAI_text_embedding_3_small"


def clear():
    client = weaviate.Client(
        url=WEAVIATE_URL,
        auth_client_secret=weaviate.AuthApiKey(api_key=WEAVIATE_API_KEY),
    )
    vectorstore = Weaviate(
        client=client,
        index_name=WEAVIATE_DOCS_INDEX_NAME,
        text_key="text",
        embedding=OpenAIEmbeddings(),
        by_text=False,
        attributes=["source", "title"],
    )

    record_manager = SQLRecordManager(
        f"weaviate/{WEAVIATE_DOCS_INDEX_NAME}", db_url=RECORD_MANAGER_DB_URL
    )
    record_manager.create_schema()

    indexing_stats = index(
        [],
        record_manager,
        vectorstore,
        cleanup="full",
        source_id_key="source",
    )

    logger.info("Indexing stats: ", indexing_stats)
    logger.info(
        "LangChain now has this many vectors: ",
        client.query.aggregate(WEAVIATE_DOCS_INDEX_NAME).with_meta_count().do(),
    )


if __name__ == "__main__":
    clear()



================================================
FILE: _scripts/evaluate_chains.py
================================================
import argparse
import functools
import json
import os
from operator import itemgetter
from typing import Literal, Optional, Union

import weaviate
from langchain import load as langchain_load
from langchain.chat_models import ChatAnthropic, ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.retriever import BaseRetriever
from langchain.schema.runnable import Runnable, RunnableMap
from langchain.smith import RunEvalConfig
from langchain.vectorstores import Weaviate
from langsmith import Client, RunEvaluator
from langsmith.evaluation.evaluator import EvaluationResult
from langsmith.schemas import Example, Run

_PROVIDER_MAP = {
    "openai": ChatOpenAI,
    "anthropic": ChatAnthropic,
}

_MODEL_MAP = {
    "openai": "gpt-3.5-turbo-1106",
    "anthropic": "claude-2",
}
WEAVIATE_DOCS_INDEX_NAME = "LangChain_Combined_Docs_OpenAI_text_embedding_3_small"


def create_chain(
    retriever: BaseRetriever,
    model_provider: Union[Literal["openai"], Literal["anthropic"]],
    chat_history: Optional[list] = None,
    model: Optional[str] = None,
    temperature: float = 0.0,
) -> Runnable:
    model_name = model or _MODEL_MAP[model_provider]
    model = _PROVIDER_MAP[model_provider](model=model_name, temperature=temperature)

    _template = """Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.

    Chat History:
    {chat_history}
    Follow Up Input: {question}
    Standalone Question:"""

    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)

    _template = """
    You are an expert programmer and problem-solver, tasked to answer any question about Langchain. Using the provided context, answer the user's question to the best of your ability using the resources provided.
    If you really don't know the answer, just say "Hmm, I'm not sure." Don't try to make up an answer.
    Anything between the following markdown blocks is retrieved from a knowledge bank, not part of the conversation with the user. 
    <context>
        {context} 
    <context/>"""

    if chat_history:
        _inputs = RunnableMap(
            {
                "standalone_question": {
                    "question": lambda x: x["question"],
                    "chat_history": lambda x: x["chat_history"],
                }
                | CONDENSE_QUESTION_PROMPT
                | model
                | StrOutputParser(),
                "question": lambda x: x["question"],
                "chat_history": lambda x: x["chat_history"],
            }
        )
        _context = {
            "context": itemgetter("standalone_question") | retriever,
            "question": lambda x: x["question"],
            "chat_history": lambda x: x["chat_history"],
        }
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", _template),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{question}"),
            ]
        )
    else:
        _inputs = RunnableMap(
            {
                "question": lambda x: x["question"],
                "chat_history": lambda x: [],
            }
        )
        _context = {
            "context": itemgetter("question") | retriever,
            "question": lambda x: x["question"],
            "chat_history": lambda x: [],
        }
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", _template),
                ("human", "{question}"),
            ]
        )

    chain = _inputs | _context | prompt | model | StrOutputParser()

    return chain


def _get_retriever():
    WEAVIATE_URL = os.environ["WEAVIATE_URL"]
    WEAVIATE_API_KEY = os.environ["WEAVIATE_API_KEY"]

    client = weaviate.Client(
        url=WEAVIATE_URL,
        auth_client_secret=weaviate.AuthApiKey(api_key=WEAVIATE_API_KEY),
    )
    weaviate_client = Weaviate(
        client=client,
        index_name=WEAVIATE_DOCS_INDEX_NAME,
        text_key="text",
        embedding=OpenAIEmbeddings(chunk_size=200),
        by_text=False,
        attributes=["source"],
    )
    return weaviate_client.as_retriever(search_kwargs=dict(k=3))


class CustomHallucinationEvaluator(RunEvaluator):
    @staticmethod
    def _get_llm_runs(run: Run) -> Run:
        runs = []
        for child in run.child_runs or []:
            if run.run_type == "llm":
                runs.append(child)
            else:
                runs.extend(CustomHallucinationEvaluator._get_llm_runs(child))

    def evaluate_run(
        self, run: Run, example: Example | None = None
    ) -> EvaluationResult:
        llm_runs = self._get_llm_runs(run)
        if not llm_runs:
            return EvaluationResult(key="hallucination", comment="No LLM runs found")
        if len(llm_runs) > 0:
            return EvaluationResult(
                key="hallucination", comment="Too many LLM runs found"
            )
        llm_run = llm_runs[0]
        messages = llm_run.inputs["messages"]
        langchain_load(json.dumps(messages))


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset-name", default="Chat LangChain Complex Questions")
    parser.add_argument("--model-provider", default="openai")
    parser.add_argument("--prompt-type", default="chat")
    args = parser.parse_args()
    client = Client()
    # Check dataset exists
    ds = client.read_dataset(dataset_name=args.dataset_name)
    retriever = _get_retriever()
    constructor = functools.partial(
        create_chain,
        retriever=retriever,
        model_provider=args.model_provider,
    )
    chain = constructor()
    eval_config = RunEvalConfig(evaluators=["qa"], prediction_key="output")
    results = client.run_on_dataset(
        dataset_name=args.dataset_name,
        llm_or_chain_factory=constructor,
        evaluation=eval_config,
        tags=["simple_chain"],
        verbose=True,
    )
    print(results)
    proj = client.read_project(project_name=results["project_name"])
    print(proj.feedback_stats)



================================================
FILE: _scripts/evaluate_chains_agent.py
================================================
import argparse
import json
import os
from typing import Optional

import weaviate
from langchain import load as langchain_load
from langchain.agents import AgentExecutor, Tool
from langchain.agents.openai_functions_agent.agent_token_buffer_memory import (
    AgentTokenBufferMemory,
)
from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.prompts import MessagesPlaceholder
from langchain.schema.messages import SystemMessage
from langchain.smith import RunEvalConfig, run_on_dataset
from langchain.vectorstores import Weaviate
from langsmith import Client, RunEvaluator
from langsmith.evaluation.evaluator import EvaluationResult
from langsmith.schemas import Example, Run

WEAVIATE_URL = os.environ["WEAVIATE_URL"]
WEAVIATE_API_KEY = os.environ["WEAVIATE_API_KEY"]
WEAVIATE_DOCS_INDEX_NAME = "LangChain_Combined_Docs_OpenAI_text_embedding_3_small"


def search(inp: str, callbacks=None) -> list:
    client = weaviate.Client(
        url=WEAVIATE_URL,
        auth_client_secret=weaviate.AuthApiKey(api_key=WEAVIATE_API_KEY),
    )
    weaviate_client = Weaviate(
        client=client,
        index_name=WEAVIATE_DOCS_INDEX_NAME,
        text_key="text",
        embedding=OpenAIEmbeddings(chunk_size=200),
        by_text=False,
        attributes=["source"],
    )
    retriever = weaviate_client.as_retriever(
        search_kwargs=dict(k=3), callbacks=callbacks
    )

    docs = retriever.get_relevant_documents(inp, callbacks=callbacks)
    return [doc.page_content for doc in docs]


def get_tools():
    langchain_tool = Tool(
        name="Documentation",
        func=search,
        description="useful for when you need to refer to LangChain's documentation, for both API reference and codebase",
    )
    ALL_TOOLS = [langchain_tool]

    return ALL_TOOLS


def get_agent(llm, *, chat_history: Optional[list] = None):
    chat_history = chat_history or []
    system_message = SystemMessage(
        content=(
            "You are an expert developer tasked answering questions about the LangChain Python package. "
            "You have access to a LangChain knowledge bank which you can query but know NOTHING about LangChain otherwise. "
            "You should always first query the knowledge bank for information on the concepts in the question. "
            "For example, given the following input question:\n"
            "-----START OF EXAMPLE INPUT QUESTION-----\n"
            "What is the transform() method for runnables? \n"
            "-----END OF EXAMPLE INPUT QUESTION-----\n"
            "Your research flow should be:\n"
            "1. Query your search tool for information on 'Runnables.transform() method' to get as much context as you can about it.\n"
            "2. Then, query your search tool for information on 'Runnables' to get as much context as you can about it.\n"
            "3. Answer the question with the context you have gathered."
            "For another example, given the following input question:\n"
            "-----START OF EXAMPLE INPUT QUESTION-----\n"
            "How can I use vLLM to run my own locally hosted model? \n"
            "-----END OF EXAMPLE INPUT QUESTION-----\n"
            "Your research flow should be:\n"
            "1. Query your search tool for information on 'run vLLM locally' to get as much context as you can about it. \n"
            "2. Answer the question as you now have enough context.\n\n"
            "Include CORRECT Python code snippets in your answer if relevant to the question. If you can't find the answer, DO NOT make up an answer. Just say you don't know. "
            "Answer the following question as best you can:"
        )
    )

    prompt = OpenAIFunctionsAgent.create_prompt(
        system_message=system_message,
        extra_prompt_messages=[MessagesPlaceholder(variable_name="chat_history")],
    )

    memory = AgentTokenBufferMemory(
        memory_key="chat_history", llm=llm, max_token_limit=2000
    )

    for msg in chat_history:
        if "question" in msg:
            memory.chat_memory.add_user_message(str(msg.pop("question")))
        if "result" in msg:
            memory.chat_memory.add_ai_message(str(msg.pop("result")))

    tools = get_tools()

    agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)
    agent_executor = AgentExecutor(
        agent=agent,
        tools=tools,
        memory=memory,
        verbose=False,
        return_intermediate_steps=True,
    )

    return agent_executor


class CustomHallucinationEvaluator(RunEvaluator):
    @staticmethod
    def _get_llm_runs(run: Run) -> Run:
        runs = []
        for child in run.child_runs or []:
            if run.run_type == "llm":
                runs.append(child)
            else:
                runs.extend(CustomHallucinationEvaluator._get_llm_runs(child))

    def evaluate_run(
        self, run: Run, example: Example | None = None
    ) -> EvaluationResult:
        llm_runs = self._get_llm_runs(run)
        if not llm_runs:
            return EvaluationResult(key="hallucination", comment="No LLM runs found")
        if len(llm_runs) > 0:
            return EvaluationResult(
                key="hallucination", comment="Too many LLM runs found"
            )
        llm_run = llm_runs[0]
        messages = llm_run.inputs["messages"]
        langchain_load(json.dumps(messages))


def return_results(client, llm):
    results = run_on_dataset(
        client=client,
        dataset_name=args.dataset_name,
        llm_or_chain_factory=lambda llm: get_agent(llm),
        evaluation=eval_config,
        verbose=True,
        concurrency_level=0,  # Add this to not go async
    )
    return results


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset-name", default="Chat LangChain Complex Questions")
    parser.add_argument("--model-provider", default="openai")
    parser.add_argument("--prompt-type", default="chat")
    args = parser.parse_args()
    client = Client()
    # Check dataset exists
    ds = client.read_dataset(dataset_name=args.dataset_name)

    llm = ChatOpenAI(model="gpt-3.5-turbo-1106", streaming=True, temperature=0)

    eval_config = RunEvalConfig(evaluators=["qa"], prediction_key="output")
    results = run_on_dataset(
        client,
        dataset_name=args.dataset_name,
        llm_or_chain_factory=lambda x: get_agent(llm),
        evaluation=eval_config,
        verbose=False,
        concurrency_level=0,  # Add this to not go async
        tags=["agent"],
        input_mapper=lambda x: x["question"],
    )
    print(results)

    proj = client.read_project(project_name=results["project_name"])
    print(proj.feedback_stats)



================================================
FILE: _scripts/evaluate_chains_improved_chain.py
================================================
import argparse
import datetime
import functools
import json
import os
from typing import Literal, Optional, Union

import weaviate
from langchain import load as langchain_load
from langchain.chat_models import ChatAnthropic, ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.output_parsers import CommaSeparatedListOutputParser
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.retriever import BaseRetriever
from langchain.schema.runnable import Runnable, RunnableMap
from langchain.smith import RunEvalConfig
from langchain.vectorstores import Weaviate
from langsmith import Client, RunEvaluator
from langsmith.evaluation.evaluator import EvaluationResult
from langsmith.schemas import Example, Run

_PROVIDER_MAP = {
    "openai": ChatOpenAI,
    "anthropic": ChatAnthropic,
}

_MODEL_MAP = {
    "openai": "gpt-3.5-turbo-1106",
    "anthropic": "claude-2",
}
WEAVIATE_DOCS_INDEX_NAME = "LangChain_Combined_Docs_OpenAI_text_embedding_3_small"


def search(search_queries, retriever: BaseRetriever):
    results = []
    for q in search_queries:
        results.extend(retriever.get_relevant_documents(q))
    return results


def create_search_queries_chain(
    retriever: BaseRetriever,
    model_provider: Union[Literal["openai"], Literal["anthropic"]],
    model: Optional[str] = None,
    temperature: float = 0.0,
    include_question_and_chat_history=True,
) -> Runnable:
    model_name = model or _MODEL_MAP[model_provider]
    model = _PROVIDER_MAP[model_provider](model=model_name, temperature=temperature)
    output_parser = CommaSeparatedListOutputParser()

    _template = """Given the following conversation and a follow up question, generate a list of search queries within LangChain's internal documentation. Keep the total number of search queries to be less than 3, and try to minimize the number of search queries if possible. We want to search as few times as possible, only retrieving the information that is absolutely necessary for answering the user's questions.

1. If the user's question is a straightforward greeting or unrelated to LangChain, there's no need for any searches. In this case, output an empty list.

2. If the user's question pertains to a specific topic or feature within LangChain, identify up to two key terms or phrases that should be searched for in the documentation. If you think there are more than two relevant terms or phrases, then choose the two that you deem to be the most important/unique.

{format_instructions}

EXAMPLES:
    Chat History:

    Follow Up Input: Hi LangChain!
    Search Queries: 

    Chat History:
    What are vector stores?
    Follow Up Input: How do I use the Chroma vector store?
    Search Queries: Chroma vector store

    Chat History:
    What are agents?
    Follow Up Input: "How do I use a ReAct agent with an Anthropic model?"
    Search Queries: ReAct Agent, Anthropic Model

END EXAMPLES. BEGIN REAL USER INPUTS. ONLY RESPOND WITH A COMMA-SEPARATED LIST. REMEMBER TO GIVE NO MORE THAN TWO RESULTS.

    Chat History:
    {chat_history}
    Follow Up Input: {question}
    Search Queries: """

    SEARCH_QUERIES_PROMPT = PromptTemplate.from_template(
        _template,
        partial_variables={
            "format_instructions": output_parser.get_format_instructions()
        },
    )

    chain_map = {
        "answer": {
            "question": lambda x: x["question"],
            "chat_history": lambda x: x.get("chat_history", []),
        }
        | SEARCH_QUERIES_PROMPT
        | model
        | output_parser,
    }

    if include_question_and_chat_history:
        chain_map["question"] = lambda x: x["question"]
        chain_map["chat_history"] = lambda x: x.get("chat_history", [])

    return RunnableMap(chain_map)


def create_chain(
    retriever: BaseRetriever,
    model_provider: Union[Literal["openai"], Literal["anthropic"]],
    chat_history: Optional[list] = None,
    model: Optional[str] = None,
    temperature: float = 0.0,
) -> Runnable:
    _inputs = create_search_queries_chain(retriever, model_provider, model, temperature)
    model_name = model or _MODEL_MAP[model_provider]
    model = _PROVIDER_MAP[model_provider](model=model_name, temperature=temperature)

    _template = """
    You are an expert programmer and problem-solver, tasked to answer any question about Langchain. Using the provided context, answer the user's question to the best of your ability using the resources provided.
    If you really don't know the answer, just say "Hmm, I'm not sure." Don't try to make up an answer.
    Anything between the following markdown blocks is retrieved from a knowledge bank, not part of the conversation with the user. 
    <context>
        {context} 
    <context/>"""

    _context = {
        "context": lambda x: search(x["answer"], retriever),
        "question": lambda x: x["question"],
        "chat_history": lambda x: x.get("chat_history", []),
    }
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", _template),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}"),
        ]
    )

    chain = _inputs | _context | prompt | model | StrOutputParser()

    return chain


def _get_retriever():
    WEAVIATE_URL = os.environ["WEAVIATE_URL"]
    WEAVIATE_API_KEY = os.environ["WEAVIATE_API_KEY"]

    client = weaviate.Client(
        url=WEAVIATE_URL,
        auth_client_secret=weaviate.AuthApiKey(api_key=WEAVIATE_API_KEY),
    )
    weaviate_client = Weaviate(
        client=client,
        index_name=WEAVIATE_DOCS_INDEX_NAME,
        text_key="text",
        embedding=OpenAIEmbeddings(chunk_size=200),
        by_text=False,
        attributes=["source"],
    )
    return weaviate_client.as_retriever(search_kwargs=dict(k=3))


class CustomHallucinationEvaluator(RunEvaluator):
    @staticmethod
    def _get_llm_runs(run: Run) -> Run:
        runs = []
        for child in run.child_runs or []:
            if run.run_type == "llm":
                runs.append(child)
            else:
                runs.extend(CustomHallucinationEvaluator._get_llm_runs(child))

    def evaluate_run(
        self, run: Run, example: Example | None = None
    ) -> EvaluationResult:
        llm_runs = self._get_llm_runs(run)
        if not llm_runs:
            return EvaluationResult(key="hallucination", comment="No LLM runs found")
        if len(llm_runs) > 0:
            return EvaluationResult(
                key="hallucination", comment="Too many LLM runs found"
            )
        llm_run = llm_runs[0]
        messages = llm_run.inputs["messages"]
        langchain_load(json.dumps(messages))


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset-name", default="Chat LangChain Simple Questions")
    parser.add_argument(
        "--search-queries-dataset-name", default="Chat LangChain Search Queries"
    )
    parser.add_argument("--model-provider", default="openai")
    parser.add_argument("--prompt-type", default="chat")
    args = parser.parse_args()
    client = Client()
    # Check dataset exists
    ds = client.read_dataset(dataset_name=args.dataset_name)
    retriever = _get_retriever()
    constructor = functools.partial(
        create_chain,
        retriever=retriever,
        model_provider=args.model_provider,
    )
    chain = constructor()
    eval_config = RunEvalConfig(evaluators=["qa"], prediction_key="output")
    results = client.run_on_dataset(
        dataset_name=args.dataset_name,
        project_name=f"improved_chain {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        llm_or_chain_factory=constructor,
        evaluation=eval_config,
        tags=["improved_chain"],
        verbose=True,
    )
    eval_config_search_queries = RunEvalConfig(evaluators=["qa"])
    search_queries_constructor = functools.partial(
        create_search_queries_chain,
        retriever=retriever,
        model_provider=args.model_provider,
        include_question_and_chat_history=False,
    )
    search_queries_chain = search_queries_constructor()
    results = client.run_on_dataset(
        dataset_name=args.search_queries_dataset_name,
        project_name=f"improved_chain {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        llm_or_chain_factory=search_queries_chain,
        evaluation=eval_config_search_queries,
        tags=["improved_chain"],
        verbose=True,
    )
    print(results)
    proj = client.read_project(project_name=results["project_name"])
    print(proj.feedback_stats)



================================================
FILE: _scripts/evaluate_chat_langchain.py
================================================
# TODO: Consolidate all these scripts into a single script
# This is ugly
import argparse

from langchain.chat_models import ChatAnthropic, ChatOpenAI
from langchain.smith import RunEvalConfig
from langsmith import Client

# Ugly. Requires PYTHONATH=$(PWD) to run
from backend.chain import create_chain, get_retriever

_PROVIDER_MAP = {
    "openai": ChatOpenAI,
    "anthropic": ChatAnthropic,
}

_MODEL_MAP = {
    "openai": "gpt-3.5-turbo-1106",
    "anthropic": "claude-2",
}


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset-name", default="Chat LangChain Complex Questions")
    parser.add_argument("--model-provider", default="openai")
    args = parser.parse_args()
    client = Client()
    # Check dataset exists
    ds = client.read_dataset(dataset_name=args.dataset_name)
    retriever = get_retriever()
    llm = _PROVIDER_MAP[args.model_provider](
        model=_MODEL_MAP[args.model_provider], temperature=0
    )

    # In app, we always pass in a chat history, but for evaluation we don't
    # necessarily do that. Add that handling here.
    def construct_eval_chain():
        chain = create_chain(
            retriever=retriever,
            llm=llm,
        )
        return {
            "question": lambda x: x["question"],
            "chat_history": (lambda x: x.get("chat_history", [])),
        } | chain

    eval_config = RunEvalConfig(
        evaluators=["qa"],
        prediction_key="output",
    )
    results = client.run_on_dataset(
        dataset_name=args.dataset_name,
        llm_or_chain_factory=construct_eval_chain,
        evaluation=eval_config,
        tags=["simple_chain"],
        verbose=True,
    )



================================================
FILE: backend/configuration.py
================================================
"""Define the configurable parameters for the agent."""

from __future__ import annotations

from dataclasses import dataclass, field, fields
from typing import Annotated, Any, Literal, Optional, Type, TypeVar

from langchain_core.runnables import RunnableConfig, ensure_config

MODEL_NAME_TO_RESPONSE_MODEL = {
    "anthropic_claude_3_5_sonnet": "anthropic/claude-3-5-sonnet-20240620",
}


def _update_configurable_for_backwards_compatibility(
    configurable: dict[str, Any],
) -> dict[str, Any]:
    update = {}
    if "k" in configurable:
        update["search_kwargs"] = {"k": configurable["k"]}

    if "model_name" in configurable:
        update["response_model"] = MODEL_NAME_TO_RESPONSE_MODEL.get(
            configurable["model_name"], configurable["model_name"]
        )

    if update:
        return {**configurable, **update}

    return configurable


@dataclass(kw_only=True)
class BaseConfiguration:
    """Configuration class for indexing and retrieval operations.

    This class defines the parameters needed for configuring the indexing and
    retrieval processes, including embedding model selection, retriever provider choice, and search parameters.
    """

    embedding_model: Annotated[
        str,
        {"__template_metadata__": {"kind": "embeddings"}},
    ] = field(
        default="openai/text-embedding-3-small",
        metadata={
            "description": "Name of the embedding model to use. Must be a valid embedding model name."
        },
    )

    retriever_provider: Annotated[
        Literal["weaviate"],
        {"__template_metadata__": {"kind": "retriever"}},
    ] = field(
        default="weaviate",
        metadata={"description": "The vector store provider to use for retrieval."},
    )

    search_kwargs: dict[str, Any] = field(
        default_factory=dict,
        metadata={
            "description": "Additional keyword arguments to pass to the search function of the retriever."
        },
    )

    # for backwards compatibility
    k: int = field(
        default=6,
        metadata={
            "description": "The number of documents to retrieve. Use search_kwargs instead."
        },
    )

    @classmethod
    def from_runnable_config(
        cls: Type[T], config: Optional[RunnableConfig] = None
    ) -> T:
        """Create an IndexConfiguration instance from a RunnableConfig object.

        Args:
            cls (Type[T]): The class itself.
            config (Optional[RunnableConfig]): The configuration object to use.

        Returns:
            T: An instance of IndexConfiguration with the specified configuration.
        """
        config = ensure_config(config)
        configurable = config.get("configurable") or {}
        configurable = _update_configurable_for_backwards_compatibility(configurable)
        _fields = {f.name for f in fields(cls) if f.init}
        return cls(**{k: v for k, v in configurable.items() if k in _fields})


T = TypeVar("T", bound=BaseConfiguration)



================================================
FILE: backend/constants.py
================================================
WEAVIATE_DOCS_INDEX_NAME = "LangChain_Combined_Docs_OpenAI_text_embedding_3_small"



================================================
FILE: backend/embeddings.py
================================================
from langchain_core.embeddings import Embeddings
from langchain_openai import OpenAIEmbeddings


def get_embeddings_model() -> Embeddings:
    return OpenAIEmbeddings(model="text-embedding-3-small", chunk_size=200)



================================================
FILE: backend/ingest.py
================================================
"""Load html from files, clean up, split, ingest into Weaviate."""
import logging
import os
import re
from typing import Optional

import weaviate
from bs4 import BeautifulSoup, SoupStrainer
from langchain.document_loaders import RecursiveUrlLoader, SitemapLoader
from langchain.indexes import SQLRecordManager, index
from langchain.utils.html import PREFIXES_TO_IGNORE_REGEX, SUFFIXES_TO_IGNORE_REGEX
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_weaviate import WeaviateVectorStore

from backend.constants import WEAVIATE_DOCS_INDEX_NAME
from backend.embeddings import get_embeddings_model
from backend.parser import langchain_docs_extractor

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def metadata_extractor(
    meta: dict, soup: BeautifulSoup, title_suffix: Optional[str] = None
) -> dict:
    title_element = soup.find("title")
    description_element = soup.find("meta", attrs={"name": "description"})
    html_element = soup.find("html")
    title = title_element.get_text() if title_element else ""
    if title_suffix is not None:
        title += title_suffix

    return {
        "source": meta["loc"],
        "title": title,
        "description": description_element.get("content", "")
        if description_element
        else "",
        "language": html_element.get("lang", "") if html_element else "",
        **meta,
    }


def load_langchain_docs():
    return SitemapLoader(
        "https://python.langchain.com/sitemap.xml",
        filter_urls=["https://python.langchain.com/"],
        parsing_function=langchain_docs_extractor,
        default_parser="lxml",
        bs_kwargs={
            "parse_only": SoupStrainer(
                name=("article", "title", "html", "lang", "content")
            ),
        },
        meta_function=metadata_extractor,
    ).load()


def load_langgraph_docs():
    return SitemapLoader(
        "https://langchain-ai.github.io/langgraph/sitemap.xml",
        parsing_function=simple_extractor,
        default_parser="lxml",
        bs_kwargs={"parse_only": SoupStrainer(name=("article", "title"))},
        meta_function=lambda meta, soup: metadata_extractor(
            meta, soup, title_suffix=" | ðŸ¦œðŸ•¸ï¸LangGraph"
        ),
    ).load()


def load_langsmith_docs():
    return RecursiveUrlLoader(
        url="https://docs.smith.langchain.com/",
        max_depth=8,
        extractor=simple_extractor,
        prevent_outside=True,
        use_async=True,
        timeout=600,
        # Drop trailing / to avoid duplicate pages.
        link_regex=(
            f"href=[\"']{PREFIXES_TO_IGNORE_REGEX}((?:{SUFFIXES_TO_IGNORE_REGEX}.)*?)"
            r"(?:[\#'\"]|\/[\#'\"])"
        ),
        check_response_status=True,
    ).load()


def simple_extractor(html: str | BeautifulSoup) -> str:
    if isinstance(html, str):
        soup = BeautifulSoup(html, "lxml")
    elif isinstance(html, BeautifulSoup):
        soup = html
    else:
        raise ValueError(
            "Input should be either BeautifulSoup object or an HTML string"
        )
    return re.sub(r"\n\n+", "\n\n", soup.text).strip()


def load_api_docs():
    return RecursiveUrlLoader(
        url="https://api.python.langchain.com/en/latest/",
        max_depth=8,
        extractor=simple_extractor,
        prevent_outside=True,
        use_async=True,
        timeout=600,
        # Drop trailing / to avoid duplicate pages.
        link_regex=(
            f"href=[\"']{PREFIXES_TO_IGNORE_REGEX}((?:{SUFFIXES_TO_IGNORE_REGEX}.)*?)"
            r"(?:[\#'\"]|\/[\#'\"])"
        ),
        check_response_status=True,
        exclude_dirs=(
            "https://api.python.langchain.com/en/latest/_sources",
            "https://api.python.langchain.com/en/latest/_modules",
        ),
    ).load()


def ingest_docs():
    WEAVIATE_URL = os.environ["WEAVIATE_URL"]
    WEAVIATE_API_KEY = os.environ["WEAVIATE_API_KEY"]
    RECORD_MANAGER_DB_URL = os.environ["RECORD_MANAGER_DB_URL"]

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=200)
    embedding = get_embeddings_model()

    with weaviate.connect_to_weaviate_cloud(
        cluster_url=WEAVIATE_URL,
        auth_credentials=weaviate.classes.init.Auth.api_key(WEAVIATE_API_KEY),
        skip_init_checks=True,
    ) as weaviate_client:
        vectorstore = WeaviateVectorStore(
            client=weaviate_client,
            index_name=WEAVIATE_DOCS_INDEX_NAME,
            text_key="text",
            embedding=embedding,
            attributes=["source", "title"],
        )

        record_manager = SQLRecordManager(
            f"weaviate/{WEAVIATE_DOCS_INDEX_NAME}", db_url=RECORD_MANAGER_DB_URL
        )
        record_manager.create_schema()

        docs_from_documentation = load_langchain_docs()
        logger.info(f"Loaded {len(docs_from_documentation)} docs from documentation")
        docs_from_api = load_api_docs()
        logger.info(f"Loaded {len(docs_from_api)} docs from API")
        docs_from_langsmith = load_langsmith_docs()
        logger.info(f"Loaded {len(docs_from_langsmith)} docs from LangSmith")
        docs_from_langgraph = load_langgraph_docs()
        logger.info(f"Loaded {len(docs_from_langgraph)} docs from LangGraph")

        docs_transformed = text_splitter.split_documents(
            docs_from_documentation
            + docs_from_api
            + docs_from_langsmith
            + docs_from_langgraph
        )
        docs_transformed = [
            doc for doc in docs_transformed if len(doc.page_content) > 10
        ]

        # We try to return 'source' and 'title' metadata when querying vector store and
        # Weaviate will error at query time if one of the attributes is missing from a
        # retrieved document.
        for doc in docs_transformed:
            if "source" not in doc.metadata:
                doc.metadata["source"] = ""
            if "title" not in doc.metadata:
                doc.metadata["title"] = ""

        indexing_stats = index(
            docs_transformed,
            record_manager,
            vectorstore,
            cleanup="full",
            source_id_key="source",
            force_update=(os.environ.get("FORCE_UPDATE") or "false").lower() == "true",
        )

        logger.info(f"Indexing stats: {indexing_stats}")
        num_vecs = (
            weaviate_client.collections.get(WEAVIATE_DOCS_INDEX_NAME)
            .aggregate.over_all()
            .total_count
        )
        logger.info(
            f"LangChain now has this many vectors: {num_vecs}",
        )


if __name__ == "__main__":
    ingest_docs()



================================================
FILE: backend/parser.py
================================================
import re
from typing import Generator

from bs4 import BeautifulSoup, Doctype, NavigableString, Tag


def langchain_docs_extractor(soup: BeautifulSoup) -> str:
    # Remove all the tags that are not meaningful for the extraction.
    SCAPE_TAGS = ["nav", "footer", "aside", "script", "style"]
    [tag.decompose() for tag in soup.find_all(SCAPE_TAGS)]

    def get_text(tag: Tag) -> Generator[str, None, None]:
        for child in tag.children:
            if isinstance(child, Doctype):
                continue

            if isinstance(child, NavigableString):
                yield child
            elif isinstance(child, Tag):
                if child.name in ["h1", "h2", "h3", "h4", "h5", "h6"]:
                    yield f"{'#' * int(child.name[1:])} {child.get_text()}\n\n"
                elif child.name == "a":
                    yield f"[{child.get_text(strip=False)}]({child.get('href')})"
                elif child.name == "img":
                    yield f"![{child.get('alt', '')}]({child.get('src')})"
                elif child.name in ["strong", "b"]:
                    yield f"**{child.get_text(strip=False)}**"
                elif child.name in ["em", "i"]:
                    yield f"_{child.get_text(strip=False)}_"
                elif child.name == "br":
                    yield "\n"
                elif child.name == "code":
                    parent = child.find_parent()
                    if parent is not None and parent.name == "pre":
                        classes = parent.attrs.get("class", "")

                        language = next(
                            filter(lambda x: re.match(r"language-\w+", x), classes),
                            None,
                        )
                        if language is None:
                            language = ""
                        else:
                            language = language.split("-")[1]

                        lines: list[str] = []
                        for span in child.find_all("span", class_="token-line"):
                            line_content = "".join(
                                token.get_text() for token in span.find_all("span")
                            )
                            lines.append(line_content)

                        code_content = "\n".join(lines)
                        yield f"```{language}\n{code_content}\n```\n\n"
                    else:
                        yield f"`{child.get_text(strip=False)}`"

                elif child.name == "p":
                    yield from get_text(child)
                    yield "\n\n"
                elif child.name == "ul":
                    for li in child.find_all("li", recursive=False):
                        yield "- "
                        yield from get_text(li)
                        yield "\n\n"
                elif child.name == "ol":
                    for i, li in enumerate(child.find_all("li", recursive=False)):
                        yield f"{i + 1}. "
                        yield from get_text(li)
                        yield "\n\n"
                elif child.name == "div" and "tabs-container" in child.attrs.get(
                    "class", [""]
                ):
                    tabs = child.find_all("li", {"role": "tab"})
                    tab_panels = child.find_all("div", {"role": "tabpanel"})
                    for tab, tab_panel in zip(tabs, tab_panels):
                        tab_name = tab.get_text(strip=True)
                        yield f"{tab_name}\n"
                        yield from get_text(tab_panel)
                elif child.name == "table":
                    thead = child.find("thead")
                    header_exists = isinstance(thead, Tag)
                    if header_exists:
                        headers = thead.find_all("th")
                        if headers:
                            yield "| "
                            yield " | ".join(header.get_text() for header in headers)
                            yield " |\n"
                            yield "| "
                            yield " | ".join("----" for _ in headers)
                            yield " |\n"

                    tbody = child.find("tbody")
                    tbody_exists = isinstance(tbody, Tag)
                    if tbody_exists:
                        for row in tbody.find_all("tr"):
                            yield "| "
                            yield " | ".join(
                                cell.get_text(strip=True) for cell in row.find_all("td")
                            )
                            yield " |\n"

                    yield "\n\n"
                elif child.name in ["button"]:
                    continue
                else:
                    yield from get_text(child)

    joined = "".join(get_text(soup))
    return re.sub(r"\n\n+", "\n\n", joined).strip()



================================================
FILE: backend/retrieval.py
================================================
import os
from contextlib import contextmanager
from typing import Iterator

import weaviate
from langchain_core.embeddings import Embeddings
from langchain_core.retrievers import BaseRetriever
from langchain_core.runnables import RunnableConfig
from langchain_weaviate import WeaviateVectorStore

from backend.configuration import BaseConfiguration
from backend.constants import WEAVIATE_DOCS_INDEX_NAME


def make_text_encoder(model: str) -> Embeddings:
    """Connect to the configured text encoder."""
    provider, model = model.split("/", maxsplit=1)
    match provider:
        case "openai":
            from langchain_openai import OpenAIEmbeddings

            return OpenAIEmbeddings(model=model)
        case _:
            raise ValueError(f"Unsupported embedding provider: {provider}")


@contextmanager
def make_weaviate_retriever(
    configuration: BaseConfiguration, embedding_model: Embeddings
) -> Iterator[BaseRetriever]:
    with weaviate.connect_to_weaviate_cloud(
        cluster_url=os.environ["WEAVIATE_URL"],
        auth_credentials=weaviate.classes.init.Auth.api_key(
            os.environ.get("WEAVIATE_API_KEY", "not_provided")
        ),
        skip_init_checks=True,
    ) as weaviate_client:
        store = WeaviateVectorStore(
            client=weaviate_client,
            index_name=WEAVIATE_DOCS_INDEX_NAME,
            text_key="text",
            embedding=embedding_model,
            attributes=["source", "title"],
        )
        search_kwargs = {**configuration.search_kwargs, "return_uuids": True}
        yield store.as_retriever(search_kwargs=search_kwargs)


@contextmanager
def make_retriever(
    config: RunnableConfig,
) -> Iterator[BaseRetriever]:
    """Create a retriever for the agent, based on the current configuration."""
    configuration = BaseConfiguration.from_runnable_config(config)
    embedding_model = make_text_encoder(configuration.embedding_model)
    match configuration.retriever_provider:
        case "weaviate":
            with make_weaviate_retriever(configuration, embedding_model) as retriever:
                yield retriever

        case _:
            raise ValueError(
                "Unrecognized retriever_provider in configuration. "
                f"Expected one of: {', '.join(BaseConfiguration.__annotations__['retriever_provider'].__args__)}\n"
                f"Got: {configuration.retriever_provider}"
            )



================================================
FILE: backend/utils.py
================================================
"""Shared utility functions used in the project.

Functions:
    format_docs: Convert documents to an xml-formatted string.
    load_chat_model: Load a chat model from a model name.
"""

import uuid
from typing import Any, Literal, Optional, Union

from langchain.chat_models import init_chat_model
from langchain_core.documents import Document
from langchain_core.language_models import BaseChatModel


def _format_doc(doc: Document) -> str:
    """Format a single document as XML.

    Args:
        doc (Document): The document to format.

    Returns:
        str: The formatted document as an XML string.
    """
    metadata = doc.metadata or {}
    meta = "".join(f" {k}={v!r}" for k, v in metadata.items())
    if meta:
        meta = f" {meta}"

    return f"<document{meta}>\n{doc.page_content}\n</document>"


def format_docs(docs: Optional[list[Document]]) -> str:
    """Format a list of documents as XML.

    This function takes a list of Document objects and formats them into a single XML string.

    Args:
        docs (Optional[list[Document]]): A list of Document objects to format, or None.

    Returns:
        str: A string containing the formatted documents in XML format.

    Examples:
        >>> docs = [Document(page_content="Hello"), Document(page_content="World")]
        >>> print(format_docs(docs))
        <documents>
        <document>
        Hello
        </document>
        <document>
        World
        </document>
        </documents>

        >>> print(format_docs(None))
        <documents></documents>
    """
    if not docs:
        return "<documents></documents>"
    formatted = "\n".join(_format_doc(doc) for doc in docs)
    return f"""<documents>
{formatted}
</documents>"""


def load_chat_model(fully_specified_name: str) -> BaseChatModel:
    """Load a chat model from a fully specified name.

    Args:
        fully_specified_name (str): String in the format 'provider/model'.
    """
    if "/" in fully_specified_name:
        provider, model = fully_specified_name.split("/", maxsplit=1)
    else:
        provider = ""
        model = fully_specified_name

    model_kwargs = {"temperature": 0}
    if provider == "google_genai":
        model_kwargs["convert_system_message_to_human"] = True
    return init_chat_model(model, model_provider=provider, **model_kwargs)


def reduce_docs(
    existing: Optional[list[Document]],
    new: Union[
        list[Document],
        list[dict[str, Any]],
        list[str],
        str,
        Literal["delete"],
    ],
) -> list[Document]:
    """Reduce and process documents based on the input type.

    This function handles various input types and converts them into a sequence of Document objects.
    It also combines existing documents with the new one based on the document ID.

    Args:
        existing (Optional[Sequence[Document]]): The existing docs in the state, if any.
        new (Union[Sequence[Document], Sequence[dict[str, Any]], Sequence[str], str, Literal["delete"]]):
            The new input to process. Can be a sequence of Documents, dictionaries, strings, or a single string.
    """
    if new == "delete":
        return []

    existing_list = list(existing) if existing else []
    if isinstance(new, str):
        return existing_list + [
            Document(page_content=new, metadata={"uuid": str(uuid.uuid4())})
        ]

    new_list = []
    if isinstance(new, list):
        existing_ids = set(doc.metadata.get("uuid") for doc in existing_list)
        for item in new:
            if isinstance(item, str):
                item_id = str(uuid.uuid4())
                new_list.append(Document(page_content=item, metadata={"uuid": item_id}))
                existing_ids.add(item_id)

            elif isinstance(item, dict):
                metadata = item.get("metadata", {})
                item_id = metadata.get("uuid", str(uuid.uuid4()))

                if item_id not in existing_ids:
                    new_list.append(
                        Document(**item, metadata={**metadata, "uuid": item_id})
                    )
                    existing_ids.add(item_id)

            elif isinstance(item, Document):
                item_id = item.metadata.get("uuid")
                if item_id is None:
                    item_id = str(uuid.uuid4())
                    new_item = item.copy(deep=True)
                    new_item.metadata["uuid"] = item_id
                else:
                    new_item = item

                if item_id not in existing_ids:
                    new_list.append(new_item)
                    existing_ids.add(item_id)

    return existing_list + new_list



================================================
FILE: backend/retrieval_graph/__init__.py
================================================
"""Retrieval Graph Module

This module provides an intelligent conversational retrieval graph system for
handling user queries about LangChain and related topics.

The main components of this system include:

1. A state management system for handling conversation context and research steps.
2. An analysis and routing mechanism to classify user queries and determine the appropriate response path.
3. A research planner that breaks down complex queries into manageable steps.
4. A researcher agent that generates queries and fetches relevant information based on research steps.
5. A response generator that formulates answers using retrieved documents and conversation history.

The graph is configured using customizable parameters defined in the AgentConfiguration class,
allowing for flexibility in model selection, retrieval methods, and system prompts.

Key Features:
- Intelligent query classification and routing
- Multi-step research planning for complex queries
- Integration with various retrieval providers (e.g., Elastic, Pinecone, MongoDB)
- Customizable language models for query analysis, research planning, and response generation
- Stateful conversation management for context-aware interactions

Usage:
    The main entry point for using this system is the `graph` object exported from this module.
    It can be invoked to process user inputs, conduct research , and generate
    informed responses based on retrieved information and conversation context.

For detailed configuration options and usage instructions, refer to the AgentConfiguration class
and individual component documentation within the retrieval_graph package.
"""  # noqa



================================================
FILE: backend/retrieval_graph/configuration.py
================================================
"""Define the configurable parameters for the agent."""

from __future__ import annotations

from dataclasses import dataclass, field

from backend.configuration import BaseConfiguration
from backend.retrieval_graph import prompts


@dataclass(kw_only=True)
class AgentConfiguration(BaseConfiguration):
    """The configuration for the agent."""

    # models

    query_model: str = field(
        default="anthropic/claude-3-5-haiku-20241022",
        metadata={
            "description": "The language model used for processing and refining queries. Should be in the form: provider/model-name."
        },
    )

    response_model: str = field(
        default="anthropic/claude-3-5-haiku-20241022",
        metadata={
            "description": "The language model used for generating responses. Should be in the form: provider/model-name."
        },
    )

    # prompts

    router_system_prompt: str = field(
        default=prompts.ROUTER_SYSTEM_PROMPT,
        metadata={
            "description": "The system prompt used for classifying user questions to route them to the correct node."
        },
    )

    more_info_system_prompt: str = field(
        default=prompts.MORE_INFO_SYSTEM_PROMPT,
        metadata={
            "description": "The system prompt used for asking for more information from the user."
        },
    )

    general_system_prompt: str = field(
        default=prompts.GENERAL_SYSTEM_PROMPT,
        metadata={
            "description": "The system prompt used for responding to general questions."
        },
    )

    research_plan_system_prompt: str = field(
        default=prompts.RESEARCH_PLAN_SYSTEM_PROMPT,
        metadata={
            "description": "The system prompt used for generating a research plan based on the user's question."
        },
    )

    generate_queries_system_prompt: str = field(
        default=prompts.GENERATE_QUERIES_SYSTEM_PROMPT,
        metadata={
            "description": "The system prompt used by the researcher to generate queries based on a step in the research plan."
        },
    )

    response_system_prompt: str = field(
        default=prompts.RESPONSE_SYSTEM_PROMPT,
        metadata={"description": "The system prompt used for generating responses."},
    )



================================================
FILE: backend/retrieval_graph/graph.py
================================================
"""Main entrypoint for the conversational retrieval graph.

This module defines the core structure and functionality of the conversational
retrieval graph. It includes the main graph definition, state management,
and key functions for processing & routing user queries, generating research plans to answer user questions,
conducting research, and formulating responses.
"""

from typing import Any, Literal, TypedDict, cast

from langchain_core.messages import BaseMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph import END, START, StateGraph

from backend.retrieval_graph.configuration import AgentConfiguration
from backend.retrieval_graph.researcher_graph.graph import graph as researcher_graph
from backend.retrieval_graph.state import AgentState, InputState, Router
from backend.utils import format_docs, load_chat_model


async def analyze_and_route_query(
    state: AgentState, *, config: RunnableConfig
) -> dict[str, Router]:
    """Analyze the user's query and determine the appropriate routing.

    This function uses a language model to classify the user's query and decide how to route it
    within the conversation flow.

    Args:
        state (AgentState): The current state of the agent, including conversation history.
        config (RunnableConfig): Configuration with the model used for query analysis.

    Returns:
        dict[str, Router]: A dictionary containing the 'router' key with the classification result (classification type and logic).
    """
    # allow skipping the router for testing
    if state.router and state.router["logic"]:
        return {"router": state.router}

    configuration = AgentConfiguration.from_runnable_config(config)
    structured_output_kwargs = (
        {"method": "function_calling"} if "openai" in configuration.query_model else {}
    )
    model = load_chat_model(configuration.query_model).with_structured_output(
        Router, **structured_output_kwargs
    )
    messages = [
        {"role": "system", "content": configuration.router_system_prompt}
    ] + state.messages
    response = cast(Router, await model.ainvoke(messages))
    return {"router": response}


def route_query(
    state: AgentState,
) -> Literal["create_research_plan", "ask_for_more_info", "respond_to_general_query"]:
    """Determine the next step based on the query classification.

    Args:
        state (AgentState): The current state of the agent, including the router's classification.

    Returns:
        Literal["create_research_plan", "ask_for_more_info", "respond_to_general_query"]: The next step to take.

    Raises:
        ValueError: If an unknown router type is encountered.
    """
    _type = state.router["type"]
    if _type == "langchain":
        return "create_research_plan"
    elif _type == "more-info":
        return "ask_for_more_info"
    elif _type == "general":
        return "respond_to_general_query"
    else:
        raise ValueError(f"Unknown router type {_type}")


async def ask_for_more_info(
    state: AgentState, *, config: RunnableConfig
) -> dict[str, list[BaseMessage]]:
    """Generate a response asking the user for more information.

    This node is called when the router determines that more information is needed from the user.

    Args:
        state (AgentState): The current state of the agent, including conversation history and router logic.
        config (RunnableConfig): Configuration with the model used to respond.

    Returns:
        dict[str, list[str]]: A dictionary with a 'messages' key containing the generated response.
    """
    configuration = AgentConfiguration.from_runnable_config(config)
    model = load_chat_model(configuration.query_model)
    system_prompt = configuration.more_info_system_prompt.format(
        logic=state.router["logic"]
    )
    messages = [{"role": "system", "content": system_prompt}] + state.messages
    response = await model.ainvoke(messages)
    return {"messages": [response]}


async def respond_to_general_query(
    state: AgentState, *, config: RunnableConfig
) -> dict[str, list[BaseMessage]]:
    """Generate a response to a general query not related to LangChain.

    This node is called when the router classifies the query as a general question.

    Args:
        state (AgentState): The current state of the agent, including conversation history and router logic.
        config (RunnableConfig): Configuration with the model used to respond.

    Returns:
        dict[str, list[str]]: A dictionary with a 'messages' key containing the generated response.
    """
    configuration = AgentConfiguration.from_runnable_config(config)
    model = load_chat_model(configuration.query_model)
    system_prompt = configuration.general_system_prompt.format(
        logic=state.router["logic"]
    )
    messages = [{"role": "system", "content": system_prompt}] + state.messages
    response = await model.ainvoke(messages)
    return {"messages": [response]}


async def create_research_plan(
    state: AgentState, *, config: RunnableConfig
) -> dict[str, list[str]]:
    """Create a step-by-step research plan for answering a LangChain-related query.

    Args:
        state (AgentState): The current state of the agent, including conversation history.
        config (RunnableConfig): Configuration with the model used to generate the plan.

    Returns:
        dict[str, list[str]]: A dictionary with a 'steps' key containing the list of research steps.
    """

    class Plan(TypedDict):
        """Generate research plan."""

        steps: list[str]

    configuration = AgentConfiguration.from_runnable_config(config)
    structured_output_kwargs = (
        {"method": "function_calling"} if "openai" in configuration.query_model else {}
    )
    model = load_chat_model(configuration.query_model).with_structured_output(
        Plan, **structured_output_kwargs
    )
    messages = [
        {"role": "system", "content": configuration.research_plan_system_prompt}
    ] + state.messages
    response = cast(
        Plan, await model.ainvoke(messages, {"tags": ["langsmith:nostream"]})
    )
    return {
        "steps": response["steps"],
        "documents": "delete",
        "query": state.messages[-1].content,
    }


async def conduct_research(state: AgentState) -> dict[str, Any]:
    """Execute the first step of the research plan.

    This function takes the first step from the research plan and uses it to conduct research.

    Args:
        state (AgentState): The current state of the agent, including the research plan steps.

    Returns:
        dict[str, list[str]]: A dictionary with 'documents' containing the research results and
                              'steps' containing the remaining research steps.

    Behavior:
        - Invokes the researcher_graph with the first step of the research plan.
        - Updates the state with the retrieved documents and removes the completed step.
    """
    result = await researcher_graph.ainvoke({"question": state.steps[0]})
    return {"documents": result["documents"], "steps": state.steps[1:]}


def check_finished(state: AgentState) -> Literal["respond", "conduct_research"]:
    """Determine if the research process is complete or if more research is needed.

    This function checks if there are any remaining steps in the research plan:
        - If there are, route back to the `conduct_research` node
        - Otherwise, route to the `respond` node

    Args:
        state (AgentState): The current state of the agent, including the remaining research steps.

    Returns:
        Literal["respond", "conduct_research"]: The next step to take based on whether research is complete.
    """
    if len(state.steps or []) > 0:
        return "conduct_research"
    else:
        return "respond"


async def respond(
    state: AgentState, *, config: RunnableConfig
) -> dict[str, list[BaseMessage]]:
    """Generate a final response to the user's query based on the conducted research.

    This function formulates a comprehensive answer using the conversation history and the documents retrieved by the researcher.

    Args:
        state (AgentState): The current state of the agent, including retrieved documents and conversation history.
        config (RunnableConfig): Configuration with the model used to respond.

    Returns:
        dict[str, list[str]]: A dictionary with a 'messages' key containing the generated response.
    """
    configuration = AgentConfiguration.from_runnable_config(config)
    model = load_chat_model(configuration.response_model)
    # TODO: add a re-ranker here
    top_k = 20
    context = format_docs(state.documents[:top_k])
    prompt = configuration.response_system_prompt.format(context=context)
    messages = [{"role": "system", "content": prompt}] + state.messages
    response = await model.ainvoke(messages)
    return {"messages": [response], "answer": response.content}


# Define the graph


builder = StateGraph(AgentState, input=InputState, config_schema=AgentConfiguration)
builder.add_node(create_research_plan)
builder.add_node(conduct_research)
builder.add_node(respond)

builder.add_edge(START, "create_research_plan")
builder.add_edge("create_research_plan", "conduct_research")
builder.add_conditional_edges("conduct_research", check_finished)
builder.add_edge("respond", END)

# Compile into a graph object that you can invoke and deploy.
graph = builder.compile()
graph.name = "RetrievalGraph"



================================================
FILE: backend/retrieval_graph/prompts.py
================================================
from langsmith import Client

"""Default prompts."""

client = Client()
# fetch from langsmith
ROUTER_SYSTEM_PROMPT = (
    client.pull_prompt("langchain-ai/chat-langchain-router-prompt")
    .messages[0]
    .prompt.template
)
GENERATE_QUERIES_SYSTEM_PROMPT = (
    client.pull_prompt("langchain-ai/chat-langchain-generate-queries-prompt")
    .messages[0]
    .prompt.template
)
MORE_INFO_SYSTEM_PROMPT = (
    client.pull_prompt("langchain-ai/chat-langchain-more-info-prompt")
    .messages[0]
    .prompt.template
)
RESEARCH_PLAN_SYSTEM_PROMPT = (
    client.pull_prompt("langchain-ai/chat-langchain-research-plan-prompt")
    .messages[0]
    .prompt.template
)
GENERAL_SYSTEM_PROMPT = (
    client.pull_prompt("langchain-ai/chat-langchain-general-prompt")
    .messages[0]
    .prompt.template
)
RESPONSE_SYSTEM_PROMPT = (
    client.pull_prompt("langchain-ai/chat-langchain-response-prompt")
    .messages[0]
    .prompt.template
)



================================================
FILE: backend/retrieval_graph/state.py
================================================
"""State management for the retrieval graph.

This module defines the state structures used in the retrieval graph. It includes
definitions for agent state, input state, and router classification schema.
"""

from dataclasses import dataclass, field
from typing import Annotated, Literal

from langchain_core.documents import Document
from langchain_core.messages import AnyMessage
from langgraph.graph import add_messages
from typing_extensions import TypedDict

from backend.utils import reduce_docs


# Optional, the InputState is a restricted version of the State that is used to
# define a narrower interface to the outside world vs. what is maintained
# internally.
@dataclass(kw_only=True)
class InputState:
    """Represents the input state for the agent.

    This class defines the structure of the input state, which includes
    the messages exchanged between the user and the agent. It serves as
    a restricted version of the full State, providing a narrower interface
    to the outside world compared to what is maintained internally.
    """

    messages: Annotated[list[AnyMessage], add_messages]
    """Messages track the primary execution state of the agent.

    Typically accumulates a pattern of Human/AI/Human/AI messages; if
    you were to combine this template with a tool-calling ReAct agent pattern,
    it may look like this:

    1. HumanMessage - user input
    2. AIMessage with .tool_calls - agent picking tool(s) to use to collect
         information
    3. ToolMessage(s) - the responses (or errors) from the executed tools
    
        (... repeat steps 2 and 3 as needed ...)
    4. AIMessage without .tool_calls - agent responding in unstructured
        format to the user.

    5. HumanMessage - user responds with the next conversational turn.

        (... repeat steps 2-5 as needed ... )
    
    Merges two lists of messages, updating existing messages by ID.

    By default, this ensures the state is "append-only", unless the
    new message has the same ID as an existing message.

    Returns:
        A new list of messages with the messages from `right` merged into `left`.
        If a message in `right` has the same ID as a message in `left`, the
        message from `right` will replace the message from `left`."""


class Router(TypedDict):
    """Classify user query."""

    logic: str
    type: Literal["more-info", "langchain", "general"]


# This is the primary state of your agent, where you can store any information


@dataclass(kw_only=True)
class AgentState(InputState):
    """State of the retrieval graph / agent."""

    router: Router = field(default_factory=lambda: Router(type="general", logic=""))
    """The router's classification of the user's query."""
    steps: list[str] = field(default_factory=list)
    """A list of steps in the research plan."""
    documents: Annotated[list[Document], reduce_docs] = field(default_factory=list)
    """Populated by the retriever. This is a list of documents that the agent can reference."""
    answer: str = field(default="")
    """Final answer. Useful for evaluations"""
    query: str = field(default="")



================================================
FILE: backend/retrieval_graph/researcher_graph/__init__.py
================================================
"""Researcher Graph Module."""



================================================
FILE: backend/retrieval_graph/researcher_graph/graph.py
================================================
"""Researcher graph used in the conversational retrieval system as a subgraph.

This module defines the core structure and functionality of the researcher graph,
which is responsible for generating search queries and retrieving relevant documents.
"""

from typing import cast

from langchain_core.documents import Document
from langchain_core.runnables import RunnableConfig
from langgraph.constants import Send
from langgraph.graph import END, START, StateGraph
from typing_extensions import TypedDict

from backend import retrieval
from backend.retrieval_graph.configuration import AgentConfiguration
from backend.retrieval_graph.researcher_graph.state import QueryState, ResearcherState
from backend.utils import load_chat_model


async def generate_queries(
    state: ResearcherState, *, config: RunnableConfig
) -> dict[str, list[str]]:
    """Generate search queries based on the question (a step in the research plan).

    This function uses a language model to generate diverse search queries to help answer the question.

    Args:
        state (ResearcherState): The current state of the researcher, including the user's question.
        config (RunnableConfig): Configuration with the model used to generate queries.

    Returns:
        dict[str, list[str]]: A dictionary with a 'queries' key containing the list of generated search queries.
    """

    class Response(TypedDict):
        queries: list[str]

    configuration = AgentConfiguration.from_runnable_config(config)
    structured_output_kwargs = (
        {"method": "function_calling"} if "openai" in configuration.query_model else {}
    )
    model = load_chat_model(configuration.query_model).with_structured_output(
        Response, **structured_output_kwargs
    )
    messages = [
        {"role": "system", "content": configuration.generate_queries_system_prompt},
        {"role": "human", "content": state.question},
    ]
    response = cast(
        Response, await model.ainvoke(messages, {"tags": ["langsmith:nostream"]})
    )
    return {"queries": response["queries"]}


async def retrieve_documents(
    state: QueryState, *, config: RunnableConfig
) -> dict[str, list[Document]]:
    """Retrieve documents based on a given query.

    This function uses a retriever to fetch relevant documents for a given query.

    Args:
        state (QueryState): The current state containing the query string.
        config (RunnableConfig): Configuration with the retriever used to fetch documents.

    Returns:
        dict[str, list[Document]]: A dictionary with a 'documents' key containing the list of retrieved documents.
    """
    with retrieval.make_retriever(config) as retriever:
        response = await retriever.ainvoke(state.query, config)
        return {"documents": response}


def retrieve_in_parallel(state: ResearcherState) -> list[Send]:
    """Create parallel retrieval tasks for each generated query.

    This function prepares parallel document retrieval tasks for each query in the researcher's state.

    Args:
        state (ResearcherState): The current state of the researcher, including the generated queries.

    Returns:
        Literal["retrieve_documents"]: A list of Send objects, each representing a document retrieval task.

    Behavior:
        - Creates a Send object for each query in the state.
        - Each Send object targets the "retrieve_documents" node with the corresponding query.
    """
    return [
        Send("retrieve_documents", QueryState(query=query)) for query in state.queries
    ]


# Define the graph
builder = StateGraph(ResearcherState)
builder.add_node(generate_queries)
builder.add_node(retrieve_documents)
builder.add_edge(START, "generate_queries")
builder.add_conditional_edges(
    "generate_queries",
    retrieve_in_parallel,  # type: ignore
    path_map=["retrieve_documents"],
)
builder.add_edge("retrieve_documents", END)
# Compile into a graph object that you can invoke and deploy.
graph = builder.compile()
graph.name = "ResearcherGraph"



================================================
FILE: backend/retrieval_graph/researcher_graph/state.py
================================================
"""State management for the researcher graph.

This module defines the state structures used in the researcher graph.
"""

from dataclasses import dataclass, field
from typing import Annotated

from langchain_core.documents import Document

from backend.utils import reduce_docs


@dataclass(kw_only=True)
class QueryState:
    """Private state for the retrieve_documents node in the researcher graph."""

    query: str


@dataclass(kw_only=True)
class ResearcherState:
    """State of the researcher graph / agent."""

    question: str
    """A step in the research plan generated by the retriever agent."""
    queries: list[str] = field(default_factory=list)
    """A list of search queries based on the question that the researcher generates."""
    documents: Annotated[list[Document], reduce_docs] = field(default_factory=list)
    """Populated by the retriever. This is a list of documents that the agent can reference."""



================================================
FILE: backend/tests/evals/test_e2e.py
================================================
import asyncio
from typing import Any

import pandas as pd
from langchain_core.documents import Document
from langchain_core.messages import AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langsmith.evaluation import EvaluationResults, aevaluate
from langsmith.schemas import Example, Run
from pydantic import BaseModel, Field

from backend.retrieval_graph.graph import graph
from backend.utils import format_docs, load_chat_model

DATASET_NAME = "chat-langchain-qa"
EXPERIMENT_PREFIX = "chat-langchain-ci"

SCORE_RETRIEVAL_RECALL = "retrieval_recall"
SCORE_ANSWER_CORRECTNESS = "answer_correctness_score"
SCORE_ANSWER_VS_CONTEXT_CORRECTNESS = "answer_vs_context_correctness_score"

JUDGE_MODEL_NAME = "anthropic/claude-3-5-haiku-20241022"

judge_llm = load_chat_model(JUDGE_MODEL_NAME)


# Evaluate retrieval


def evaluate_retrieval_recall(run: Run, example: Example) -> dict:
    documents: list[Document] = run.outputs.get("documents") or []
    sources = [doc.metadata["source"] for doc in documents]
    expected_sources = set(example.outputs.get("sources") or [])
    # NOTE: since we're currently assuming only ~1 correct document per question
    # this score is equivalent to recall @K where K is number of retrieved documents
    score = float(any(source in expected_sources for source in sources))
    return {"key": SCORE_RETRIEVAL_RECALL, "score": score}


# QA Evaluation Schema


class GradeAnswer(BaseModel):
    """Evaluate correctness of the answer and assign a continuous score."""

    reason: str = Field(
        description="1-2 short sentences with the reason why the score was assigned"
    )
    score: float = Field(
        description="Score that shows how correct the answer is. Use 1.0 if completely correct and 0.0 if completely incorrect",
        minimum=0.0,
        maximum=1.0,
    )


# Evaluate the answer based on the reference answers


QA_SYSTEM_PROMPT = """You are an expert programmer and problem-solver, tasked with grading answers to questions about Langchain.
You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.

Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements."""

QA_PROMPT = ChatPromptTemplate.from_messages(
    [
        ("system", QA_SYSTEM_PROMPT),
        (
            "human",
            "QUESTION: \n\n {question} \n\n TRUE ANSWER: {true_answer} \n\n STUDENT ANSWER: {answer}",
        ),
    ]
)

qa_chain = QA_PROMPT | judge_llm.with_structured_output(GradeAnswer)


def evaluate_qa(run: Run, example: Example) -> dict:
    messages = run.outputs.get("messages") or []
    if not messages:
        return {"score": 0.0}

    last_message = messages[-1]
    if not isinstance(last_message, AIMessage):
        return {"score": 0.0}

    score: GradeAnswer = qa_chain.invoke(
        {
            "question": example.inputs["question"],
            "true_answer": example.outputs["answer"],
            "answer": last_message.content,
        }
    )
    return {"key": SCORE_ANSWER_CORRECTNESS, "score": float(score.score)}


# Evaluate the answer based on the provided context

CONTEXT_QA_SYSTEM_PROMPT = """You are an expert programmer and problem-solver, tasked with grading answers to questions about Langchain.
You are given a question, the context for answering the question, and the student's answer. You are asked to score the student's answer as either CORRECT or INCORRECT, based on the context.

Grade the student answer BOTH based on its factual accuracy AND on whether it is supported by the context. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements."""

CONTEXT_QA_PROMPT = ChatPromptTemplate.from_messages(
    [
        ("system", CONTEXT_QA_SYSTEM_PROMPT),
        (
            "human",
            "QUESTION: \n\n {question} \n\n CONTEXT: {context} \n\n STUDENT ANSWER: {answer}",
        ),
    ]
)

context_qa_chain = CONTEXT_QA_PROMPT | judge_llm.with_structured_output(GradeAnswer)


def evaluate_qa_context(run: Run, example: Example) -> dict:
    messages = run.outputs.get("messages") or []
    if not messages:
        return {"score": 0.0}

    documents = run.outputs.get("documents") or []
    if not documents:
        return {"score": 0.0}

    context = format_docs(documents)

    last_message = messages[-1]
    if not isinstance(last_message, AIMessage):
        return {"score": 0.0}

    score: GradeAnswer = context_qa_chain.invoke(
        {
            "question": example.inputs["question"],
            "context": context,
            "answer": last_message.content,
        }
    )
    return {"key": SCORE_ANSWER_VS_CONTEXT_CORRECTNESS, "score": float(score.score)}


# Run evaluation


async def run_graph(inputs: dict[str, Any]) -> dict[str, Any]:
    results = await graph.ainvoke(
        {
            "messages": [("human", inputs["question"])],
        }
    )
    return results


# Check results


def convert_single_example_results(evaluation_results: EvaluationResults):
    converted = {}
    for r in evaluation_results["results"]:
        converted[r.key] = r.score
    return converted


# NOTE: this is more of a regression test
def test_scores_regression():
    # test most commonly used model
    experiment_results = asyncio.run(
        aevaluate(
            run_graph,
            data=DATASET_NAME,
            evaluators=[evaluate_retrieval_recall, evaluate_qa, evaluate_qa_context],
            experiment_prefix=EXPERIMENT_PREFIX,
            metadata={"judge_model_name": JUDGE_MODEL_NAME},
            max_concurrency=4,
        )
    )
    experiment_result_df = pd.DataFrame(
        convert_single_example_results(result["evaluation_results"])
        for result in experiment_results._results
    )
    average_scores = experiment_result_df.mean()

    assert average_scores[SCORE_RETRIEVAL_RECALL] >= 0.65
    assert average_scores[SCORE_ANSWER_CORRECTNESS] >= 0.9
    assert average_scores[SCORE_ANSWER_VS_CONTEXT_CORRECTNESS] >= 0.9



================================================
FILE: frontend/components.json
================================================
{
  "$schema": "https://ui.shadcn.com/schema.json",
  "style": "new-york",
  "rsc": true,
  "tsx": true,
  "tailwind": {
    "config": "tailwind.config.ts",
    "css": "app/globals.css",
    "baseColor": "neutral",
    "cssVariables": true,
    "prefix": ""
  },
  "aliases": {
    "components": "@/components",
    "utils": "@/utils/cn",
    "ui": "@/components/ui",
    "lib": "@/utils",
    "hooks": "@/hooks"
  }
}



================================================
FILE: frontend/next.config.js
================================================
/** @type {import('next').NextConfig} */
const nextConfig = {};

module.exports = nextConfig;



================================================
FILE: frontend/package.json
================================================
{
  "name": "frontend",
  "version": "0.1.0",
  "private": true,
  "packageManager": "yarn@1.22.19",
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint",
    "format": "prettier --write ."
  },
  "dependencies": {
    "@assistant-ui/react": "^0.5.75",
    "@assistant-ui/react-markdown": "^0.2.19",
    "@assistant-ui/react-syntax-highlighter": "^0.0.14",
    "@chakra-ui/icons": "^2.1.0",
    "@chakra-ui/react": "^2.8.1",
    "@emotion/react": "^11.11.1",
    "@emotion/styled": "^11.11.0",
    "@langchain/core": "^0.3.10",
    "@langchain/langgraph-sdk": "^0.0.20",
    "@radix-ui/react-collapsible": "^1.1.1",
    "@radix-ui/react-dialog": "^1.1.2",
    "@radix-ui/react-dropdown-menu": "^2.1.1",
    "@radix-ui/react-icons": "^1.3.0",
    "@radix-ui/react-progress": "^1.1.0",
    "@radix-ui/react-select": "^2.1.2",
    "@radix-ui/react-slot": "^1.1.0",
    "@radix-ui/react-toast": "^1.2.1",
    "@types/dompurify": "^3.0.5",
    "@types/lodash.orderby": "^4.6.9",
    "@types/marked": "^5.0.1",
    "@types/node": "20.4.9",
    "@types/react": "18.2.20",
    "@types/react-dom": "18.2.7",
    "autoprefixer": "10.4.14",
    "class-variance-authority": "^0.7.0",
    "clsx": "^2.1.1",
    "date-fns": "^4.1.0",
    "dompurify": "^3.0.8",
    "embla-carousel-react": "^8.3.0",
    "emojisplosion": "^2.6.1",
    "eslint": "8.46.0",
    "eslint-config-next": "13.4.13",
    "framer-motion": "^10.16.4",
    "graphql": "^16.8.1",
    "highlight.js": "^11.8.0",
    "js-cookie": "^3.0.5",
    "langsmith": "^0.0.41",
    "lodash.orderby": "^4.6.0",
    "lucide-react": "^0.452.0",
    "marked": "^7.0.2",
    "next": "14.2.25",
    "nuqs": "^2.4.1",
    "postcss": "8.4.31",
    "react": "18.2.0",
    "react-dom": "18.2.0",
    "react-query": "^3.39.3",
    "react-syntax-highlighter": "^15.5.0",
    "react-textarea-autosize": "^8.5.3",
    "react-toastify": "^9.1.3",
    "rehype-katex": "^7.0.1",
    "remark-gfm": "^4.0.0",
    "remark-math": "^6.0.0",
    "tailwind-merge": "^2.5.3",
    "tailwindcss": "3.3.3",
    "tailwindcss-animate": "^1.0.7",
    "typescript": "5.1.6"
  },
  "devDependencies": {
    "@types/js-cookie": "^3.0.6",
    "@types/react-syntax-highlighter": "^15.5.13",
    "prettier": "^3.0.3",
    "tailwind-scrollbar": "^3.1.0",
    "typescript": "5.1.6",
    "weaviate-ts-client": "^1.5.0"
  }
}



================================================
FILE: frontend/postcss.config.js
================================================
module.exports = {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
};



================================================
FILE: frontend/tailwind.config.ts
================================================
import type { Config } from "tailwindcss";

const config: Config = {
  darkMode: ["class"],
  content: [
    "./pages/**/*.{js,ts,jsx,tsx,mdx}",
    "./components/**/*.{js,ts,jsx,tsx,mdx}",
    "./app/**/*.{js,ts,jsx,tsx,mdx}",
  ],
  theme: {
    extend: {
      backgroundImage: {
        "gradient-radial": "radial-gradient(var(--tw-gradient-stops))",
        "gradient-conic":
          "conic-gradient(from 180deg at 50% 50%, var(--tw-gradient-stops))",
      },
      borderRadius: {
        lg: "var(--radius)",
        md: "calc(var(--radius) - 2px)",
        sm: "calc(var(--radius) - 4px)",
      },
      colors: {
        background: "hsl(var(--background))",
        foreground: "hsl(var(--foreground))",
        card: {
          DEFAULT: "hsl(var(--card))",
          foreground: "hsl(var(--card-foreground))",
        },
        popover: {
          DEFAULT: "hsl(var(--popover))",
          foreground: "hsl(var(--popover-foreground))",
        },
        primary: {
          DEFAULT: "hsl(var(--primary))",
          foreground: "hsl(var(--primary-foreground))",
        },
        secondary: {
          DEFAULT: "hsl(var(--secondary))",
          foreground: "hsl(var(--secondary-foreground))",
        },
        muted: {
          DEFAULT: "hsl(var(--muted))",
          foreground: "hsl(var(--muted-foreground))",
        },
        accent: {
          DEFAULT: "hsl(var(--accent))",
          foreground: "hsl(var(--accent-foreground))",
        },
        destructive: {
          DEFAULT: "hsl(var(--destructive))",
          foreground: "hsl(var(--destructive-foreground))",
        },
        border: "hsl(var(--border))",
        input: "hsl(var(--input))",
        ring: "hsl(var(--ring))",
        chart: {
          "1": "hsl(var(--chart-1))",
          "2": "hsl(var(--chart-2))",
          "3": "hsl(var(--chart-3))",
          "4": "hsl(var(--chart-4))",
          "5": "hsl(var(--chart-5))",
        },
      },
    },
  },
  plugins: [require("tailwind-scrollbar"), require("tailwindcss-animate")],
};
export default config;



================================================
FILE: frontend/tsconfig.json
================================================
{
  "compilerOptions": {
    "target": "es5",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "forceConsistentCasingInFileNames": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./*"]
    }
  },
  "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx", ".next/types/**/*.ts"],
  "exclude": ["node_modules"]
}



================================================
FILE: frontend/vercel.json
================================================
{
  "git": {
    "deploymentEnabled": {
      "main": false
    }
  }
}



================================================
FILE: frontend/.env.example
================================================
## For JS backend:

# LANGCHAIN_TRACING_V2=true
# LANGCHAIN_ENDPOINT="https://api.smith.langchain.com"
# LANGCHAIN_API_KEY="YOUR_LANGSMITH_KEY"
# LANGCHAIN_PROJECT="YOUR_PROJECT_NAME"

# NEXT_PUBLIC_API_BASE_URL="http://localhost:3000/api"
# OPENAI_API_KEY="YOUR_OPENAI_API_KEY"
# WEAVIATE_HOST="YOUR_WEAVIATE_HOST"
# WEAVIATE_API_KEY="YOUR_WEAVIATE_API_KEY"
# WEAVIATE_INDEX_NAME="YOUR_WEAVIATE_INDEX_NAME"


================================================
FILE: frontend/.eslintrc.json
================================================
{
  "extends": "next/core-web-vitals"
}



================================================
FILE: frontend/.prettierrc
================================================
{
  "endOfLine": "lf"
}



================================================
FILE: frontend/.yarnrc.yml
================================================
nodeLinker: node-modules



================================================
FILE: frontend/app/globals.css
================================================
@tailwind base;
@tailwind components;
@tailwind utilities;

body {
  color: #f8f8f8;
  background: #131318;
  max-height: 100vh;
  overflow-x: hidden;
}

body input,
body textarea {
  color: black;
}

a {
  color: #2d7bd4;
}

a:hover {
  border-bottom: 1px solid;
}

p {
  margin: 8px 0;
}

code {
  color: #ffa500;
}

li {
  padding: 4px;
}

@layer base {
  :root {
    --background: 0 0% 100%;
    --foreground: 0 0% 3.9%;
    --card: 0 0% 100%;
    --card-foreground: 0 0% 3.9%;
    --popover: 0 0% 100%;
    --popover-foreground: 0 0% 3.9%;
    --primary: 0 0% 9%;
    --primary-foreground: 0 0% 98%;
    --secondary: 0 0% 96.1%;
    --secondary-foreground: 0 0% 9%;
    --muted: 0 0% 96.1%;
    --muted-foreground: 0 0% 45.1%;
    --accent: 0 0% 96.1%;
    --accent-foreground: 0 0% 9%;
    --destructive: 0 84.2% 60.2%;
    --destructive-foreground: 0 0% 98%;
    --border: 0 0% 89.8%;
    --input: 0 0% 89.8%;
    --ring: 0 0% 3.9%;
    --chart-1: 12 76% 61%;
    --chart-2: 173 58% 39%;
    --chart-3: 197 37% 24%;
    --chart-4: 43 74% 66%;
    --chart-5: 27 87% 67%;
    --radius: 0.5rem;
  }
  .dark {
    --background: 0 0% 3.9%;
    --foreground: 0 0% 98%;
    --card: 0 0% 3.9%;
    --card-foreground: 0 0% 98%;
    --popover: 0 0% 3.9%;
    --popover-foreground: 0 0% 98%;
    --primary: 0 0% 98%;
    --primary-foreground: 0 0% 9%;
    --secondary: 0 0% 14.9%;
    --secondary-foreground: 0 0% 98%;
    --muted: 0 0% 14.9%;
    --muted-foreground: 0 0% 63.9%;
    --accent: 0 0% 14.9%;
    --accent-foreground: 0 0% 98%;
    --destructive: 0 62.8% 30.6%;
    --destructive-foreground: 0 0% 98%;
    --border: 0 0% 14.9%;
    --input: 0 0% 14.9%;
    --ring: 0 0% 83.1%;
    --chart-1: 220 70% 50%;
    --chart-2: 160 60% 45%;
    --chart-3: 30 80% 55%;
    --chart-4: 280 65% 60%;
    --chart-5: 340 75% 55%;
  }
}

@layer base {
  * {
    @apply border-border;
  }
  body {
    @apply bg-background text-foreground;
  }
}



================================================
FILE: frontend/app/layout.tsx
================================================
import "./globals.css";
import type { Metadata } from "next";
import { Inter } from "next/font/google";
import { NuqsAdapter } from "nuqs/adapters/next/app";

const inter = Inter({ subsets: ["latin"] });

export const metadata: Metadata = {
  title: "Chat LangChain",
  description: "Chatbot for LangChain",
};

export default function RootLayout({
  children,
}: {
  children: React.ReactNode;
}) {
  return (
    <html lang="en" className="h-full">
      <body className={`${inter.className} h-full`}>
        <div
          className="flex flex-col h-full w-full"
          style={{ background: "rgb(38, 38, 41)" }}
        >
          <NuqsAdapter>{children}</NuqsAdapter>
        </div>
      </body>
    </html>
  );
}



================================================
FILE: frontend/app/page.tsx
================================================
"use client";

import React from "react";
import { GraphProvider } from "./contexts/GraphContext";
import { ChatLangChain } from "./components/ChatLangChain";

export default function Page(): React.ReactElement {
  return (
    <main className="w-full h-full">
      <React.Suspense fallback={null}>
        <GraphProvider>
          <ChatLangChain />
        </GraphProvider>
      </React.Suspense>
    </main>
  );
}



================================================
FILE: frontend/app/types.ts
================================================
export type Source = {
  url: string;
  title: string;
};

export type Message = {
  id: string;
  createdAt?: Date;
  content: string;
  type: "system" | "human" | "ai" | "function";
  sources?: Source[];
  name?: string;
  function_call?: { name: string };
};

export type Feedback = {
  feedback_id: string;
  score: number;
  comment?: string;
};

export type ModelOptions =
  | "openai/gpt-4.1-mini"
  | "anthropic/claude-3-5-haiku-20241022"
  | "google_genai/gemini-2.0-flash";



================================================
FILE: frontend/app/api/[..._path]/route.ts
================================================
import { NextRequest, NextResponse } from "next/server";

export const runtime = "edge";

function getCorsHeaders() {
  return {
    "Access-Control-Allow-Origin": "*",
    "Access-Control-Allow-Methods": "GET, POST, PUT, PATCH, DELETE, OPTIONS",
    "Access-Control-Allow-Headers": "*",
  };
}

async function handleRequest(req: NextRequest, method: string) {
  try {
    const path = req.nextUrl.pathname.replace(/^\/?api\//, "");
    const url = new URL(req.url);
    const searchParams = new URLSearchParams(url.search);
    searchParams.delete("_path");
    searchParams.delete("nxtP_path");
    const queryString = searchParams.toString()
      ? `?${searchParams.toString()}`
      : "";

    const options: RequestInit = {
      method,
      headers: {
        "x-api-key": process.env.LANGCHAIN_API_KEY || "",
      },
    };

    if (["POST", "PUT", "PATCH"].includes(method)) {
      options.body = await req.text();
    }

    const res = await fetch(
      `${process.env.API_BASE_URL}/${path}${queryString}`,
      options,
    );

    return new NextResponse(res.body, {
      status: res.status,
      statusText: res.statusText,
      headers: {
        ...res.headers,
        ...getCorsHeaders(),
      },
    });
  } catch (e: any) {
    return NextResponse.json({ error: e.message }, { status: e.status ?? 500 });
  }
}

export const GET = (req: NextRequest) => handleRequest(req, "GET");
export const POST = (req: NextRequest) => handleRequest(req, "POST");
export const PUT = (req: NextRequest) => handleRequest(req, "PUT");
export const PATCH = (req: NextRequest) => handleRequest(req, "PATCH");
export const DELETE = (req: NextRequest) => handleRequest(req, "DELETE");

// Add a new OPTIONS handler
export const OPTIONS = () => {
  return new NextResponse(null, {
    status: 204,
    headers: {
      ...getCorsHeaders(),
    },
  });
};



================================================
FILE: frontend/app/api/runs/feedback/route.ts
================================================
import { Client, Feedback } from "langsmith";
import { NextRequest, NextResponse } from "next/server";

export async function POST(req: NextRequest) {
  try {
    const body = await req.json();
    const { runId, feedbackKey, score, comment } = body;

    if (!runId || !feedbackKey) {
      return NextResponse.json(
        { error: "`runId` and `feedbackKey` are required." },
        { status: 400 },
      );
    }

    const lsClient = new Client({
      apiKey: process.env.LANGCHAIN_API_KEY,
    });

    const feedback = await lsClient.createFeedback(runId, feedbackKey, {
      score,
      comment,
    });

    return NextResponse.json(
      { success: true, feedback: feedback },
      { status: 200 },
    );
  } catch (error) {
    console.error("Failed to process feedback request:", error);

    return NextResponse.json(
      { error: "Failed to submit feedback." },
      { status: 500 },
    );
  }
}

export async function GET(req: NextRequest) {
  try {
    const searchParams = req.nextUrl.searchParams;
    const runId = searchParams.get("runId");
    const feedbackKey = searchParams.get("feedbackKey");

    if (!runId || !feedbackKey) {
      return new NextResponse(
        JSON.stringify({
          error: "`runId` and `feedbackKey` are required.",
        }),
        {
          status: 400,
          headers: { "Content-Type": "application/json" },
        },
      );
    }

    const lsClient = new Client({
      apiKey: process.env.LANGCHAIN_API_KEY,
    });

    const runFeedback: Feedback[] = [];

    const run_feedback = lsClient.listFeedback({
      runIds: [runId],
      feedbackKeys: [feedbackKey],
    });

    for await (const feedback of run_feedback) {
      runFeedback.push(feedback);
    }

    return new NextResponse(
      JSON.stringify({
        feedback: runFeedback,
      }),
      {
        status: 200,
        headers: { "Content-Type": "application/json" },
      },
    );
  } catch (error) {
    console.error("Failed to fetch feedback:", error);
    return NextResponse.json(
      { error: "Failed to fetch feedback." },
      { status: 500 },
    );
  }
}



================================================
FILE: frontend/app/api/runs/share/route.ts
================================================
import { NextRequest, NextResponse } from "next/server";
import { Client } from "langsmith";

const MAX_RETRIES = 5;
const RETRY_DELAY = 5000; // 5 seconds

async function shareRunWithRetry(
  lsClient: Client,
  runId: string,
): Promise<string> {
  for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
    try {
      return await lsClient.shareRun(runId);
    } catch (error) {
      if (attempt === MAX_RETRIES) {
        throw error;
      }
      console.warn(
        `Attempt ${attempt} failed. Retrying in ${RETRY_DELAY / 1000} seconds...`,
      );
      await new Promise((resolve) => setTimeout(resolve, RETRY_DELAY));
    }
  }
  throw new Error("Max retries reached"); // This line should never be reached due to the throw in the loop
}

export async function POST(req: NextRequest) {
  const { runId } = await req.json();

  if (!runId) {
    return new NextResponse(
      JSON.stringify({
        error: "`runId` is required to share run.",
      }),
      {
        status: 400,
        headers: { "Content-Type": "application/json" },
      },
    );
  }

  const lsClient = new Client({
    apiKey: process.env.LANGCHAIN_API_KEY,
  });

  try {
    const sharedRunURL = await shareRunWithRetry(lsClient, runId);

    return new NextResponse(JSON.stringify({ sharedRunURL }), {
      status: 200,
      headers: { "Content-Type": "application/json" },
    });
  } catch (error) {
    console.error(
      `Failed to share run with id ${runId} after ${MAX_RETRIES} attempts:\n`,
      error,
    );
    return new NextResponse(
      JSON.stringify({ error: "Failed to share run after multiple attempts." }),
      {
        status: 500,
        headers: { "Content-Type": "application/json" },
      },
    );
  }
}



================================================
FILE: frontend/app/components/AnswerHeaderToolUI.tsx
================================================
import { useAssistantToolUI } from "@assistant-ui/react";
import { BrainCog } from "lucide-react";

export const useAnswerHeaderToolUI = () =>
  useAssistantToolUI({
    toolName: "answer_header",
    render: (_) => {
      return (
        <div className="flex flex-row gap-2 items-center justify-start pb-4 text-gray-300">
          <BrainCog className="w-5 h-5" />
          <p className="text-xl">Answer</p>
        </div>
      );
    },
  });



================================================
FILE: frontend/app/components/ChatLangChain.tsx
================================================
"use client";

import React, { useEffect, useRef, useState } from "react";
import {
  AppendMessage,
  AssistantRuntimeProvider,
  useExternalStoreRuntime,
} from "@assistant-ui/react";
import { v4 as uuidv4 } from "uuid";
import { useExternalMessageConverter } from "@assistant-ui/react";
import { BaseMessage, HumanMessage } from "@langchain/core/messages";
import { useToast } from "../hooks/use-toast";
import {
  convertToOpenAIFormat,
  convertLangchainMessages,
} from "../utils/convert_messages";
import { ThreadChat } from "./chat-interface";
import { SelectModel } from "./SelectModel";
import { ThreadHistory } from "./thread-history";
import { Toaster } from "./ui/toaster";
import { useGraphContext } from "../contexts/GraphContext";
import { useQueryState } from "nuqs";

function ChatLangChainComponent(): React.ReactElement {
  const { toast } = useToast();
  const { threadsData, userData, graphData } = useGraphContext();
  const { userId } = userData;
  const { getUserThreads, createThread, getThreadById } = threadsData;
  const { messages, setMessages, streamMessage, switchSelectedThread } =
    graphData;
  const [isRunning, setIsRunning] = useState(false);
  const [threadId, setThreadId] = useQueryState("threadId");

  const hasCheckedThreadIdParam = useRef(false);
  useEffect(() => {
    if (typeof window === "undefined" || hasCheckedThreadIdParam.current)
      return;
    if (!threadId) {
      hasCheckedThreadIdParam.current = true;
      return;
    }

    hasCheckedThreadIdParam.current = true;

    try {
      getThreadById(threadId).then((thread) => {
        if (!thread) {
          setThreadId(null);
          return;
        }

        switchSelectedThread(thread);
      });
    } catch (e) {
      console.error("Failed to fetch thread in query param", e);
      setThreadId(null);
    }
  }, [threadId]);

  const isSubmitDisabled = !userId;

  async function onNew(message: AppendMessage): Promise<void> {
    if (isSubmitDisabled) {
      toast({
        title: "Failed to send message",
        description: "Unable to find user ID. Please try again later.",
      });
      return;
    }
    if (message.content[0]?.type !== "text") {
      throw new Error("Only text messages are supported");
    }

    setIsRunning(true);

    let currentThreadId = threadId;
    if (!currentThreadId) {
      const thread = await createThread(userId);
      if (!thread) {
        toast({
          title: "Error",
          description: "Thread creation failed.",
        });
        return;
      }
      setThreadId(thread.thread_id);
      currentThreadId = thread.thread_id;
    }

    try {
      const humanMessage = new HumanMessage({
        content: message.content[0].text,
        id: uuidv4(),
      });

      setMessages((prevMessages) => [...prevMessages, humanMessage]);

      await streamMessage(currentThreadId, {
        messages: [convertToOpenAIFormat(humanMessage)],
      });
    } finally {
      setIsRunning(false);
      // Re-fetch threads so that the current thread's title is updated.
      await getUserThreads(userId);
    }
  }

  const threadMessages = useExternalMessageConverter<BaseMessage>({
    callback: convertLangchainMessages,
    messages: messages,
    isRunning,
  });

  const runtime = useExternalStoreRuntime({
    messages: threadMessages,
    isRunning,
    onNew,
  });

  return (
    <div className="overflow-hidden w-full flex md:flex-row flex-col relative">
      {messages.length > 0 ? (
        <div className="absolute top-4 right-4 z-10">
          <SelectModel />
        </div>
      ) : null}
      <div>
        <ThreadHistory />
      </div>
      <div className="w-full h-full overflow-hidden">
        <AssistantRuntimeProvider runtime={runtime}>
          <ThreadChat submitDisabled={isSubmitDisabled} messages={messages} />
        </AssistantRuntimeProvider>
      </div>
      <Toaster />
    </div>
  );
}

export const ChatLangChain = React.memo(ChatLangChainComponent);



================================================
FILE: frontend/app/components/DocumentCard.tsx
================================================
import { Card, CardContent, CardHeader, CardTitle } from "./ui/card";

export type Document = {
  page_content: string;
  metadata: Record<string, any>;
};

export const DocumentCard = ({ document }: { document: Document }) => {
  const description = document.page_content.slice(0, 250);

  return (
    <Card className="md:w-[200px] sm:w-[200px] w-full h-[110px] bg-inherit border-gray-500 flex flex-col">
      <CardHeader className="flex-shrink-0 px-3 pt-2 pb-0">
        <CardTitle className="text-sm font-light text-gray-300 line-clamp-1 overflow-hidden p-[-24px]">
          {document.metadata.title}
        </CardTitle>
      </CardHeader>
      <CardContent className="flex flex-col px-3 flex-grow justify-between">
        <p className="text-xs font-light text-gray-400 line-clamp-4 overflow-hidden">
          {description}
        </p>
      </CardContent>
    </Card>
  );
};



================================================
FILE: frontend/app/components/DocumentDialog.tsx
================================================
import { SquareArrowOutUpRight, File } from "lucide-react";
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogHeader,
  DialogTitle,
  DialogTrigger,
} from "../components/ui/dialog";
import { TooltipIconButton } from "./ui/assistant-ui/tooltip-icon-button";

interface DocumentDialogProps {
  document: Record<string, any>;
  trigger?: React.ReactNode;
}

export function DocumentDialog(props: DocumentDialogProps) {
  const trigger = props.trigger || (
    <TooltipIconButton
      tooltip={props.document.metadata.title}
      variant="outline"
      className="w-6 h-6 z-50 transition-colors ease-in-out bg-transparent hover:bg-gray-500 border-gray-400 text-gray-300"
    >
      <File />
    </TooltipIconButton>
  );

  return (
    <Dialog>
      <DialogTrigger asChild={!props.trigger}>{trigger}</DialogTrigger>
      <DialogContent className="max-w-3xl max-h-[80vh] overflow-y-auto bg-gray-700 text-gray-200">
        <DialogHeader>
          <DialogTitle className="flex items-center justify-start gap-4">
            <p className="text-gray-100 break-words">
              {props.document.metadata.title}
            </p>
            <div className="flex flex-wrap justify-start">
              <a
                href={props.document.metadata.source}
                target="_blank"
                rel="noopener noreferrer"
                className="flex items-center text-blue-400 hover:text-blue-300 transition-colors duration-200 break-all"
              >
                Source{" "}
                <SquareArrowOutUpRight className="ml-1 h-4 w-4 flex-shrink-0" />
              </a>
            </div>
          </DialogTitle>
        </DialogHeader>
        <DialogDescription className="text-gray-300 break-words whitespace-normal">
          {props.document.metadata.description}
        </DialogDescription>

        <hr />
        <div className="mt-2 overflow-hidden">
          <p className="whitespace-pre-wrap text-gray-200 break-words overflow-wrap-anywhere">
            {props.document.page_content}
          </p>
        </div>
      </DialogContent>
    </Dialog>
  );
}



================================================
FILE: frontend/app/components/GeneratingQuestionsToolUI.tsx
================================================
import { useAssistantToolUI } from "@assistant-ui/react";
import { Card, CardContent, CardHeader, CardTitle } from "./ui/card";
import { LoaderCircle, Globe, Plus } from "lucide-react";
import { DocumentDialog } from "./DocumentDialog";
import {
  Tooltip,
  TooltipContent,
  TooltipProvider,
  TooltipTrigger,
} from "./ui/tooltip";
import { Sheet, SheetContent, SheetTrigger } from "./ui/sheet";
import { TooltipIconButton } from "./ui/assistant-ui/tooltip-icon-button";
import { DocumentCard, Document } from "./DocumentCard";
import { useCallback } from "react";

type Question = {
  question: string;
  step: number;
  // Not rendered in the UI ATM.
  queries?: string[];
  documents?: Document[];
};

const QuestionCard = ({ question }: { question: Question }) => {
  const displayedDocuments = question.documents?.slice(0, 6) || [];
  const remainingDocuments = question.documents?.slice(6) || [];

  return (
    <Card className="md:w-[250px] sm:w-[250px] w-full md:max-w-full h-[140px] bg-inherit border-gray-500 flex flex-col gap-2">
      <CardHeader className="flex-shrink-0 px-3 pt-2 pb-0">
        <TooltipProvider>
          <Tooltip>
            <TooltipTrigger asChild>
              <CardTitle className="text-sm font-light text-gray-300 line-clamp-4 overflow-hidden">
                {question.question}
              </CardTitle>
            </TooltipTrigger>
            <TooltipContent className="max-w-[600px] whitespace-pre-wrap">
              <p>{question.question}</p>
            </TooltipContent>
          </Tooltip>
        </TooltipProvider>
      </CardHeader>
      <CardContent className="flex flex-col flex-grow px-3 pb-2 justify-between mt-auto">
        <div className="flex flex-col gap-1 mt-auto">
          <hr className="border-gray-400" />
          <div className="flex flex-wrap items-start justify-start gap-2 pt-1">
            {question.documents?.length ? (
              <>
                {displayedDocuments.map((doc: Document, idx: number) => (
                  <DocumentDialog
                    key={`document-${question.step}-${idx}`}
                    document={doc}
                  />
                ))}
                {remainingDocuments.length > 0 && (
                  <Sheet>
                    <SheetTrigger>
                      <TooltipIconButton
                        tooltip={`See ${remainingDocuments.length} more documents`}
                        variant="outline"
                        className="w-6 h-6 z-50 transition-colors ease-in-out bg-transparent hover:bg-gray-500 border-gray-400 text-gray-300"
                      >
                        <Plus />
                      </TooltipIconButton>
                    </SheetTrigger>
                    <SheetContent
                      side="right"
                      className="bg-[#282828] border-none overflow-y-auto flex-grow scrollbar-thin scrollbar-thumb-gray-600 scrollbar-track-transparent md:min-w-[50vw] min-w-[70vw]"
                    >
                      <div className="flex flex-col gap-4">
                        <h2 className="text-xl font-semibold text-gray-300">
                          All Documents for Question
                        </h2>
                        <div className="flex flex-wrap gap-2">
                          {question.documents?.map(
                            (doc: Document, idx: number) => (
                              <DocumentDialog
                                key={`all-documents-${idx}`}
                                document={doc}
                                trigger={<DocumentCard document={doc} />}
                              />
                            ),
                          )}
                        </div>
                      </div>
                    </SheetContent>
                  </Sheet>
                )}
              </>
            ) : (
              <span className="flex items-center justify-start gap-2 text-gray-400">
                <p className="text-sm">Finding documents</p>
                <LoaderCircle className="animate-spin w-4 h-4" />
              </span>
            )}
          </div>
        </div>
      </CardContent>
    </Card>
  );
};

export const useGeneratingQuestionsUI = () =>
  useAssistantToolUI({
    toolName: "generating_questions",
    // Wrap the component in a useCallback to keep the identity stable.
    // Allows the component to be interactable and not be re-rendered on every state change.
    render: useCallback((input) => {
      if (!input.args?.questions || input.args.questions.length === 0) {
        return null;
      }

      return (
        <div className="flex flex-col mb-4">
          <span className="flex flex-row gap-2 items-center justify-start pb-4 text-gray-300">
            <Globe className="w-5 h-5" />
            <p className="text-xl">Research Plan & Sources</p>
          </span>
          <div className="mb-10">
            <div className="flex flex-wrap items-start justify-start gap-2">
              {(input.args.questions as Question[]).map(
                (question, questionIndex) => (
                  <QuestionCard
                    key={`question-${questionIndex}`}
                    question={question}
                  />
                ),
              )}
            </div>
          </div>
        </div>
      );
    }, []),
  });



================================================
FILE: frontend/app/components/LangSmithLinkToolUI.tsx
================================================
import { ExternalLink } from "lucide-react";
import { LangSmithSVG } from "./icons/langsmith";
import { TooltipIconButton } from "./ui/assistant-ui/tooltip-icon-button";
import { useAssistantToolUI } from "@assistant-ui/react";
import { useCallback } from "react";

export const useLangSmithLinkToolUI = () =>
  useAssistantToolUI({
    toolName: "langsmith_tool_ui",
    render: useCallback((input) => {
      return (
        <TooltipIconButton
          tooltip="View run in LangSmith"
          variant="ghost"
          className="transition-colors w-4 h-3 ml-3 mt-2 mb-[-8px]"
          delayDuration={400}
          onClick={() => window.open(input.args.sharedRunURL, "_blank")}
        >
          <span className="flex flex-row items-center gap-1 w-11 h-7">
            <ExternalLink />
            <LangSmithSVG className="text-[#CA632B] hover:text-[#CA632B]/95" />
          </span>
        </TooltipIconButton>
      );
    }, []),
  });



================================================
FILE: frontend/app/components/ProgressToolUI.tsx
================================================
import { useAssistantToolUI } from "@assistant-ui/react";
import { Progress } from "./ui/progress";
import { cn } from "../utils/cn";
import { useCallback } from "react";

export const stepToProgressFields = (step: number) => {
  switch (step) {
    case 0:
      return {
        text: "Routing Query",
        progress: 20,
      };
    case 1:
      return {
        text: "Generating questions",
        progress: 40,
      };
    case 2:
      return {
        text: "Doing research",
        progress: 65,
      };
    case 3:
      return {
        text: "Generating answer",
        progress: 85,
      };
    case 4:
      return {
        text: "Done",
        progress: 100,
      };
    default:
      return {
        text: "Working on it",
        progress: 0,
      };
  }
};

export const useProgressToolUI = () =>
  useAssistantToolUI({
    toolName: "progress",
    // Wrap the component in a useCallback to keep the identity stable.
    // Allows the component to be interactable and not be re-rendered on every state change.
    render: useCallback((input) => {
      const { text, progress } = stepToProgressFields(input.args.step);

      return (
        <div className="flex flex-row md:max-w-[550px] w-full items-center justify-start gap-3 pb-4 ml-[-5px] mt-[16px]">
          <Progress
            value={progress}
            indicatorClassName="bg-gray-700"
            className="w-[375px]"
          />
          <p
            className={cn(
              "text-gray-500 text-sm font-light",
              progress !== 100 ? "animate-pulse" : "",
            )}
          >
            {text}
          </p>
        </div>
      );
    }, []),
  });



================================================
FILE: frontend/app/components/RouterLogicToolUI.tsx
================================================
import { useAssistantToolUI } from "@assistant-ui/react";
import { Route } from "lucide-react";

export const useRouterLogicUI = () =>
  useAssistantToolUI({
    toolName: "router_logic",
    render: (input) => {
      if (!input.args?.logic || input.args.logic === 0) {
        return null;
      }

      return (
        <div className="flex flex-col mb-4">
          <span className="flex flex-row gap-2 items-center justify-start pb-0 text-gray-300">
            <Route className="w-5 h-5" />
            <p className="text-xl">Router Logic</p>
          </span>
          <p className="text-sm text-gray-400">{input.args.logic}</p>
        </div>
      );
    },
  });



================================================
FILE: frontend/app/components/SelectedDocumentsToolUI.tsx
================================================
import { useAssistantToolUI } from "@assistant-ui/react";
import { BookOpenText, Plus } from "lucide-react";
import { Sheet, SheetContent, SheetTrigger } from "./ui/sheet";
import { DocumentDialog } from "./DocumentDialog";
import { DocumentCard, Document } from "./DocumentCard";
import { useCallback } from "react";

export const useSelectedDocumentsUI = () =>
  useAssistantToolUI({
    toolName: "selected_documents",
    // Wrap the component in a useCallback to keep the identity stable.
    // Allows the component to be interactable and not be re-rendered on every state change.
    render: useCallback((input) => {
      if (!input.args?.documents || input.args.documents.length === 0) {
        return null;
      }

      // Filter out duplicate documents
      const uniqueDocuments = (input.args.documents as Document[]).reduce(
        (acc, current) => {
          const x = acc.find(
            (item) => item.page_content === current.page_content,
          );
          if (!x) {
            return acc.concat([current]);
          } else {
            return acc;
          }
        },
        [] as Document[],
      );

      const displayedDocuments = uniqueDocuments.slice(0, 3);
      const remainingDocuments = uniqueDocuments.slice(3);

      return (
        <div className="flex flex-col mb-4">
          <span className="flex flex-row gap-2 items-center justify-start pb-4 text-gray-300">
            <BookOpenText className="w-5 h-5" />
            <p className="text-xl">Selected Context</p>
          </span>
          <div className="flex flex-wrap items-center justify-start gap-2">
            {displayedDocuments.map((document, docIndex) => (
              <DocumentDialog
                key={`all-documents-${docIndex}`}
                document={document}
                trigger={<DocumentCard document={document} />}
              />
            ))}
            {remainingDocuments.length > 0 && (
              <Sheet>
                <SheetTrigger>
                  <div className="flex items-center border-[1px] border-gray-500 justify-center w-[40px] h-[110px] bg-[#282828] hover:bg-[#2b2b2b] rounded-md cursor-pointer transition-colors duration-200">
                    <Plus className="w-6 h-6 text-gray-300" />
                  </div>
                </SheetTrigger>
                <SheetContent
                  side="right"
                  className="bg-[#282828] border-none overflow-y-auto flex-grow scrollbar-thin scrollbar-thumb-gray-600 scrollbar-track-transparent md:min-w-[50vw] min-w-[70vw]"
                >
                  <div className="flex flex-col gap-4">
                    <h2 className="text-xl font-semibold text-gray-300">
                      All Selected Documents
                    </h2>
                    <div className="flex flex-wrap gap-2">
                      {uniqueDocuments.map((document, docIndex) => (
                        <DocumentDialog
                          key={`all-documents-${docIndex}`}
                          document={document}
                          trigger={<DocumentCard document={document} />}
                        />
                      ))}
                    </div>
                  </div>
                </SheetContent>
              </Sheet>
            )}
          </div>
        </div>
      );
    }, []),
  });



================================================
FILE: frontend/app/components/SelectModel.tsx
================================================
import React from "react";
import { useGraphContext } from "../contexts/GraphContext";
import { ModelOptions } from "../types";
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from "./ui/select";

const modelOptionsAndLabels: Partial<Record<ModelOptions, string>> = {
  "anthropic/claude-3-5-haiku-20241022": "Claude 3.5 Haiku",
  "openai/gpt-4.1-mini": "GPT 4.1 Mini",
  "google_genai/gemini-2.0-flash": "Gemini 2.0 Flash",
};

export function SelectModelComponent() {
  const {
    graphData: { selectedModel, setSelectedModel },
  } = useGraphContext();
  return (
    <Select
      onValueChange={(v) => setSelectedModel(v as ModelOptions)}
      value={selectedModel}
      defaultValue="anthropic/claude-3-5-haiku-20241022"
    >
      <SelectTrigger className="w-[180px] border-gray-600 text-gray-200">
        <SelectValue placeholder="Model" />
      </SelectTrigger>
      <SelectContent className="bg-[#282828] text-gray-200 border-gray-600">
        {Object.entries(modelOptionsAndLabels).map(([model, label]) => (
          <SelectItem className="hover:bg-[#2b2b2b]" key={model} value={model}>
            {label}
          </SelectItem>
        ))}
      </SelectContent>
    </Select>
  );
}

export const SelectModel = React.memo(SelectModelComponent);



================================================
FILE: frontend/app/components/SuggestedQuestions.tsx
================================================
import { useThreadRuntime } from "@assistant-ui/react";
import { Card, CardTitle } from "./ui/card";

const suggestedQuestions = [
  "How do I use a RecursiveUrlLoader to load content from a page?",
  "How can I define the state schema for my LangGraph graph?",
  "How can I run a model locally on my laptop with Ollama?",
  "Explain RAG techniques and how LangGraph can implement them.",
];

export function SuggestedQuestions() {
  const threadRuntime = useThreadRuntime();

  const handleSend = (text: string) => {
    threadRuntime.append({
      role: "user",
      content: [{ type: "text", text }],
    });
  };

  return (
    <div className="w-full grid grid-cols-1 sm:grid-cols-2 gap-4">
      {suggestedQuestions.map((question, idx) => (
        <Card
          onClick={() => handleSend(question)}
          key={`suggested-question-${idx}`}
          className="w-full bg-[#282828] border-gray-600 cursor-pointer transition-colors ease-in hover:bg-[#2b2b2b]"
        >
          <CardTitle className="p-4 text-gray-200 font-normal text-sm">
            {question}
          </CardTitle>
        </Card>
      ))}
    </div>
  );
}



================================================
FILE: frontend/app/components/chat-interface/chat-composer.tsx
================================================
"use client";

import { ComposerPrimitive, ThreadPrimitive } from "@assistant-ui/react";
import { type FC } from "react";

import { SendHorizontalIcon } from "lucide-react";
import { BaseMessage } from "@langchain/core/messages";
import { TooltipIconButton } from "../ui/assistant-ui/tooltip-icon-button";
import { cn } from "@/app/utils/cn";

export interface ChatComposerProps {
  messages: BaseMessage[];
  submitDisabled: boolean;
}

const CircleStopIcon = () => {
  return (
    <svg
      xmlns="http://www.w3.org/2000/svg"
      viewBox="0 0 16 16"
      fill="currentColor"
      width="16"
      height="16"
    >
      <rect width="10" height="10" x="3" y="3" rx="2" />
    </svg>
  );
};

export const ChatComposer: FC<ChatComposerProps> = (
  props: ChatComposerProps,
) => {
  const isEmpty = props.messages.length === 0;

  return (
    <ComposerPrimitive.Root
      className={cn(
        "focus-within:border-aui-ring/20 flex w-full items-center md:justify-left justify-center rounded-lg border px-2.5 py-2.5 shadow-sm transition-all duration-300 ease-in-out bg-[#282828] border-gray-600",
        isEmpty ? "" : "md:ml-24 ml-3 mb-6",
        isEmpty ? "w-full" : "md:w-[70%] w-[95%] md:max-w-[832px]",
      )}
    >
      <ComposerPrimitive.Input
        autoFocus
        placeholder="How can I..."
        rows={1}
        className="placeholder:text-gray-400 text-gray-100 max-h-40 flex-1 resize-none border-none bg-transparent px-2 py-2 text-sm outline-none focus:ring-0 disabled:cursor-not-allowed"
      />
      <div className="flex-shrink-0">
        <ThreadPrimitive.If running={false} disabled={props.submitDisabled}>
          <ComposerPrimitive.Send asChild>
            <TooltipIconButton
              tooltip="Send"
              variant="default"
              className="my-1 size-8 p-2 transition-opacity ease-in"
            >
              <SendHorizontalIcon />
            </TooltipIconButton>
          </ComposerPrimitive.Send>
        </ThreadPrimitive.If>
        <ThreadPrimitive.If running>
          <ComposerPrimitive.Cancel asChild>
            <TooltipIconButton
              tooltip="Cancel"
              variant="default"
              className="my-1 size-8 p-2 transition-opacity ease-in"
            >
              <CircleStopIcon />
            </TooltipIconButton>
          </ComposerPrimitive.Cancel>
        </ThreadPrimitive.If>
      </div>
    </ComposerPrimitive.Root>
  );
};



================================================
FILE: frontend/app/components/chat-interface/index.tsx
================================================
"use client";

import { ThreadPrimitive } from "@assistant-ui/react";
import { type FC } from "react";
import NextImage from "next/image";

import { ArrowDownIcon } from "lucide-react";
import { useAnswerHeaderToolUI } from "../AnswerHeaderToolUI";
import { useGeneratingQuestionsUI } from "../GeneratingQuestionsToolUI";
import { useProgressToolUI } from "../ProgressToolUI";
import { useRouterLogicUI } from "../RouterLogicToolUI";
import { useSelectedDocumentsUI } from "../SelectedDocumentsToolUI";
import { SelectModel } from "../SelectModel";
import { SuggestedQuestions } from "../SuggestedQuestions";
import { TooltipIconButton } from "../ui/assistant-ui/tooltip-icon-button";
import { AssistantMessage, UserMessage } from "./messages";
import { ChatComposer, ChatComposerProps } from "./chat-composer";
import { cn } from "@/app/utils/cn";

export interface ThreadChatProps extends ChatComposerProps {}

export const ThreadChat: FC<ThreadChatProps> = (props: ThreadChatProps) => {
  const isEmpty = props.messages.length === 0;

  useGeneratingQuestionsUI();
  useAnswerHeaderToolUI();
  useProgressToolUI();
  useSelectedDocumentsUI();
  useRouterLogicUI();

  return (
    <ThreadPrimitive.Root className="flex flex-col h-screen overflow-hidden w-full">
      {!isEmpty ? (
        <ThreadPrimitive.Viewport
          className={cn(
            "flex-1 overflow-y-auto scroll-smooth bg-inherit transition-all duration-300 ease-in-out w-full",
            isEmpty ? "pb-[30vh] sm:pb-[50vh]" : "pb-32 sm:pb-24",
            "scrollbar-thin scrollbar-thumb-gray-600 scrollbar-track-transparent",
          )}
        >
          <div className="md:pl-8 lg:pl-24 mt-2 max-w-full">
            <ThreadPrimitive.Messages
              components={{
                UserMessage: UserMessage,
                AssistantMessage: AssistantMessage,
              }}
            />
          </div>
        </ThreadPrimitive.Viewport>
      ) : null}
      <ThreadChatScrollToBottom />
      {isEmpty ? (
        <div className="flex items-center justify-center flex-grow my-auto">
          <div className="flex flex-col items-center mx-4 md:mt-0 mt-24">
            <div className="flex flex-row gap-1 items-center justify-center">
              <p className="text-xl sm:text-2xl">Chat LangChain</p>
              <NextImage
                src="/images/lc_logo.jpg"
                className="rounded-3xl"
                alt="LangChain Logo"
                width={32}
                height={32}
                style={{ width: "auto", height: "auto" }}
              />
            </div>
            <div className="mb-4 sm:mb-[24px] mt-1 sm:mt-2">
              <SelectModel />
            </div>
            <div className="md:mb-8 mb-4">
              <SuggestedQuestions />
            </div>
            <ChatComposer
              submitDisabled={props.submitDisabled}
              messages={props.messages}
            />
          </div>
        </div>
      ) : (
        <ChatComposer
          submitDisabled={props.submitDisabled}
          messages={props.messages}
        />
      )}
    </ThreadPrimitive.Root>
  );
};

const ThreadChatScrollToBottom: FC = () => {
  return (
    <ThreadPrimitive.ScrollToBottom asChild>
      <TooltipIconButton
        tooltip="Scroll to bottom"
        variant="outline"
        className="absolute bottom-28 left-1/2 transform -translate-x-1/2 rounded-full disabled:invisible bg-white bg-opacity-75"
      >
        <ArrowDownIcon className="text-gray-600 hover:text-gray-800 transition-colors ease-in-out" />
      </TooltipIconButton>
    </ThreadPrimitive.ScrollToBottom>
  );
};



================================================
FILE: frontend/app/components/chat-interface/messages.tsx
================================================
"use client";

import {
  MessagePrimitive,
  useMessage,
  useThreadRuntime,
} from "@assistant-ui/react";
import { useState, type FC } from "react";

import { MarkdownText } from "../ui/assistant-ui/markdown-text";
import { useGraphContext } from "@/app/contexts/GraphContext";
import { useRuns } from "@/app/hooks/useRuns";
import { TooltipIconButton } from "../ui/assistant-ui/tooltip-icon-button";
import { ThumbsDownIcon, ThumbsUpIcon } from "lucide-react";

export const UserMessage: FC = () => {
  return (
    <MessagePrimitive.Root className="pt-2 sm:pt-4 flex w-full md:max-w-4xl md:mx-0 mx-auto max-w-[95%] md:py-4 py-2">
      <div className="bg-inherit text-white break-words rounded-2xl sm:rounded-3xl pt-2 md:pt-2.5 mb-[-15px] sm:mb-[-25px] text-2xl sm:text-4xl font-light">
        <MessagePrimitive.Content />
      </div>
    </MessagePrimitive.Root>
  );
};

function FeedbackButtons() {
  const {
    graphData: { runId, isStreaming },
  } = useGraphContext();
  const { sendFeedback } = useRuns();
  const [feedback, setFeedback] = useState<"good" | "bad">();

  const feedbackKey = "user_feedback";
  const goodScore = 1;
  const badScore = 0;

  if (!runId || isStreaming) return null;

  if (feedback) {
    return (
      <div className="flex gap-2 items-center mt-4">
        {feedback === "good" ? (
          <ThumbsUpIcon className="w-4 h-4 text-green-500" />
        ) : (
          <ThumbsDownIcon className="w-4 h-4 text-red-500" />
        )}
      </div>
    );
  }

  return (
    <div className="flex gap-2 items-center mt-4">
      <TooltipIconButton
        delayDuration={200}
        variant="ghost"
        tooltip="Good response"
        onClick={() => {
          sendFeedback(runId, feedbackKey, goodScore);
          setFeedback("good");
        }}
      >
        <ThumbsUpIcon className="w-4 h-4" />
      </TooltipIconButton>
      <TooltipIconButton
        delayDuration={200}
        variant="ghost"
        tooltip="Bad response"
        onClick={() => {
          sendFeedback(runId, feedbackKey, badScore);
          setFeedback("bad");
        }}
      >
        <ThumbsDownIcon className="w-4 h-4" />
      </TooltipIconButton>
    </div>
  );
}

export const AssistantMessage: FC = () => {
  const threadRuntime = useThreadRuntime();
  const threadState = threadRuntime.getState();
  const isLast = useMessage((m) => m.isLast);
  const shouldRenderMessageBreak =
    threadState.messages.filter((msg) => msg.role === "user")?.length > 1 &&
    !isLast;

  return (
    <MessagePrimitive.Root className="flex w-full md:max-w-4xl md:mx-0 mx-auto max-w-[95%] md:py-4 py-2">
      <div className="bg-inherit text-white max-w-full sm:max-w-3xl break-words leading-6 sm:leading-7">
        <MessagePrimitive.Content components={{ Text: MarkdownText }} />
        {shouldRenderMessageBreak ? (
          <hr className="relative left-1/2 -translate-x-1/2 w-[90vw] sm:w-[45vw] mt-4 sm:mt-6 border-gray-600" />
        ) : null}
        {isLast && <FeedbackButtons />}
      </div>
    </MessagePrimitive.Root>
  );
};



================================================
FILE: frontend/app/components/icons/langsmith.tsx
================================================
export const LangSmithSVG = ({ className }: { className?: string }) => (
  <svg
    width="48"
    height="24"
    viewBox="0 0 48 24"
    fill="none"
    xmlns="http://www.w3.org/2000/svg"
    className={className}
  >
    <path
      fillRule="evenodd"
      clipRule="evenodd"
      d="M48 12C48 18.6163 42.5277 24 35.8024 24H12.1976C5.47233 24 0 18.6173 0 12C0 5.38272 5.47233 0 12.1976 0H35.8024C42.5286 0 48 5.38368 48 12ZM19.7123 11.4817C20.7324 12.7095 21.5077 14.1636 22.1424 15.6141C22.246 15.8062 22.301 16.0576 22.3633 16.2666C22.4568 16.5804 22.5007 16.8897 22.7315 17.1366C22.7871 17.2094 22.964 17.3017 23.0997 17.4266C23.4252 17.7262 23.8302 18.1273 23.6888 18.2965C23.7324 18.3897 23.9035 18.477 24.057 18.5865C24.3119 18.7685 24.5388 18.9438 24.3515 19.094C23.9457 19.1763 23.4835 19.1965 23.1733 18.8765C23.0815 19.0852 22.9108 19.0662 22.7315 19.0215C22.6884 19.0108 22.6244 18.9558 22.5842 18.949C22.5649 19.0022 22.6025 19.0419 22.5842 19.094C21.8997 19.1392 21.3653 18.4557 21.0378 17.934C20.7232 17.7663 20.3441 17.6359 20.0068 17.4991C19.6484 17.3537 19.317 17.2506 18.9759 17.0641C18.9684 17.1804 18.9769 17.3031 18.9759 17.4266C18.9715 17.9366 18.9239 18.4206 18.4604 18.7315C18.4451 19.347 18.9923 19.3897 19.4914 19.384C19.9228 19.3791 20.3732 19.3498 20.4487 19.7465C20.4154 19.7499 20.3355 19.7463 20.3014 19.7465L20.2995 19.7465C20.2043 19.7472 20.1559 19.7476 20.0805 19.819C19.838 20.0519 19.5414 19.9964 19.2704 19.8915C19.021 19.795 18.792 19.6637 18.5341 19.819C18.3017 19.9347 18.0919 20.0121 17.9449 20.109C17.6359 20.3128 17.4456 20.4937 16.8404 20.544C16.7905 20.4708 16.7976 20.4431 16.8404 20.399C16.9237 20.3039 17.0059 20.2059 17.0613 20.109C17.1736 19.9126 17.215 19.6835 17.5031 19.6015C17.1157 19.5418 16.7986 19.7181 16.4722 19.8915C16.4257 19.9162 16.3712 19.9402 16.3249 19.964C16.1218 20.0461 16.0057 20.0315 15.883 19.964C15.7133 19.8707 15.5794 19.7611 15.1467 20.0365C15.0643 19.9703 15.0999 19.8702 15.1467 19.819C15.3358 19.5913 15.5287 19.5885 15.8094 19.6015C15.0767 19.1984 14.6179 19.442 14.1893 19.674C13.8095 19.8797 13.4585 20.0685 13.1584 19.674C13.0222 19.7096 12.9396 19.7958 12.8638 19.8915C12.8327 19.9309 12.8255 20.0014 12.7902 20.0365C12.7171 19.9566 12.6982 19.8402 12.7166 19.7465C12.7242 19.7073 12.789 19.7134 12.7902 19.674C12.7719 19.6657 12.7355 19.6092 12.7166 19.6015C12.6057 19.5563 12.4644 19.5519 12.4956 19.384C12.2637 19.3071 12.1513 19.4 11.9802 19.529C11.9704 19.5364 11.9163 19.5217 11.9065 19.529C11.7944 19.4432 11.8938 19.3437 11.9802 19.239C12.02 19.1908 12.0378 19.1352 12.0538 19.094C12.1332 18.9576 12.2888 18.9526 12.422 18.949C12.5346 18.946 12.6388 18.9597 12.7166 18.8765C12.9947 18.7205 13.3292 18.795 13.6739 18.8765C13.9253 18.936 14.1786 18.9803 14.4103 18.949C14.8334 19.0022 15.3602 18.5789 15.1467 18.1515C14.7592 17.6641 14.7757 17.0781 14.7785 16.4841C14.7789 16.3826 14.7799 16.2936 14.7785 16.1941C14.7462 15.9688 14.4418 15.6581 14.1157 15.3966C13.8667 15.1969 13.5741 15.0025 13.453 14.8166C13.1185 14.4429 12.9036 14.0108 12.6429 13.5841C12.6349 13.571 12.5773 13.5973 12.5693 13.5841C12.1276 12.7402 12.0105 11.7794 11.8329 10.8292C11.6195 9.68758 11.3966 8.58029 10.7283 7.63924C10.174 7.94118 9.43899 7.7545 8.96097 7.34925C8.71062 7.57283 8.68582 7.90832 8.66642 8.21923C8.04804 7.60834 8.09747 6.43438 8.59278 5.75428C8.79517 5.48645 9.05863 5.29539 9.32917 5.10179C9.38917 5.05866 9.40484 5.025 9.40281 4.95679C9.89201 2.78508 13.2073 3.20155 14.263 4.7393C14.6117 5.17006 14.9089 5.62293 15.1467 6.11677C15.431 6.70738 15.7051 7.30513 16.1776 7.78424C16.6349 8.27771 17.0947 8.76368 17.5767 9.23421C18.3264 9.96597 19.0561 10.6642 19.7123 11.4817ZM28.1808 15.7591C28.8714 15.1352 29.5728 14.4611 29.5799 14.4541C29.627 14.4149 30.4062 13.735 31.5682 12.4242L32.9673 14.2366C32.341 15.0057 31.9743 15.4912 31.6418 15.9766C30.642 17.4371 29.809 18.648 29.8008 18.659C29.7968 18.6671 29.146 19.6015 28.1808 19.6015C28.0343 19.6015 27.8996 19.5792 27.7389 19.529C27.2029 19.3605 26.8113 19.0763 26.6344 18.659C26.4045 18.1194 26.5591 17.573 26.6344 17.4266C26.9069 16.8969 27.5034 16.371 28.1808 15.7591ZM39.0793 17.7166C39.2584 17.9162 39.3739 18.1747 39.3739 18.4415V18.5865C39.3739 18.8313 39.3015 19.0535 39.153 19.239C38.9465 19.4918 38.5988 19.674 38.2693 19.674C38.1981 19.674 38.0794 19.645 38.0082 19.632L37.9011 19.6015C37.6265 19.5494 37.3315 19.3882 37.1647 19.1665C36.6623 18.4985 35.5739 17.0117 34.5874 15.6866C34.1687 15.124 33.8028 14.5934 33.4828 14.1641C33.3674 14.0094 33.2073 13.8535 33.1146 13.7291C32.9051 13.4483 32.1449 12.4745 31.9364 12.2067C30.842 10.7953 30.1035 9.96723 30.0954 9.9592C30.0079 9.85387 29.7361 9.6268 29.5063 9.59671C29.1411 9.61276 28.7135 9.38924 28.6962 9.37921C28.5355 9.29896 28.3891 9.23421 28.328 9.23421C28.2009 9.31446 28.1757 9.35769 28.1808 9.52421C28.1808 9.54161 28.1804 9.57814 28.1808 9.59671C28.1815 9.63594 28.1828 9.62291 28.1808 9.66921C28.1716 9.90594 27.9822 10.0861 27.9598 10.1042C27.9598 10.1042 27.7115 10.3539 27.3707 10.6117C26.9558 10.9247 26.6466 10.6942 26.6344 10.6842C26.6079 10.6621 26.0979 10.2217 25.8243 9.8867C25.5548 9.55568 25.4718 9.44705 25.1616 9.01672C24.8036 8.52069 25.0752 8.16153 25.0879 8.14673L25.677 7.63924L25.6797 7.63712C25.7699 7.56636 26.1769 7.24701 26.4134 7.42175C26.6107 7.55516 26.8471 7.42676 26.8553 7.42175C26.8634 7.41673 26.9716 7.32608 26.9289 7.05925C26.8628 6.64899 27.1386 6.34731 27.1498 6.33427C27.158 6.32825 27.953 5.67875 28.9172 4.8843C30.3918 3.92958 32.2223 4.29896 32.9423 4.44426L32.9673 4.4493C33.4779 4.55262 34.0277 4.79256 33.9983 5.02929C33.9922 5.07844 33.9909 5.25582 33.7773 5.24679C32.1429 5.17958 31.0419 5.94251 30.7581 6.18927C30.7429 6.20231 30.7602 6.24271 30.7581 6.26177V6.33427C30.7561 6.34731 30.7531 6.39473 30.7581 6.40677L31.1263 7.27675C31.2148 7.4854 31.2736 7.70254 31.2736 7.92924V8.50923L31.4209 8.87172C31.5399 9.02419 32.9397 10.8507 34.2192 12.2792C35.2383 13.4167 37.912 16.3585 38.9321 17.4991L39.0793 17.7166ZM40.0367 6.26177L40.2576 6.47927C40.3369 6.55751 40.3587 6.73443 40.3312 6.84176L39.963 8.29173C39.9427 8.36395 39.4308 10.017 37.9011 10.3942C37.3469 10.5311 36.9805 10.5302 36.7229 10.5392C36.3043 10.5532 36.1952 10.5667 35.692 11.1192C35.588 11.2338 35.4395 11.3591 35.3238 11.4817C35.1326 11.6841 34.9595 11.8755 34.7346 12.1342L33.1882 10.3942C33.232 10.3431 33.2963 10.2907 33.3355 10.2492C33.4064 10.174 33.4647 10.0839 33.5564 9.9592C33.8351 9.59207 34.0418 9.3523 34.1455 9.16171C34.2299 9.00824 34.2283 8.82575 34.2192 8.65422L34.2183 8.6388C34.1937 8.1989 34.1488 7.39707 34.2192 6.91426C34.3158 6.2472 34.8854 5.53112 35.692 5.10179C36.258 4.79926 37.0596 4.49896 37.3781 4.37962L37.3857 4.3768C37.5118 4.32966 37.6664 4.42049 37.7538 4.5218L37.9011 4.6668C38.0018 4.78517 38.0008 4.91093 37.9011 5.02929L36.5756 6.62426C36.54 6.66639 36.4999 6.71409 36.502 6.76926V7.13175C36.504 7.19595 36.5258 7.30712 36.5756 7.34925L37.3857 8.00174C37.4335 8.04086 37.4709 8.00776 37.5329 8.00174H38.0484C38.1074 7.99572 38.157 7.97438 38.1957 7.92924L39.5212 6.33427C39.5792 6.26606 39.6526 6.19328 39.7421 6.18927C39.8316 6.18225 39.9736 6.19958 40.0367 6.26177Z"
      fill="currentColor"
    ></path>
    <path
      d="M15.6621 18.5865C15.593 18.9527 15.8816 19.1389 16.1776 19.0215C16.4715 18.8881 16.6681 19.0738 16.7667 19.3115C17.2203 19.3767 17.8459 19.1887 17.8713 18.659C17.3315 18.3521 17.1396 17.8263 16.914 17.2816C16.899 17.2452 16.8553 17.173 16.8404 17.1366C16.806 17.0531 16.7286 17.0019 16.6931 16.9191C16.6878 16.9068 16.6984 16.8588 16.6931 16.8466C16.6654 16.9532 16.6451 17.0843 16.6194 17.2091C16.4924 17.8258 16.357 18.5607 15.6621 18.5865Z"
      fill="currentColor"
    ></path>
    <path
      d="M28.549 18.659C28.4096 18.8877 28.0223 18.9432 27.6653 18.7315C27.4822 18.6232 27.2967 18.459 27.2235 18.2965C27.1563 18.1491 27.1594 18.0394 27.2235 17.934C27.2967 17.8137 27.4863 17.7166 27.6653 17.7166C27.826 17.7166 28.0119 17.7612 28.1808 17.8616C28.5378 18.0732 28.6883 18.4303 28.549 18.659Z"
      fill="currentColor"
    ></path>
  </svg>
);



================================================
FILE: frontend/app/components/thread-history/index.tsx
================================================
import { TooltipIconButton } from "../ui/assistant-ui/tooltip-icon-button";
import { SquarePen, History } from "lucide-react";
import { Sheet, SheetContent, SheetTrigger } from "../ui/sheet";
import { Skeleton } from "../ui/skeleton";
import React from "react";
import { useGraphContext } from "../../contexts/GraphContext";
import { groupThreads } from "./utils";
import { ThreadsList } from "./thread-list";
import { useQueryState } from "nuqs";

const LoadingThread = () => <Skeleton className="w-full h-8 bg-[#373737]" />;

function ThreadHistoryComponent() {
  const { threadsData, userData, graphData } = useGraphContext();
  const { userThreads, isUserThreadsLoading, deleteThread } = threadsData;
  const { userId } = userData;
  const { switchSelectedThread, setMessages } = graphData;
  const [_threadId, setThreadId] = useQueryState("threadId");

  const clearMessages = () => {
    setMessages([]);
  };

  const deleteThreadAndClearMessages = async (id: string) => {
    clearMessages();
    await deleteThread(id, clearMessages);
  };

  const groupedThreads = groupThreads(
    userThreads,
    switchSelectedThread,
    deleteThreadAndClearMessages,
  );

  const createNewSession = async () => {
    setThreadId(null);
    clearMessages();
  };

  return (
    <span>
      {/* Tablet & up */}
      <div className="hidden md:flex flex-col w-[260px] h-full">
        <div className="flex-grow border-r-[1px] border-[#393939] my-6 flex flex-col overflow-hidden">
          <div className="flex flex-row items-center justify-between border-b-[1px] border-[#393939] pt-3 px-2 mx-4 -mt-4 text-gray-200">
            <p className="text-lg font-medium">Chat History</p>
            {userId ? (
              <TooltipIconButton
                tooltip="New chat"
                variant="ghost"
                className="w-fit p-2"
                onClick={createNewSession}
              >
                <SquarePen className="w-5 h-5" />
              </TooltipIconButton>
            ) : null}
          </div>
          <div className="overflow-y-auto flex-grow scrollbar-thin scrollbar-thumb-gray-600 scrollbar-track-transparent">
            {isUserThreadsLoading && !userThreads.length ? (
              <div className="flex flex-col gap-1 px-3 pt-3">
                {Array.from({ length: 25 }).map((_, i) => (
                  <LoadingThread key={`loading-thread-${i}`} />
                ))}
              </div>
            ) : (
              <ThreadsList groupedThreads={groupedThreads} />
            )}
          </div>
        </div>
      </div>
      {/* Mobile */}
      <span className="md:hidden flex flex-row gap-2 mt-2 ml-2">
        <Sheet>
          <SheetTrigger asChild>
            <TooltipIconButton
              tooltip="New chat"
              variant="ghost"
              className="w-fit h-fit p-2"
            >
              <History className="w-6 h-6" />
            </TooltipIconButton>
          </SheetTrigger>
          <SheetContent side="left" className="bg-[#282828] border-none">
            {isUserThreadsLoading && !userThreads.length ? (
              <div className="flex flex-col gap-1 px-3 pt-3">
                {Array.from({ length: 25 }).map((_, i) => (
                  <LoadingThread key={`loading-thread-${i}`} />
                ))}
              </div>
            ) : (
              <ThreadsList groupedThreads={groupedThreads} />
            )}
          </SheetContent>
        </Sheet>
        {userId ? (
          <TooltipIconButton
            tooltip="New chat"
            variant="ghost"
            className="w-fit h-fit p-2"
            onClick={createNewSession}
          >
            <SquarePen className="w-6 h-6" />
          </TooltipIconButton>
        ) : null}
      </span>
    </span>
  );
}

export const ThreadHistory = React.memo(ThreadHistoryComponent);



================================================
FILE: frontend/app/components/thread-history/thread-item.tsx
================================================
import { useState } from "react";
import { Button } from "../ui/button";
import { TooltipIconButton } from "../ui/assistant-ui/tooltip-icon-button";
import { Trash2 } from "lucide-react";

export interface ThreadProps {
  id: string;
  onClick: () => void;
  onDelete: () => void;
  label: string;
  createdAt: Date;
}

export function Thread(props: ThreadProps) {
  const [isHovering, setIsHovering] = useState(false);

  return (
    <div
      className="flex flex-row gap-0 items-center justify-start w-full"
      onMouseEnter={() => setIsHovering(true)}
      onMouseLeave={() => setIsHovering(false)}
    >
      <Button
        className="px-2 hover:bg-[#393939] hover:text-white justify-start items-center flex-grow min-w-[191px] pr-0"
        size="sm"
        variant="ghost"
        onClick={props.onClick}
      >
        <p className="truncate text-sm font-light w-full text-left">
          {props.label}
        </p>
      </Button>
      {isHovering && (
        <TooltipIconButton
          tooltip="Delete thread"
          variant="ghost"
          className="hover:bg-[#373737] flex-shrink-0 p-2"
          onClick={props.onDelete}
        >
          <Trash2 className="w-4 h-4 text-[#575757] hover:text-red-500 transition-colors ease-in" />
        </TooltipIconButton>
      )}
    </div>
  );
}



================================================
FILE: frontend/app/components/thread-history/thread-list.tsx
================================================
import { Thread, ThreadProps } from "./thread-item";
import { prettifyDateLabel } from "./utils";

export interface ThreadsListProps {
  groupedThreads: {
    today: ThreadProps[];
    yesterday: ThreadProps[];
    lastSevenDays: ThreadProps[];
    older: ThreadProps[];
  };
}

export function ThreadsList(props: ThreadsListProps) {
  return (
    <div className="flex flex-col px-3 pt-3 gap-4">
      {Object.entries(props.groupedThreads).map(([group, threads]) =>
        threads.length > 0 ? (
          <div key={group}>
            <h3 className="text-sm font-medium text-gray-400 mb-1 pl-2">
              {prettifyDateLabel(group)}
            </h3>
            <div className="flex flex-col gap-1">
              {threads.map((thread) => (
                <Thread key={thread.id} {...thread} />
              ))}
            </div>
          </div>
        ) : null,
      )}
    </div>
  );
}



================================================
FILE: frontend/app/components/thread-history/utils.ts
================================================
import { Thread } from "@langchain/langgraph-sdk";
import { ThreadProps } from "./thread-item";
import { subDays, isToday, isYesterday, isWithinInterval } from "date-fns";

export function convertThreadActualToThreadProps(
  thread: Thread,
  switchSelectedThread: (thread: Thread) => void,
  deleteThread: (id: string) => void,
): ThreadProps {
  const values = thread.values as Record<string, any> | undefined;
  return {
    id: thread.thread_id,
    label: values?.messages?.[0].content || "Untitled",
    createdAt: new Date(thread.created_at),
    onClick: () => {
      return switchSelectedThread(thread);
    },
    onDelete: () => {
      return deleteThread(thread.thread_id);
    },
  };
}

export function groupThreads(
  threads: Thread[],
  switchSelectedThread: (thread: Thread) => void,
  deleteThread: (id: string) => void,
) {
  const today = new Date();
  const yesterday = subDays(today, 1);
  const sevenDaysAgo = subDays(today, 7);

  return {
    today: threads
      .filter((thread) => isToday(new Date(thread.created_at)))
      .sort(
        (a, b) =>
          new Date(b.created_at).getTime() - new Date(a.created_at).getTime(),
      )
      .map((t) =>
        convertThreadActualToThreadProps(t, switchSelectedThread, deleteThread),
      ),
    yesterday: threads
      .filter((thread) => isYesterday(new Date(thread.created_at)))
      .sort(
        (a, b) =>
          new Date(b.created_at).getTime() - new Date(a.created_at).getTime(),
      )
      .map((t) =>
        convertThreadActualToThreadProps(t, switchSelectedThread, deleteThread),
      ),
    lastSevenDays: threads
      .filter((thread) =>
        isWithinInterval(new Date(thread.created_at), {
          start: sevenDaysAgo,
          end: yesterday,
        }),
      )
      .sort(
        (a, b) =>
          new Date(b.created_at).getTime() - new Date(a.created_at).getTime(),
      )
      .map((t) =>
        convertThreadActualToThreadProps(t, switchSelectedThread, deleteThread),
      ),
    older: threads
      .filter((thread) => new Date(thread.created_at) < sevenDaysAgo)
      .sort(
        (a, b) =>
          new Date(b.created_at).getTime() - new Date(a.created_at).getTime(),
      )
      .map((t) =>
        convertThreadActualToThreadProps(t, switchSelectedThread, deleteThread),
      ),
  };
}

export function prettifyDateLabel(group: string): string {
  switch (group) {
    case "today":
      return "Today";
    case "yesterday":
      return "Yesterday";
    case "lastSevenDays":
      return "Last 7 days";
    case "older":
      return "Older";
    default:
      return group;
  }
}



================================================
FILE: frontend/app/components/ui/avatar.tsx
================================================
"use client";

import * as React from "react";
import * as AvatarPrimitive from "@radix-ui/react-avatar";

import { cn } from "../../utils/cn";

const Avatar = React.forwardRef<
  React.ElementRef<typeof AvatarPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof AvatarPrimitive.Root>
>(({ className, ...props }, ref) => (
  <AvatarPrimitive.Root
    ref={ref}
    className={cn(
      "relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full",
      className,
    )}
    {...props}
  />
));
Avatar.displayName = AvatarPrimitive.Root.displayName;

const AvatarImage = React.forwardRef<
  React.ElementRef<typeof AvatarPrimitive.Image>,
  React.ComponentPropsWithoutRef<typeof AvatarPrimitive.Image>
>(({ className, ...props }, ref) => (
  <AvatarPrimitive.Image
    ref={ref}
    className={cn("aspect-square h-full w-full", className)}
    {...props}
  />
));
AvatarImage.displayName = AvatarPrimitive.Image.displayName;

const AvatarFallback = React.forwardRef<
  React.ElementRef<typeof AvatarPrimitive.Fallback>,
  React.ComponentPropsWithoutRef<typeof AvatarPrimitive.Fallback>
>(({ className, ...props }, ref) => (
  <AvatarPrimitive.Fallback
    ref={ref}
    className={cn(
      "flex h-full w-full items-center justify-center rounded-full bg-muted",
      className,
    )}
    {...props}
  />
));
AvatarFallback.displayName = AvatarPrimitive.Fallback.displayName;

export { Avatar, AvatarImage, AvatarFallback };



================================================
FILE: frontend/app/components/ui/button.tsx
================================================
import * as React from "react";
import { Slot } from "@radix-ui/react-slot";
import { cva, type VariantProps } from "class-variance-authority";

import { cn } from "../../utils/cn";

const buttonVariants = cva(
  "inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50",
  {
    variants: {
      variant: {
        default:
          "bg-primary text-primary-foreground shadow hover:bg-primary/90",
        destructive:
          "bg-destructive text-destructive-foreground shadow-sm hover:bg-destructive/90",
        outline:
          "border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground",
        secondary:
          "bg-secondary text-secondary-foreground shadow-sm hover:bg-secondary/80",
        ghost: "hover:bg-accent hover:text-accent-foreground",
        link: "text-primary underline-offset-4 hover:underline",
      },
      size: {
        default: "h-9 px-4 py-2",
        sm: "h-8 rounded-md px-3 text-xs",
        lg: "h-10 rounded-md px-8",
        icon: "h-9 w-9",
      },
    },
    defaultVariants: {
      variant: "default",
      size: "default",
    },
  },
);

export interface ButtonProps
  extends React.ButtonHTMLAttributes<HTMLButtonElement>,
    VariantProps<typeof buttonVariants> {
  asChild?: boolean;
}

const Button = React.forwardRef<HTMLButtonElement, ButtonProps>(
  ({ className, variant, size, asChild = false, ...props }, ref) => {
    const Comp = asChild ? Slot : "button";
    return (
      <Comp
        className={cn(buttonVariants({ variant, size, className }))}
        ref={ref}
        {...props}
      />
    );
  },
);
Button.displayName = "Button";

export { Button, buttonVariants };



================================================
FILE: frontend/app/components/ui/card.tsx
================================================
import * as React from "react";

import { cn } from "../../utils/cn";

const Card = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn(
      "rounded-xl border bg-card text-card-foreground shadow",
      className,
    )}
    {...props}
  />
));
Card.displayName = "Card";

const CardHeader = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn("flex flex-col space-y-1.5 p-6", className)}
    {...props}
  />
));
CardHeader.displayName = "CardHeader";

const CardTitle = React.forwardRef<
  HTMLParagraphElement,
  React.HTMLAttributes<HTMLHeadingElement>
>(({ className, ...props }, ref) => (
  <h3
    ref={ref}
    className={cn("font-semibold leading-none tracking-tight", className)}
    {...props}
  />
));
CardTitle.displayName = "CardTitle";

const CardDescription = React.forwardRef<
  HTMLParagraphElement,
  React.HTMLAttributes<HTMLParagraphElement>
>(({ className, ...props }, ref) => (
  <p
    ref={ref}
    className={cn("text-sm text-muted-foreground", className)}
    {...props}
  />
));
CardDescription.displayName = "CardDescription";

const CardContent = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div ref={ref} className={cn("p-6 pt-0", className)} {...props} />
));
CardContent.displayName = "CardContent";

const CardFooter = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn("flex items-center p-6 pt-0", className)}
    {...props}
  />
));
CardFooter.displayName = "CardFooter";

export {
  Card,
  CardHeader,
  CardFooter,
  CardTitle,
  CardDescription,
  CardContent,
};



================================================
FILE: frontend/app/components/ui/carousel.tsx
================================================
"use client";

import * as React from "react";
import { ArrowLeftIcon, ArrowRightIcon } from "@radix-ui/react-icons";
import useEmblaCarousel, {
  type UseEmblaCarouselType,
} from "embla-carousel-react";

import { cn } from "../../utils/cn";
import { Button } from "./button";

type CarouselApi = UseEmblaCarouselType[1];
type UseCarouselParameters = Parameters<typeof useEmblaCarousel>;
type CarouselOptions = UseCarouselParameters[0];
type CarouselPlugin = UseCarouselParameters[1];

type CarouselProps = {
  opts?: CarouselOptions;
  plugins?: CarouselPlugin;
  orientation?: "horizontal" | "vertical";
  setApi?: (api: CarouselApi) => void;
};

type CarouselContextProps = {
  carouselRef: ReturnType<typeof useEmblaCarousel>[0];
  api: ReturnType<typeof useEmblaCarousel>[1];
  scrollPrev: () => void;
  scrollNext: () => void;
  canScrollPrev: boolean;
  canScrollNext: boolean;
} & CarouselProps;

const CarouselContext = React.createContext<CarouselContextProps | null>(null);

function useCarousel() {
  const context = React.useContext(CarouselContext);

  if (!context) {
    throw new Error("useCarousel must be used within a <Carousel />");
  }

  return context;
}

const Carousel = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement> & CarouselProps
>(
  (
    {
      orientation = "horizontal",
      opts,
      setApi,
      plugins,
      className,
      children,
      ...props
    },
    ref,
  ) => {
    const [carouselRef, api] = useEmblaCarousel(
      {
        ...opts,
        axis: orientation === "horizontal" ? "x" : "y",
      },
      plugins,
    );
    const [canScrollPrev, setCanScrollPrev] = React.useState(false);
    const [canScrollNext, setCanScrollNext] = React.useState(false);

    const onSelect = React.useCallback((api: CarouselApi) => {
      if (!api) {
        return;
      }

      setCanScrollPrev(api.canScrollPrev());
      setCanScrollNext(api.canScrollNext());
    }, []);

    const scrollPrev = React.useCallback(() => {
      api?.scrollPrev();
    }, [api]);

    const scrollNext = React.useCallback(() => {
      api?.scrollNext();
    }, [api]);

    const handleKeyDown = React.useCallback(
      (event: React.KeyboardEvent<HTMLDivElement>) => {
        if (event.key === "ArrowLeft") {
          event.preventDefault();
          scrollPrev();
        } else if (event.key === "ArrowRight") {
          event.preventDefault();
          scrollNext();
        }
      },
      [scrollPrev, scrollNext],
    );

    React.useEffect(() => {
      if (!api || !setApi) {
        return;
      }

      setApi(api);
    }, [api, setApi]);

    React.useEffect(() => {
      if (!api) {
        return;
      }

      onSelect(api);
      api.on("reInit", onSelect);
      api.on("select", onSelect);

      return () => {
        api?.off("select", onSelect);
      };
    }, [api, onSelect]);

    return (
      <CarouselContext.Provider
        value={{
          carouselRef,
          api: api,
          opts,
          orientation:
            orientation || (opts?.axis === "y" ? "vertical" : "horizontal"),
          scrollPrev,
          scrollNext,
          canScrollPrev,
          canScrollNext,
        }}
      >
        <div
          ref={ref}
          onKeyDownCapture={handleKeyDown}
          className={cn("relative", className)}
          role="region"
          aria-roledescription="carousel"
          {...props}
        >
          {children}
        </div>
      </CarouselContext.Provider>
    );
  },
);
Carousel.displayName = "Carousel";

const CarouselContent = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => {
  const { carouselRef, orientation } = useCarousel();

  return (
    <div ref={carouselRef} className="overflow-hidden">
      <div
        ref={ref}
        className={cn(
          "flex",
          orientation === "horizontal" ? "-ml-4" : "-mt-4 flex-col",
          className,
        )}
        {...props}
      />
    </div>
  );
});
CarouselContent.displayName = "CarouselContent";

const CarouselItem = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => {
  const { orientation } = useCarousel();

  return (
    <div
      ref={ref}
      role="group"
      aria-roledescription="slide"
      className={cn(
        "min-w-0 shrink-0 grow-0 basis-full",
        orientation === "horizontal" ? "pl-4" : "pt-4",
        className,
      )}
      {...props}
    />
  );
});
CarouselItem.displayName = "CarouselItem";

const CarouselPrevious = React.forwardRef<
  HTMLButtonElement,
  React.ComponentProps<typeof Button>
>(({ className, variant = "outline", size = "icon", ...props }, ref) => {
  const { orientation, scrollPrev, canScrollPrev } = useCarousel();

  return (
    <Button
      ref={ref}
      variant={variant}
      size={size}
      className={cn(
        "absolute  h-8 w-8 rounded-full",
        orientation === "horizontal"
          ? "-left-12 top-1/2 -translate-y-1/2"
          : "-top-12 left-1/2 -translate-x-1/2 rotate-90",
        className,
      )}
      disabled={!canScrollPrev}
      onClick={scrollPrev}
      {...props}
    >
      <ArrowLeftIcon className="h-4 w-4 text-black" />
      <span className="sr-only">Previous slide</span>
    </Button>
  );
});
CarouselPrevious.displayName = "CarouselPrevious";

const CarouselNext = React.forwardRef<
  HTMLButtonElement,
  React.ComponentProps<typeof Button>
>(({ className, variant = "outline", size = "icon", ...props }, ref) => {
  const { orientation, scrollNext, canScrollNext } = useCarousel();

  return (
    <Button
      ref={ref}
      variant={variant}
      size={size}
      className={cn(
        "absolute h-8 w-8 rounded-full",
        orientation === "horizontal"
          ? "-right-12 top-1/2 -translate-y-1/2"
          : "-bottom-12 left-1/2 -translate-x-1/2 rotate-90",
        className,
      )}
      disabled={!canScrollNext}
      onClick={scrollNext}
      {...props}
    >
      <ArrowRightIcon className="h-4 w-4 text-black" />
      <span className="sr-only">Next slide</span>
    </Button>
  );
});
CarouselNext.displayName = "CarouselNext";

export {
  type CarouselApi,
  Carousel,
  CarouselContent,
  CarouselItem,
  CarouselPrevious,
  CarouselNext,
};



================================================
FILE: frontend/app/components/ui/collapsible.tsx
================================================
"use client";

import * as CollapsiblePrimitive from "@radix-ui/react-collapsible";

const Collapsible = CollapsiblePrimitive.Root;

const CollapsibleTrigger = CollapsiblePrimitive.CollapsibleTrigger;

const CollapsibleContent = CollapsiblePrimitive.CollapsibleContent;

export { Collapsible, CollapsibleTrigger, CollapsibleContent };



================================================
FILE: frontend/app/components/ui/dialog.tsx
================================================
"use client";

import * as React from "react";
import * as DialogPrimitive from "@radix-ui/react-dialog";
import { Cross2Icon } from "@radix-ui/react-icons";

import { cn } from "../../utils/cn";

const Dialog = DialogPrimitive.Root;

const DialogTrigger = DialogPrimitive.Trigger;

const DialogPortal = DialogPrimitive.Portal;

const DialogClose = DialogPrimitive.Close;

const DialogOverlay = React.forwardRef<
  React.ElementRef<typeof DialogPrimitive.Overlay>,
  React.ComponentPropsWithoutRef<typeof DialogPrimitive.Overlay>
>(({ className, ...props }, ref) => (
  <DialogPrimitive.Overlay
    ref={ref}
    className={cn(
      "fixed inset-0 z-50 bg-black/80  data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0",
      className,
    )}
    {...props}
  />
));
DialogOverlay.displayName = DialogPrimitive.Overlay.displayName;

const DialogContent = React.forwardRef<
  React.ElementRef<typeof DialogPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof DialogPrimitive.Content> & {
    hideCloseIcon?: boolean;
  }
>(({ className, children, hideCloseIcon = false, ...props }, ref) => (
  <DialogPortal>
    <DialogOverlay />
    <DialogPrimitive.Content
      ref={ref}
      className={cn(
        "fixed left-[50%] top-[50%] z-50 grid w-full max-w-lg translate-x-[-50%] translate-y-[-50%] gap-4 border bg-background p-6 shadow-lg duration-200 data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[state=closed]:slide-out-to-left-1/2 data-[state=closed]:slide-out-to-top-[48%] data-[state=open]:slide-in-from-left-1/2 data-[state=open]:slide-in-from-top-[48%] sm:rounded-lg",
        className,
      )}
      {...props}
    >
      {children}
      {!hideCloseIcon && (
        <DialogPrimitive.Close className="absolute right-4 top-4 rounded-sm opacity-70 ring-offset-background transition-opacity hover:opacity-100 focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 disabled:pointer-events-none data-[state=open]:bg-accent data-[state=open]:text-muted-foreground">
          <Cross2Icon className="h-4 w-4" />
          <span className="sr-only">Close</span>
        </DialogPrimitive.Close>
      )}
    </DialogPrimitive.Content>
  </DialogPortal>
));
DialogContent.displayName = DialogPrimitive.Content.displayName;

const DialogHeader = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) => (
  <div
    className={cn(
      "flex flex-col space-y-1.5 text-center sm:text-left",
      className,
    )}
    {...props}
  />
);
DialogHeader.displayName = "DialogHeader";

const DialogFooter = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) => (
  <div
    className={cn(
      "flex flex-col-reverse sm:flex-row sm:justify-end sm:space-x-2",
      className,
    )}
    {...props}
  />
);
DialogFooter.displayName = "DialogFooter";

const DialogTitle = React.forwardRef<
  React.ElementRef<typeof DialogPrimitive.Title>,
  React.ComponentPropsWithoutRef<typeof DialogPrimitive.Title>
>(({ className, ...props }, ref) => (
  <DialogPrimitive.Title
    ref={ref}
    className={cn(
      "text-lg font-semibold leading-none tracking-tight",
      className,
    )}
    {...props}
  />
));
DialogTitle.displayName = DialogPrimitive.Title.displayName;

const DialogDescription = React.forwardRef<
  React.ElementRef<typeof DialogPrimitive.Description>,
  React.ComponentPropsWithoutRef<typeof DialogPrimitive.Description>
>(({ className, ...props }, ref) => (
  <DialogPrimitive.Description
    ref={ref}
    className={cn("text-sm text-muted-foreground", className)}
    {...props}
  />
));
DialogDescription.displayName = DialogPrimitive.Description.displayName;

export {
  Dialog,
  DialogPortal,
  DialogOverlay,
  DialogTrigger,
  DialogClose,
  DialogContent,
  DialogHeader,
  DialogFooter,
  DialogTitle,
  DialogDescription,
};



================================================
FILE: frontend/app/components/ui/dropdown-menu.tsx
================================================
"use client";

import * as React from "react";
import * as DropdownMenuPrimitive from "@radix-ui/react-dropdown-menu";
import {
  CheckIcon,
  ChevronRightIcon,
  DotFilledIcon,
} from "@radix-ui/react-icons";

import { cn } from "../../utils/cn";

const DropdownMenu = DropdownMenuPrimitive.Root;

const DropdownMenuTrigger = DropdownMenuPrimitive.Trigger;

const DropdownMenuGroup = DropdownMenuPrimitive.Group;

const DropdownMenuPortal = DropdownMenuPrimitive.Portal;

const DropdownMenuSub = DropdownMenuPrimitive.Sub;

const DropdownMenuRadioGroup = DropdownMenuPrimitive.RadioGroup;

const DropdownMenuSubTrigger = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.SubTrigger>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.SubTrigger> & {
    inset?: boolean;
  }
>(({ className, inset, children, ...props }, ref) => (
  <DropdownMenuPrimitive.SubTrigger
    ref={ref}
    className={cn(
      "flex cursor-default select-none items-center rounded-sm px-2 py-1.5 text-sm outline-none focus:bg-accent data-[state=open]:bg-accent",
      inset && "pl-8",
      className,
    )}
    {...props}
  >
    {children}
    <ChevronRightIcon className="ml-auto h-4 w-4" />
  </DropdownMenuPrimitive.SubTrigger>
));
DropdownMenuSubTrigger.displayName =
  DropdownMenuPrimitive.SubTrigger.displayName;

const DropdownMenuSubContent = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.SubContent>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.SubContent>
>(({ className, ...props }, ref) => (
  <DropdownMenuPrimitive.SubContent
    ref={ref}
    className={cn(
      "z-50 min-w-[8rem] overflow-hidden rounded-md border bg-popover p-1 text-popover-foreground shadow-lg data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
      className,
    )}
    {...props}
  />
));
DropdownMenuSubContent.displayName =
  DropdownMenuPrimitive.SubContent.displayName;

const DropdownMenuContent = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Content>
>(({ className, sideOffset = 4, ...props }, ref) => (
  <DropdownMenuPrimitive.Portal>
    <DropdownMenuPrimitive.Content
      ref={ref}
      sideOffset={sideOffset}
      className={cn(
        "z-50 min-w-[8rem] overflow-hidden rounded-md border bg-popover p-1 text-popover-foreground shadow-md",
        "data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
        className,
      )}
      {...props}
    />
  </DropdownMenuPrimitive.Portal>
));
DropdownMenuContent.displayName = DropdownMenuPrimitive.Content.displayName;

const DropdownMenuItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Item> & {
    inset?: boolean;
  }
>(({ className, inset, ...props }, ref) => (
  <DropdownMenuPrimitive.Item
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm px-2 py-1.5 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      inset && "pl-8",
      className,
    )}
    {...props}
  />
));
DropdownMenuItem.displayName = DropdownMenuPrimitive.Item.displayName;

const DropdownMenuCheckboxItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.CheckboxItem>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.CheckboxItem>
>(({ className, children, checked, ...props }, ref) => (
  <DropdownMenuPrimitive.CheckboxItem
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className,
    )}
    checked={checked}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <DropdownMenuPrimitive.ItemIndicator>
        <CheckIcon className="h-4 w-4" />
      </DropdownMenuPrimitive.ItemIndicator>
    </span>
    {children}
  </DropdownMenuPrimitive.CheckboxItem>
));
DropdownMenuCheckboxItem.displayName =
  DropdownMenuPrimitive.CheckboxItem.displayName;

const DropdownMenuRadioItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.RadioItem>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.RadioItem>
>(({ className, children, ...props }, ref) => (
  <DropdownMenuPrimitive.RadioItem
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className,
    )}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <DropdownMenuPrimitive.ItemIndicator>
        <DotFilledIcon className="h-4 w-4 fill-current" />
      </DropdownMenuPrimitive.ItemIndicator>
    </span>
    {children}
  </DropdownMenuPrimitive.RadioItem>
));
DropdownMenuRadioItem.displayName = DropdownMenuPrimitive.RadioItem.displayName;

const DropdownMenuLabel = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Label>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Label> & {
    inset?: boolean;
  }
>(({ className, inset, ...props }, ref) => (
  <DropdownMenuPrimitive.Label
    ref={ref}
    className={cn(
      "px-2 py-1.5 text-sm font-semibold",
      inset && "pl-8",
      className,
    )}
    {...props}
  />
));
DropdownMenuLabel.displayName = DropdownMenuPrimitive.Label.displayName;

const DropdownMenuSeparator = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Separator>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Separator>
>(({ className, ...props }, ref) => (
  <DropdownMenuPrimitive.Separator
    ref={ref}
    className={cn("-mx-1 my-1 h-px bg-muted", className)}
    {...props}
  />
));
DropdownMenuSeparator.displayName = DropdownMenuPrimitive.Separator.displayName;

const DropdownMenuShortcut = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLSpanElement>) => {
  return (
    <span
      className={cn("ml-auto text-xs tracking-widest opacity-60", className)}
      {...props}
    />
  );
};
DropdownMenuShortcut.displayName = "DropdownMenuShortcut";

export {
  DropdownMenu,
  DropdownMenuTrigger,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuCheckboxItem,
  DropdownMenuRadioItem,
  DropdownMenuLabel,
  DropdownMenuSeparator,
  DropdownMenuShortcut,
  DropdownMenuGroup,
  DropdownMenuPortal,
  DropdownMenuSub,
  DropdownMenuSubContent,
  DropdownMenuSubTrigger,
  DropdownMenuRadioGroup,
};



================================================
FILE: frontend/app/components/ui/input.tsx
================================================
import * as React from "react";

import { cn } from "../../utils/cn";

export interface InputProps
  extends React.InputHTMLAttributes<HTMLInputElement> {}

const Input = React.forwardRef<HTMLInputElement, InputProps>(
  ({ className, type, ...props }, ref) => {
    return (
      <input
        type={type}
        className={cn(
          "flex h-9 w-full rounded-md border border-input bg-transparent px-3 py-1 text-sm shadow-sm transition-colors file:border-0 file:bg-transparent file:text-sm file:font-medium file:text-foreground placeholder:text-muted-foreground focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:cursor-not-allowed disabled:opacity-50",
          className,
        )}
        ref={ref}
        {...props}
      />
    );
  },
);
Input.displayName = "Input";

export { Input };



================================================
FILE: frontend/app/components/ui/progress.tsx
================================================
"use client";

import * as React from "react";
import * as ProgressPrimitive from "@radix-ui/react-progress";

import { cn } from "../../utils/cn";

const Progress = React.forwardRef<
  React.ElementRef<typeof ProgressPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof ProgressPrimitive.Root> & {
    indicatorClassName?: string;
  }
>(({ className, value, indicatorClassName, ...props }, ref) => (
  <ProgressPrimitive.Root
    ref={ref}
    className={cn(
      "relative h-2 w-full overflow-hidden rounded-full bg-primary/20",
      className,
    )}
    {...props}
  >
    <ProgressPrimitive.Indicator
      className={cn(
        "h-full w-full flex-1 transition-all",
        indicatorClassName || "bg-primary",
      )}
      style={{ transform: `translateX(-${100 - (value || 0)}%)` }}
    />
  </ProgressPrimitive.Root>
));
Progress.displayName = ProgressPrimitive.Root.displayName;

export { Progress };



================================================
FILE: frontend/app/components/ui/select.tsx
================================================
"use client";

import * as React from "react";
import {
  CaretSortIcon,
  CheckIcon,
  ChevronDownIcon,
  ChevronUpIcon,
} from "@radix-ui/react-icons";
import * as SelectPrimitive from "@radix-ui/react-select";

import { cn } from "../../utils/cn";

const Select = SelectPrimitive.Root;

const SelectGroup = SelectPrimitive.Group;

const SelectValue = SelectPrimitive.Value;

const SelectTrigger = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Trigger>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Trigger>
>(({ className, children, ...props }, ref) => (
  <SelectPrimitive.Trigger
    ref={ref}
    className={cn(
      "flex h-9 w-full items-center justify-between whitespace-nowrap rounded-md border border-input bg-transparent px-3 py-2 text-sm shadow-sm ring-offset-background placeholder:text-muted-foreground focus:outline-none focus:ring-1 focus:ring-ring disabled:cursor-not-allowed disabled:opacity-50 [&>span]:line-clamp-1",
      className,
    )}
    {...props}
  >
    {children}
    <SelectPrimitive.Icon asChild>
      <CaretSortIcon className="h-4 w-4 opacity-50" />
    </SelectPrimitive.Icon>
  </SelectPrimitive.Trigger>
));
SelectTrigger.displayName = SelectPrimitive.Trigger.displayName;

const SelectScrollUpButton = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.ScrollUpButton>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.ScrollUpButton>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.ScrollUpButton
    ref={ref}
    className={cn(
      "flex cursor-default items-center justify-center py-1",
      className,
    )}
    {...props}
  >
    <ChevronUpIcon />
  </SelectPrimitive.ScrollUpButton>
));
SelectScrollUpButton.displayName = SelectPrimitive.ScrollUpButton.displayName;

const SelectScrollDownButton = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.ScrollDownButton>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.ScrollDownButton>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.ScrollDownButton
    ref={ref}
    className={cn(
      "flex cursor-default items-center justify-center py-1",
      className,
    )}
    {...props}
  >
    <ChevronDownIcon />
  </SelectPrimitive.ScrollDownButton>
));
SelectScrollDownButton.displayName =
  SelectPrimitive.ScrollDownButton.displayName;

const SelectContent = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Content>
>(({ className, children, position = "popper", ...props }, ref) => (
  <SelectPrimitive.Portal>
    <SelectPrimitive.Content
      ref={ref}
      className={cn(
        "relative z-50 max-h-96 min-w-[8rem] overflow-hidden rounded-md border bg-popover text-popover-foreground shadow-md data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
        position === "popper" &&
          "data-[side=bottom]:translate-y-1 data-[side=left]:-translate-x-1 data-[side=right]:translate-x-1 data-[side=top]:-translate-y-1",
        className,
      )}
      position={position}
      {...props}
    >
      <SelectScrollUpButton />
      <SelectPrimitive.Viewport
        className={cn(
          "p-1",
          position === "popper" &&
            "h-[var(--radix-select-trigger-height)] w-full min-w-[var(--radix-select-trigger-width)]",
        )}
      >
        {children}
      </SelectPrimitive.Viewport>
      <SelectScrollDownButton />
    </SelectPrimitive.Content>
  </SelectPrimitive.Portal>
));
SelectContent.displayName = SelectPrimitive.Content.displayName;

const SelectLabel = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Label>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Label>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.Label
    ref={ref}
    className={cn("px-2 py-1.5 text-sm font-semibold", className)}
    {...props}
  />
));
SelectLabel.displayName = SelectPrimitive.Label.displayName;

const SelectItem = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Item>
>(({ className, children, ...props }, ref) => (
  <SelectPrimitive.Item
    ref={ref}
    className={cn(
      "relative flex w-full cursor-default select-none items-center rounded-sm py-1.5 pl-2 pr-8 text-sm outline-none focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className,
    )}
    {...props}
  >
    <span className="absolute right-2 flex h-3.5 w-3.5 items-center justify-center">
      <SelectPrimitive.ItemIndicator>
        <CheckIcon className="h-4 w-4" />
      </SelectPrimitive.ItemIndicator>
    </span>
    <SelectPrimitive.ItemText>{children}</SelectPrimitive.ItemText>
  </SelectPrimitive.Item>
));
SelectItem.displayName = SelectPrimitive.Item.displayName;

const SelectSeparator = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Separator>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Separator>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.Separator
    ref={ref}
    className={cn("-mx-1 my-1 h-px bg-muted", className)}
    {...props}
  />
));
SelectSeparator.displayName = SelectPrimitive.Separator.displayName;

export {
  Select,
  SelectGroup,
  SelectValue,
  SelectTrigger,
  SelectContent,
  SelectLabel,
  SelectItem,
  SelectSeparator,
  SelectScrollUpButton,
  SelectScrollDownButton,
};



================================================
FILE: frontend/app/components/ui/sheet.tsx
================================================
"use client";

import * as React from "react";
import * as SheetPrimitive from "@radix-ui/react-dialog";
import { Cross2Icon } from "@radix-ui/react-icons";
import { cva, type VariantProps } from "class-variance-authority";

import { cn } from "../../utils/cn";

const Sheet = SheetPrimitive.Root;

const SheetTrigger = SheetPrimitive.Trigger;

const SheetClose = SheetPrimitive.Close;

const SheetPortal = SheetPrimitive.Portal;

const SheetOverlay = React.forwardRef<
  React.ElementRef<typeof SheetPrimitive.Overlay>,
  React.ComponentPropsWithoutRef<typeof SheetPrimitive.Overlay>
>(({ className, ...props }, ref) => (
  <SheetPrimitive.Overlay
    className={cn(
      "fixed inset-0 z-50 bg-black/80  data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0",
      className,
    )}
    {...props}
    ref={ref}
  />
));
SheetOverlay.displayName = SheetPrimitive.Overlay.displayName;

const sheetVariants = cva(
  "fixed z-50 gap-4 bg-background p-6 shadow-lg transition ease-in-out data-[state=closed]:duration-300 data-[state=open]:duration-500 data-[state=open]:animate-in data-[state=closed]:animate-out",
  {
    variants: {
      side: {
        top: "inset-x-0 top-0 border-b data-[state=closed]:slide-out-to-top data-[state=open]:slide-in-from-top",
        bottom:
          "inset-x-0 bottom-0 border-t data-[state=closed]:slide-out-to-bottom data-[state=open]:slide-in-from-bottom",
        left: "inset-y-0 left-0 h-full w-3/4 border-r data-[state=closed]:slide-out-to-left data-[state=open]:slide-in-from-left sm:max-w-sm",
        right:
          "inset-y-0 right-0 h-full w-3/4 border-l data-[state=closed]:slide-out-to-right data-[state=open]:slide-in-from-right sm:max-w-sm",
      },
    },
    defaultVariants: {
      side: "right",
    },
  },
);

interface SheetContentProps
  extends React.ComponentPropsWithoutRef<typeof SheetPrimitive.Content>,
    VariantProps<typeof sheetVariants> {}

const SheetContent = React.forwardRef<
  React.ElementRef<typeof SheetPrimitive.Content>,
  SheetContentProps
>(({ side = "right", className, children, ...props }, ref) => (
  <SheetPortal>
    <SheetOverlay />
    <SheetPrimitive.Content
      ref={ref}
      className={cn(sheetVariants({ side }), className)}
      {...props}
    >
      <SheetPrimitive.Close className="absolute right-4 top-4 rounded-sm opacity-70 ring-offset-background transition-opacity hover:opacity-100 focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 disabled:pointer-events-none data-[state=open]:bg-secondary">
        <Cross2Icon className="h-4 w-4" />
        <span className="sr-only">Close</span>
      </SheetPrimitive.Close>
      {children}
    </SheetPrimitive.Content>
  </SheetPortal>
));
SheetContent.displayName = SheetPrimitive.Content.displayName;

const SheetHeader = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) => (
  <div
    className={cn(
      "flex flex-col space-y-2 text-center sm:text-left",
      className,
    )}
    {...props}
  />
);
SheetHeader.displayName = "SheetHeader";

const SheetFooter = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) => (
  <div
    className={cn(
      "flex flex-col-reverse sm:flex-row sm:justify-end sm:space-x-2",
      className,
    )}
    {...props}
  />
);
SheetFooter.displayName = "SheetFooter";

const SheetTitle = React.forwardRef<
  React.ElementRef<typeof SheetPrimitive.Title>,
  React.ComponentPropsWithoutRef<typeof SheetPrimitive.Title>
>(({ className, ...props }, ref) => (
  <SheetPrimitive.Title
    ref={ref}
    className={cn("text-lg font-semibold text-foreground", className)}
    {...props}
  />
));
SheetTitle.displayName = SheetPrimitive.Title.displayName;

const SheetDescription = React.forwardRef<
  React.ElementRef<typeof SheetPrimitive.Description>,
  React.ComponentPropsWithoutRef<typeof SheetPrimitive.Description>
>(({ className, ...props }, ref) => (
  <SheetPrimitive.Description
    ref={ref}
    className={cn("text-sm text-muted-foreground", className)}
    {...props}
  />
));
SheetDescription.displayName = SheetPrimitive.Description.displayName;

export {
  Sheet,
  SheetPortal,
  SheetOverlay,
  SheetTrigger,
  SheetClose,
  SheetContent,
  SheetHeader,
  SheetFooter,
  SheetTitle,
  SheetDescription,
};



================================================
FILE: frontend/app/components/ui/skeleton.tsx
================================================
import { cn } from "../../utils/cn";

function Skeleton({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) {
  return (
    <div
      className={cn("animate-pulse rounded-md bg-primary/10", className)}
      {...props}
    />
  );
}

export { Skeleton };



================================================
FILE: frontend/app/components/ui/textarea.tsx
================================================
import * as React from "react";

import { cn } from "../../utils/cn";

export interface TextareaProps
  extends React.TextareaHTMLAttributes<HTMLTextAreaElement> {}

const Textarea = React.forwardRef<HTMLTextAreaElement, TextareaProps>(
  ({ className, ...props }, ref) => {
    return (
      <textarea
        className={cn(
          "flex min-h-[60px] w-full rounded-md border border-input bg-transparent px-3 py-2 text-sm shadow-sm placeholder:text-muted-foreground focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:cursor-not-allowed disabled:opacity-50",
          className,
        )}
        ref={ref}
        {...props}
      />
    );
  },
);
Textarea.displayName = "Textarea";

export { Textarea };



================================================
FILE: frontend/app/components/ui/toast.tsx
================================================
"use client";

import * as React from "react";
import { Cross2Icon } from "@radix-ui/react-icons";
import * as ToastPrimitives from "@radix-ui/react-toast";
import { cva, type VariantProps } from "class-variance-authority";

import { cn } from "../../utils/cn";

const ToastProvider = ToastPrimitives.Provider;

const ToastViewport = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Viewport>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Viewport>
>(({ className, ...props }, ref) => (
  <ToastPrimitives.Viewport
    ref={ref}
    className={cn(
      "fixed top-0 z-[100] flex max-h-screen w-full flex-col-reverse p-4 sm:bottom-0 sm:right-0 sm:top-auto sm:flex-col md:max-w-[420px]",
      className,
    )}
    {...props}
  />
));
ToastViewport.displayName = ToastPrimitives.Viewport.displayName;

const toastVariants = cva(
  "group pointer-events-auto relative flex w-full items-center justify-between space-x-2 overflow-hidden rounded-md border p-4 pr-6 shadow-lg transition-all data-[swipe=cancel]:translate-x-0 data-[swipe=end]:translate-x-[var(--radix-toast-swipe-end-x)] data-[swipe=move]:translate-x-[var(--radix-toast-swipe-move-x)] data-[swipe=move]:transition-none data-[state=open]:animate-in data-[state=closed]:animate-out data-[swipe=end]:animate-out data-[state=closed]:fade-out-80 data-[state=closed]:slide-out-to-right-full data-[state=open]:slide-in-from-top-full data-[state=open]:sm:slide-in-from-bottom-full",
  {
    variants: {
      variant: {
        default: "border bg-background text-foreground",
        destructive:
          "destructive group border-destructive bg-destructive text-destructive-foreground",
      },
    },
    defaultVariants: {
      variant: "default",
    },
  },
);

const Toast = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Root>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Root> &
    VariantProps<typeof toastVariants>
>(({ className, variant, ...props }, ref) => {
  return (
    <ToastPrimitives.Root
      ref={ref}
      className={cn(toastVariants({ variant }), className)}
      {...props}
    />
  );
});
Toast.displayName = ToastPrimitives.Root.displayName;

const ToastAction = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Action>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Action>
>(({ className, ...props }, ref) => (
  <ToastPrimitives.Action
    ref={ref}
    className={cn(
      "inline-flex h-8 shrink-0 items-center justify-center rounded-md border bg-transparent px-3 text-sm font-medium transition-colors hover:bg-secondary focus:outline-none focus:ring-1 focus:ring-ring disabled:pointer-events-none disabled:opacity-50 group-[.destructive]:border-muted/40 group-[.destructive]:hover:border-destructive/30 group-[.destructive]:hover:bg-destructive group-[.destructive]:hover:text-destructive-foreground group-[.destructive]:focus:ring-destructive",
      className,
    )}
    {...props}
  />
));
ToastAction.displayName = ToastPrimitives.Action.displayName;

const ToastClose = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Close>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Close>
>(({ className, ...props }, ref) => (
  <ToastPrimitives.Close
    ref={ref}
    className={cn(
      "absolute right-1 top-1 rounded-md p-1 text-foreground/50 opacity-0 transition-opacity hover:text-foreground focus:opacity-100 focus:outline-none focus:ring-1 group-hover:opacity-100 group-[.destructive]:text-red-300 group-[.destructive]:hover:text-red-50 group-[.destructive]:focus:ring-red-400 group-[.destructive]:focus:ring-offset-red-600",
      className,
    )}
    toast-close=""
    {...props}
  >
    <Cross2Icon className="h-4 w-4" />
  </ToastPrimitives.Close>
));
ToastClose.displayName = ToastPrimitives.Close.displayName;

const ToastTitle = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Title>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Title>
>(({ className, ...props }, ref) => (
  <ToastPrimitives.Title
    ref={ref}
    className={cn("text-sm font-semibold [&+div]:text-xs", className)}
    {...props}
  />
));
ToastTitle.displayName = ToastPrimitives.Title.displayName;

const ToastDescription = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Description>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Description>
>(({ className, ...props }, ref) => (
  <ToastPrimitives.Description
    ref={ref}
    className={cn("text-sm opacity-90", className)}
    {...props}
  />
));
ToastDescription.displayName = ToastPrimitives.Description.displayName;

type ToastProps = React.ComponentPropsWithoutRef<typeof Toast>;

type ToastActionElement = React.ReactElement<typeof ToastAction>;

export {
  type ToastProps,
  type ToastActionElement,
  ToastProvider,
  ToastViewport,
  Toast,
  ToastTitle,
  ToastDescription,
  ToastClose,
  ToastAction,
};



================================================
FILE: frontend/app/components/ui/toaster.tsx
================================================
"use client";

import { useToast } from "../../hooks/use-toast";
import {
  Toast,
  ToastClose,
  ToastDescription,
  ToastProvider,
  ToastTitle,
  ToastViewport,
} from "./toast";

export function Toaster() {
  const { toasts } = useToast();

  return (
    <ToastProvider>
      {toasts.map(function ({ id, title, description, action, ...props }) {
        return (
          <Toast key={id} {...props}>
            <div className="grid gap-1">
              {title && <ToastTitle>{title}</ToastTitle>}
              {description && (
                <ToastDescription>{description}</ToastDescription>
              )}
            </div>
            {action}
            <ToastClose />
          </Toast>
        );
      })}
      <ToastViewport />
    </ToastProvider>
  );
}



================================================
FILE: frontend/app/components/ui/tooltip.tsx
================================================
"use client";

import * as React from "react";
import * as TooltipPrimitive from "@radix-ui/react-tooltip";

import { cn } from "../../utils/cn";

const TooltipProvider = TooltipPrimitive.Provider;

const Tooltip = TooltipPrimitive.Root;

const TooltipTrigger = TooltipPrimitive.Trigger;

const TooltipContent = React.forwardRef<
  React.ElementRef<typeof TooltipPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof TooltipPrimitive.Content>
>(({ className, sideOffset = 4, ...props }, ref) => (
  <TooltipPrimitive.Content
    ref={ref}
    sideOffset={sideOffset}
    className={cn(
      "z-50 overflow-hidden rounded-md bg-primary px-3 py-1.5 text-xs text-primary-foreground animate-in fade-in-0 zoom-in-95 data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=closed]:zoom-out-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
      className,
    )}
    {...props}
  />
));
TooltipContent.displayName = TooltipPrimitive.Content.displayName;

export { Tooltip, TooltipTrigger, TooltipContent, TooltipProvider };



================================================
FILE: frontend/app/components/ui/assistant-ui/markdown-text.tsx
================================================
"use client";

import {
  CodeHeaderProps,
  MarkdownTextPrimitive,
  useIsMarkdownCodeBlock,
} from "@assistant-ui/react-markdown";
import remarkGfm from "remark-gfm";
import rehypeKatex from "rehype-katex";
import remarkMath from "remark-math";
import { FC, memo, useState } from "react";
import { CheckIcon, CopyIcon, ExternalLink } from "lucide-react";

import { TooltipIconButton } from "./tooltip-icon-button";
import { SyntaxHighlighter } from "./syntax-highlighter";
import { cn } from "../../../utils/cn";

import "katex/dist/katex.min.css";

const MarkdownTextImpl = () => {
  return (
    <MarkdownTextPrimitive
      remarkPlugins={[remarkGfm, remarkMath]}
      rehypePlugins={[rehypeKatex]}
      components={{
        h1: ({ node: _node, className, ...props }) => (
          <h1
            className={cn(
              "mb-8 scroll-m-20 text-4xl font-extrabold tracking-tight last:mb-0",
              className,
            )}
            {...props}
          />
        ),
        h2: ({ node: _node, className, ...props }) => (
          <h2
            className={cn(
              "mb-4 mt-8 scroll-m-20 text-3xl font-semibold tracking-tight first:mt-0 last:mb-0",
              className,
            )}
            {...props}
          />
        ),
        h3: ({ node: _node, className, ...props }) => (
          <h3
            className={cn(
              "mb-4 mt-6 scroll-m-20 text-2xl font-semibold tracking-tight first:mt-0 last:mb-0",
              className,
            )}
            {...props}
          />
        ),
        h4: ({ node: _node, className, ...props }) => (
          <h4
            className={cn(
              "mb-4 mt-6 scroll-m-20 text-xl font-semibold tracking-tight first:mt-0 last:mb-0",
              className,
            )}
            {...props}
          />
        ),
        h5: ({ node: _node, className, ...props }) => (
          <h5
            className={cn(
              "my-4 text-lg font-semibold first:mt-0 last:mb-0",
              className,
            )}
            {...props}
          />
        ),
        h6: ({ node: _node, className, ...props }) => (
          <h6
            className={cn("my-4 font-semibold first:mt-0 last:mb-0", className)}
            {...props}
          />
        ),
        p: ({ node: _node, className, ...props }) => (
          <p
            className={cn(
              "mb-5 mt-5 leading-7 first:mt-0 last:mb-0",
              className,
            )}
            {...props}
          />
        ),
        a: ({ node: _node, className, ...props }) => (
          <a
            target="_blank"
            className={cn(
              "text-primary text-blue-400 font-medium underline underline-offset-4 inline-flex items-baseline relative pr-[1.1em]",
              className,
            )}
            {...props}
          >
            {props.children}
            <ExternalLink className="w-3 h-3 absolute top-1 right-1" />
          </a>
        ),
        blockquote: ({ node: _node, className, ...props }) => (
          <blockquote
            className={cn("border-l-2 pl-6 italic", className)}
            {...props}
          />
        ),
        ul: ({ node: _node, className, ...props }) => (
          <ul
            className={cn("my-5 ml-6 list-disc [&>li]:mt-2", className)}
            {...props}
          />
        ),
        ol: ({ node: _node, className, ...props }) => (
          <ol
            className={cn("my-5 ml-6 list-decimal [&>li]:mt-2", className)}
            {...props}
          />
        ),
        hr: ({ node: _node, className, ...props }) => (
          <hr className={cn("my-5 border-b", className)} {...props} />
        ),
        table: ({ node: _node, className, ...props }) => (
          <table
            className={cn(
              "my-5 w-full border-separate border-spacing-0 overflow-y-auto",
              className,
            )}
            {...props}
          />
        ),
        th: ({ node: _node, className, ...props }) => (
          <th
            className={cn(
              "bg-muted px-4 py-2 text-left font-bold first:rounded-tl-lg last:rounded-tr-lg [&[align=center]]:text-center [&[align=right]]:text-right",
              className,
            )}
            {...props}
          />
        ),
        td: ({ node: _node, className, ...props }) => (
          <td
            className={cn(
              "border-b border-l px-4 py-2 text-left last:border-r [&[align=center]]:text-center [&[align=right]]:text-right",
              className,
            )}
            {...props}
          />
        ),
        tr: ({ node: _node, className, ...props }) => (
          <tr
            className={cn(
              "m-0 border-b p-0 first:border-t [&:last-child>td:first-child]:rounded-bl-lg [&:last-child>td:last-child]:rounded-br-lg",
              className,
            )}
            {...props}
          />
        ),
        sup: ({ node: _node, className, ...props }) => (
          <sup
            className={cn("[&>a]:text-xs [&>a]:no-underline", className)}
            {...props}
          />
        ),
        pre: ({ node: _node, className, ...props }) => (
          <pre
            className={cn(
              "overflow-x-auto rounded-b-lg bg-black p-4 text-white",
              className,
            )}
            {...props}
          />
        ),
        code: function Code({ node: _node, className, ...props }) {
          const isCodeBlock = useIsMarkdownCodeBlock();
          return (
            <code
              className={cn(
                !isCodeBlock && "bg-aui-muted rounded border font-semibold",
                className,
              )}
              {...props}
            />
          );
        },
        CodeHeader,
        SyntaxHighlighter,
      }}
    />
  );
};

export const MarkdownText = memo(MarkdownTextImpl);

const CodeHeader: FC<CodeHeaderProps> = ({ language, code }) => {
  const { isCopied, copyToClipboard } = useCopyToClipboard();
  const onCopy = () => {
    if (!code || isCopied) return;
    copyToClipboard(code);
  };

  return (
    <div className="flex items-center justify-between gap-4 rounded-t-lg bg-zinc-900 px-4 py-2 text-sm font-semibold text-white">
      <span className="lowercase [&>span]:text-xs">{language}</span>
      <TooltipIconButton tooltip="Copy" onClick={onCopy}>
        {!isCopied && <CopyIcon />}
        {isCopied && <CheckIcon />}
      </TooltipIconButton>
    </div>
  );
};

const useCopyToClipboard = ({
  copiedDuration = 3000,
}: {
  copiedDuration?: number;
} = {}) => {
  const [isCopied, setIsCopied] = useState<boolean>(false);

  const copyToClipboard = (value: string) => {
    if (!value) return;

    navigator.clipboard.writeText(value).then(() => {
      setIsCopied(true);
      setTimeout(() => setIsCopied(false), copiedDuration);
    });
  };

  return { isCopied, copyToClipboard };
};



================================================
FILE: frontend/app/components/ui/assistant-ui/syntax-highlighter.tsx
================================================
import { PrismAsyncLight } from "react-syntax-highlighter";
import { makePrismAsyncLightSyntaxHighlighter } from "@assistant-ui/react-syntax-highlighter";

import tsx from "react-syntax-highlighter/dist/esm/languages/prism/tsx";
import python from "react-syntax-highlighter/dist/esm/languages/prism/python";

import { coldarkDark } from "react-syntax-highlighter/dist/cjs/styles/prism";

// register languages you want to support
PrismAsyncLight.registerLanguage("js", tsx);
PrismAsyncLight.registerLanguage("jsx", tsx);
PrismAsyncLight.registerLanguage("ts", tsx);
PrismAsyncLight.registerLanguage("tsx", tsx);
PrismAsyncLight.registerLanguage("python", python);

export const SyntaxHighlighter = makePrismAsyncLightSyntaxHighlighter({
  style: coldarkDark,
  customStyle: {
    margin: 0,
    width: "100%",
    background: "#2b2b2b",
    padding: "1.5rem 1rem",
  },
});



================================================
FILE: frontend/app/components/ui/assistant-ui/tooltip-icon-button.tsx
================================================
"use client";

import { forwardRef } from "react";

import {
  Tooltip,
  TooltipContent,
  TooltipProvider,
  TooltipTrigger,
} from "../tooltip";
import { Button, ButtonProps } from "../button";
import { cn } from "../../../utils/cn";

export type TooltipIconButtonProps = ButtonProps & {
  tooltip: string | React.ReactNode;
  side?: "top" | "bottom" | "left" | "right";
  /**
   * @default 700
   */
  delayDuration?: number;
};

export const TooltipIconButton = forwardRef<
  HTMLButtonElement,
  TooltipIconButtonProps
>(
  (
    { children, tooltip, side = "bottom", className, delayDuration, ...rest },
    ref,
  ) => {
    return (
      <TooltipProvider>
        <Tooltip delayDuration={delayDuration ?? 700}>
          <TooltipTrigger asChild>
            <Button
              variant="ghost"
              size="icon"
              {...rest}
              className={cn("size-6 p-1", className)}
              ref={ref}
            >
              {children}
              <span className="sr-only">{tooltip}</span>
            </Button>
          </TooltipTrigger>
          <TooltipContent side={side}>{tooltip}</TooltipContent>
        </Tooltip>
      </TooltipProvider>
    );
  },
);

TooltipIconButton.displayName = "TooltipIconButton";



================================================
FILE: frontend/app/contexts/GraphContext.tsx
================================================
"use client";

import { parsePartialJson } from "@langchain/core/output_parsers";
import {
  createContext,
  Dispatch,
  ReactNode,
  SetStateAction,
  useContext,
  useState,
} from "react";
import { AIMessage, BaseMessage, HumanMessage } from "@langchain/core/messages";
import { useToast } from "../hooks/use-toast";
import { v4 as uuidv4 } from "uuid";

import { useThreads } from "../hooks/useThreads";
import { ModelOptions } from "../types";
import { useRuns } from "../hooks/useRuns";
import { useUser } from "../hooks/useUser";
import { addDocumentLinks, createClient, nodeToStep } from "./utils";
import { Thread } from "@langchain/langgraph-sdk";
import { useQueryState } from "nuqs";

interface GraphData {
  runId: string;
  isStreaming: boolean;
  messages: BaseMessage[];
  selectedModel: ModelOptions;
  setSelectedModel: Dispatch<SetStateAction<ModelOptions>>;
  setMessages: Dispatch<SetStateAction<BaseMessage[]>>;
  streamMessage: (currentThreadId: string, params: GraphInput) => Promise<void>;
  switchSelectedThread: (thread: Thread) => void;
}

type UserDataContextType = ReturnType<typeof useUser>;

type ThreadsDataContextType = ReturnType<typeof useThreads>;

type GraphContentType = {
  graphData: GraphData;
  userData: UserDataContextType;
  threadsData: ThreadsDataContextType;
};

const GraphContext = createContext<GraphContentType | undefined>(undefined);

export interface GraphInput {
  messages?: Record<string, any>[];
}

export function GraphProvider({ children }: { children: ReactNode }) {
  const { userId } = useUser();
  const {
    isUserThreadsLoading,
    userThreads,
    getThreadById,
    setUserThreads,
    getUserThreads,
    createThread,
    deleteThread,
  } = useThreads(userId);
  const [runId, setRunId] = useState("");
  const [isStreaming, setIsStreaming] = useState(false);
  const { toast } = useToast();
  const { shareRun } = useRuns();
  const [messages, setMessages] = useState<BaseMessage[]>([]);
  const [selectedModel, setSelectedModel] = useState<ModelOptions>(
    "openai/gpt-4.1-mini",
  );
  const [_threadId, setThreadId] = useQueryState("threadId");

  const streamMessage = async (
    currentThreadId: string,
    params: GraphInput,
  ): Promise<void> => {
    if (!userId) {
      toast({
        title: "Error",
        description: "User ID not found",
      });
      return;
    }
    const client = createClient();

    const input = {
      messages: params.messages?.filter((msg) => {
        if (msg.role !== "assistant") {
          return true;
        }
        const aiMsg = msg as AIMessage;
        // Filter our artifact ui tool calls from going to the server.
        if (
          aiMsg.tool_calls &&
          aiMsg.tool_calls.some((tc) => tc.name === "artifact_ui")
        ) {
          return false;
        }
        return true;
      }),
    };

    // Clear the runId from the state
    setRunId("");

    const stream = client.runs.stream(currentThreadId, "chat", {
      input,
      streamMode: "events",
      config: {
        configurable: {
          query_model: selectedModel,
          response_model: selectedModel,
        },
      },
    });

    setIsStreaming(true);

    try {
      let _runId: string | undefined = undefined;
      let fullRoutingStr = "";
      let generatingQuestionsMessageId: string | undefined = undefined;
      let fullGeneratingQuestionsStr = "";
      const progressAIMessageId = uuidv4();
      let hasProgressBeenSet = false;

      for await (const chunk of stream) {
        if (!runId && chunk.data?.metadata?.run_id) {
          _runId = chunk.data.metadata.run_id;
          setRunId(_runId ?? "");
        }
        if (!hasProgressBeenSet) {
          setMessages((prevMessages) => {
            const existingMessageIndex = prevMessages.findIndex(
              (msg) => msg.id === progressAIMessageId,
            );

            if (existingMessageIndex !== -1) {
              return [
                ...prevMessages.slice(0, existingMessageIndex),
                new AIMessage({
                  id: progressAIMessageId,
                  content: "",
                  tool_calls: [
                    {
                      name: "progress",
                      args: {
                        step: nodeToStep(chunk?.data?.metadata?.langgraph_node),
                      },
                    },
                  ],
                }),
                ...prevMessages.slice(existingMessageIndex + 1),
              ];
            } else {
              console.warn(
                "Progress message ID is defined but not found in messages",
              );
              const newMessage = new AIMessage({
                id: progressAIMessageId,
                content: "",
                tool_calls: [
                  {
                    name: "progress",
                    args: {
                      step: nodeToStep(chunk?.data?.metadata?.langgraph_node),
                    },
                  },
                ],
              });
              return [...prevMessages, newMessage];
            }
          });
          hasProgressBeenSet = true;
        }

        if (chunk.data.event === "on_chain_start") {
          const node = chunk?.data?.metadata?.langgraph_node;
          if (
            [
              "analyze_and_route_query",
              "create_research_plan",
              "conduct_research",
              "respond",
            ].includes(node)
          ) {
            setMessages((prevMessages) => {
              const existingMessageIndex = prevMessages.findIndex(
                (msg) => msg.id === progressAIMessageId,
              );

              if (existingMessageIndex !== -1) {
                return [
                  ...prevMessages.slice(0, existingMessageIndex),
                  new AIMessage({
                    id: progressAIMessageId,
                    content: "",
                    tool_calls: [
                      {
                        name: "progress",
                        args: {
                          step: nodeToStep(node),
                        },
                      },
                    ],
                  }),
                  ...prevMessages.slice(existingMessageIndex + 1),
                ];
              } else {
                console.warn(
                  "Progress message ID is defined but not found in messages",
                );
                return prevMessages;
              }
            });
          }

          if (node === "respond") {
            setMessages((prevMessages) => {
              const selectedDocumentsAIMessage = new AIMessage({
                content: "",
                tool_calls: [
                  {
                    name: "selected_documents",
                    args: {
                      documents: chunk.data.data.input.documents,
                    },
                  },
                ],
              });
              return [...prevMessages, selectedDocumentsAIMessage];
            });
          }
        }

        if (chunk.data.event === "on_chat_model_stream") {
          if (
            chunk.data.metadata.langgraph_node === "analyze_and_route_query"
          ) {
            const message = chunk.data.data.chunk;
            const toolCallChunk = message.tool_call_chunks?.[0];
            fullRoutingStr += toolCallChunk?.args || "";
            try {
              const parsedData: { logic: string } =
                parsePartialJson(fullRoutingStr);
              if (parsedData && parsedData.logic !== "") {
                setMessages((prevMessages) => {
                  const existingMessageIndex = prevMessages.findIndex(
                    (msg) => msg.id === message.id,
                  );

                  if (existingMessageIndex !== -1) {
                    const newMessage = new AIMessage({
                      ...prevMessages[existingMessageIndex],
                      tool_calls: [
                        {
                          name: "router_logic",
                          args: parsedData,
                        },
                      ],
                    });

                    return [
                      ...prevMessages.slice(0, existingMessageIndex),
                      newMessage,
                      ...prevMessages.slice(existingMessageIndex + 1),
                    ];
                  } else {
                    const newMessage = new AIMessage({
                      ...message,
                      tool_calls: [
                        {
                          name: "router_logic",
                          args: parsedData,
                        },
                      ],
                    });
                    return [...prevMessages, newMessage];
                  }
                });
              }
            } catch (error) {
              console.error("Error parsing router logic data:", error);
            }
          }

          if (
            chunk.data.metadata.langgraph_node === "respond_to_general_query"
          ) {
            const message = chunk.data.data.chunk;
            setMessages((prevMessages) => {
              const existingMessageIndex = prevMessages.findIndex(
                (msg) => msg.id === message.id,
              );
              if (existingMessageIndex !== -1) {
                // Create a new array with the updated message
                return [
                  ...prevMessages.slice(0, existingMessageIndex),
                  new AIMessage({
                    ...prevMessages[existingMessageIndex],
                    content:
                      prevMessages[existingMessageIndex].content +
                      message.content,
                  }),
                  ...prevMessages.slice(existingMessageIndex + 1),
                ];
              } else {
                const newMessage = new AIMessage({
                  ...message,
                });
                return [...prevMessages, newMessage];
              }
            });
          }

          if (chunk.data.metadata.langgraph_node === "create_research_plan") {
            const message = chunk.data.data.chunk;
            generatingQuestionsMessageId = message.id;
            const toolCallChunk = message.tool_call_chunks?.[0];
            fullGeneratingQuestionsStr += toolCallChunk?.args || "";
            try {
              const parsedData: { steps: string[] } = parsePartialJson(
                fullGeneratingQuestionsStr,
              );
              if (parsedData && Array.isArray(parsedData.steps)) {
                setMessages((prevMessages) => {
                  const existingMessageIndex = prevMessages.findIndex(
                    (msg) => msg.id === message.id,
                  );

                  const questions = parsedData.steps
                    .map((step, index) => ({
                      step: index + 1,
                      question: step.trim(),
                    }))
                    .filter((q) => q.question !== "");

                  if (existingMessageIndex !== -1) {
                    const existingMessage = prevMessages[
                      existingMessageIndex
                    ] as AIMessage;
                    const existingToolCalls = existingMessage.tool_calls || [];

                    let updatedToolCall;
                    if (existingToolCalls[0].name === "generating_questions") {
                      // Update existing tool call
                      updatedToolCall = {
                        ...existingToolCalls[0],
                        args: {
                          questions,
                        },
                      };
                    } else {
                      // Create new tool call
                      updatedToolCall = {
                        name: "generating_questions",
                        args: { questions },
                      };
                    }

                    return [
                      ...prevMessages.slice(0, existingMessageIndex),
                      new AIMessage({
                        ...existingMessage,
                        content: "",
                        tool_calls: [updatedToolCall],
                      }),
                      ...prevMessages.slice(existingMessageIndex + 1),
                    ];
                  } else if (questions.length > 0) {
                    // Create new message with tool call
                    const newToolCall = {
                      name: "generating_questions",
                      args: { questions },
                    };

                    const newMessage = new AIMessage({
                      ...message,
                      content: "",
                      tool_calls: [newToolCall],
                    });
                    return [...prevMessages, newMessage];
                  }
                  return prevMessages;
                });
              }
            } catch (error) {
              console.error("Error parsing generating questions data:", error);
            }
          }

          if (chunk.data.metadata.langgraph_node === "respond") {
            const message = chunk.data.data.chunk;
            setMessages((prevMessages) => {
              const existingMessageIndex = prevMessages.findIndex(
                (msg) => msg.id === message.id,
              );
              if (existingMessageIndex !== -1) {
                // Create a new array with the updated message
                return [
                  ...prevMessages.slice(0, existingMessageIndex),
                  new AIMessage({
                    ...prevMessages[existingMessageIndex],
                    content:
                      prevMessages[existingMessageIndex].content +
                      message.content,
                  }),
                  ...prevMessages.slice(existingMessageIndex + 1),
                ];
              } else {
                const answerHeaderToolMsg = new AIMessage({
                  content: "",
                  tool_calls: [
                    {
                      name: "answer_header",
                      args: {},
                    },
                  ],
                });
                const newMessage = new AIMessage({
                  ...message,
                });
                return [...prevMessages, answerHeaderToolMsg, newMessage];
              }
            });
          }
        }

        if (chunk.data.event === "on_chain_end") {
          if (
            chunk.data.metadata.langgraph_node === "conduct_research" &&
            chunk.data.data?.output &&
            typeof chunk.data.data.output === "object" &&
            "question" in chunk.data.data.output
          ) {
            setMessages((prevMessages) => {
              const foundIndex = prevMessages.findIndex(
                (msg) =>
                  "tool_calls" in msg &&
                  Array.isArray(msg.tool_calls) &&
                  msg.tool_calls.length > 0 &&
                  msg.tool_calls[0].name === "generating_questions" &&
                  msg.id === generatingQuestionsMessageId,
              );

              if (foundIndex !== -1) {
                const messageToUpdate = prevMessages[foundIndex] as AIMessage;
                const updatedToolCalls = messageToUpdate.tool_calls?.map(
                  (toolCall) => {
                    if (toolCall.name === "generating_questions") {
                      const updatedQuestions = toolCall.args.questions.map(
                        (q: any) => {
                          if (q.question === chunk.data.data.output.question) {
                            return {
                              ...q,
                              queries: chunk.data.data.output.queries,
                              documents: chunk.data.data.output.documents,
                            };
                          }
                          return q;
                        },
                      );

                      return {
                        ...toolCall,
                        args: {
                          ...toolCall.args,
                          questions: updatedQuestions,
                        },
                      };
                    }
                    return toolCall;
                  },
                );

                const updatedMessage = new AIMessage({
                  ...messageToUpdate,
                  tool_calls: updatedToolCalls,
                });

                return [
                  ...prevMessages.slice(0, foundIndex),
                  updatedMessage,
                  ...prevMessages.slice(foundIndex + 1),
                ];
              }

              // Return the previous messages unchanged if no matching message found
              return prevMessages;
            });
          }

          if (
            [
              "respond",
              "respond_to_general_query",
              "ask_for_more_info",
            ].includes(chunk?.data?.metadata?.langgraph_node)
          ) {
            setMessages((prevMessages) => {
              const existingMessageIndex = prevMessages.findIndex(
                (msg) => msg.id === progressAIMessageId,
              );
              if (existingMessageIndex !== -1) {
                // Create a new array with the updated message
                return [
                  ...prevMessages.slice(0, existingMessageIndex),
                  new AIMessage({
                    id: progressAIMessageId,
                    content: "",
                    tool_calls: [
                      {
                        name: "progress",
                        args: {
                          step: 4,
                        },
                      },
                    ],
                  }),
                  ...prevMessages.slice(existingMessageIndex + 1),
                ];
              } else {
                console.warn(
                  "Progress message ID is defined but not found in messages",
                );
                return prevMessages;
              }
            });
          }

          if (chunk.data.metadata.langgraph_node === "respond") {
            const inputDocuments = chunk.data.data.input.documents;
            const message = chunk.data.data.output.messages[0];
            setMessages((prevMessages) => {
              const existingMessageIndex = prevMessages.findIndex(
                (pMsg) => pMsg.id === message.id,
              );
              if (existingMessageIndex !== -1) {
                const newMessageWithLinks = new AIMessage({
                  ...message,
                  content: addDocumentLinks(message.content, inputDocuments),
                });

                return [
                  ...prevMessages.slice(0, existingMessageIndex),
                  newMessageWithLinks,
                  ...prevMessages.slice(existingMessageIndex + 1),
                ];
              } else {
                return prevMessages;
              }
            });
          }
        }
      }
    } catch (e) {
      console.error(e);
    } finally {
      setIsStreaming(false);
    }

    if (runId) {
      // Chain `.then` to not block the stream
      shareRun(runId).then((sharedRunURL) => {
        if (sharedRunURL) {
          setMessages((prevMessages) => {
            const langSmithToolCallMessage = new AIMessage({
              content: "",
              id: uuidv4(),
              tool_calls: [
                {
                  name: "langsmith_tool_ui",
                  args: { sharedRunURL },
                  id: sharedRunURL
                    ?.split("https://smith.langchain.com/public/")[1]
                    .split("/")[0],
                },
              ],
            });
            return [...prevMessages, langSmithToolCallMessage];
          });
        }
      });
    }
  };

  const switchSelectedThread = (thread: Thread) => {
    setThreadId(thread.thread_id);
    if (!thread.values) {
      setMessages([]);
      return;
    }
    const threadValues = thread.values as Record<string, any>;

    const actualMessages = (
      threadValues.messages as Record<string, any>[]
    ).flatMap((msg, index, array) => {
      if (msg.type === "human") {
        // insert progress bar afterwards
        const progressAIMessage = new AIMessage({
          id: uuidv4(),
          content: "",
          tool_calls: [
            {
              name: "progress",
              args: {
                step: 4, // Set to done.
              },
            },
          ],
        });
        return [
          new HumanMessage({
            ...msg,
            content: msg.content,
          }),
          progressAIMessage,
        ];
      }

      if (msg.type === "ai") {
        const isLastAiMessage =
          index === array.length - 1 || array[index + 1].type === "human";
        if (isLastAiMessage) {
          const routerMessage = threadValues.router
            ? new AIMessage({
                content: "",
                id: uuidv4(),
                tool_calls: [
                  {
                    name: "router_logic",
                    args: threadValues.router,
                  },
                ],
              })
            : undefined;
          const selectedDocumentsAIMessage = threadValues.documents?.length
            ? new AIMessage({
                content: "",
                id: uuidv4(),
                tool_calls: [
                  {
                    name: "selected_documents",
                    args: {
                      documents: threadValues.documents,
                    },
                  },
                ],
              })
            : undefined;
          const answerHeaderToolMsg = new AIMessage({
            content: "",
            tool_calls: [
              {
                name: "answer_header",
                args: {},
              },
            ],
          });
          return [
            ...(routerMessage ? [routerMessage] : []),
            ...(selectedDocumentsAIMessage ? [selectedDocumentsAIMessage] : []),
            answerHeaderToolMsg,
            new AIMessage({
              ...msg,
              content: msg.content,
            }),
          ];
        }
        return new AIMessage({
          ...msg,
          content: msg.content,
        });
      }

      return []; // Return an empty array for any other message types
    });

    setMessages(actualMessages);
  };

  const contextValue: GraphContentType = {
    userData: {
      userId,
    },
    threadsData: {
      isUserThreadsLoading,
      userThreads,
      getThreadById,
      setUserThreads,
      getUserThreads,
      createThread,
      deleteThread,
    },
    graphData: {
      runId,
      isStreaming,
      messages,
      selectedModel,
      setSelectedModel,
      setMessages,
      streamMessage,
      switchSelectedThread,
    },
  };

  return (
    <GraphContext.Provider value={contextValue}>
      {children}
    </GraphContext.Provider>
  );
}

export function useGraphContext() {
  const context = useContext(GraphContext);
  if (context === undefined) {
    throw new Error("useGraphContext must be used within a GraphProvider");
  }
  return context;
}



================================================
FILE: frontend/app/contexts/utils.ts
================================================
import { Client } from "@langchain/langgraph-sdk";

export function createClient() {
  const apiUrl = process.env.NEXT_PUBLIC_API_URL ?? "http://localhost:3000/api";
  return new Client({
    apiUrl,
  });
}

export function nodeToStep(node: string) {
  switch (node) {
    case "analyze_and_route_query":
      return 0;
    case "create_research_plan":
      return 1;
    case "conduct_research":
      return 2;
    case "respond":
      return 3;
    default:
      return 0;
  }
}

export function addDocumentLinks(
  text: string,
  inputDocuments: Record<string, any>[],
): string {
  return text.replace(/\[(\d+)\]/g, (match, number) => {
    const index = parseInt(number, 10);
    if (index >= 0 && index < inputDocuments.length) {
      const document = inputDocuments[index];
      if (document && document.metadata && document.metadata.source) {
        return `[[${number}]](${document.metadata.source})`;
      }
    }
    // Return the original match if no corresponding document is found
    return match;
  });
}



================================================
FILE: frontend/app/hooks/use-toast.ts
================================================
"use client";

// Inspired by react-hot-toast library
import * as React from "react";

import type { ToastActionElement, ToastProps } from "../components/ui/toast";

const TOAST_LIMIT = 1;
const TOAST_REMOVE_DELAY = 1000000;

type ToasterToast = ToastProps & {
  id: string;
  title?: React.ReactNode;
  description?: React.ReactNode;
  action?: ToastActionElement;
};

const UNUSED_actionTypes = {
  ADD_TOAST: "ADD_TOAST",
  UPDATE_TOAST: "UPDATE_TOAST",
  DISMISS_TOAST: "DISMISS_TOAST",
  REMOVE_TOAST: "REMOVE_TOAST",
} as const;

let count = 0;

function genId() {
  count = (count + 1) % Number.MAX_SAFE_INTEGER;
  return count.toString();
}

type ActionType = typeof UNUSED_actionTypes;

type Action =
  | {
      type: ActionType["ADD_TOAST"];
      toast: ToasterToast;
    }
  | {
      type: ActionType["UPDATE_TOAST"];
      toast: Partial<ToasterToast>;
    }
  | {
      type: ActionType["DISMISS_TOAST"];
      toastId?: ToasterToast["id"];
    }
  | {
      type: ActionType["REMOVE_TOAST"];
      toastId?: ToasterToast["id"];
    };

interface State {
  toasts: ToasterToast[];
}

const toastTimeouts = new Map<string, ReturnType<typeof setTimeout>>();

const addToRemoveQueue = (toastId: string) => {
  if (toastTimeouts.has(toastId)) {
    return;
  }

  const timeout = setTimeout(() => {
    toastTimeouts.delete(toastId);
    dispatch({
      type: "REMOVE_TOAST",
      toastId: toastId,
    });
  }, TOAST_REMOVE_DELAY);

  toastTimeouts.set(toastId, timeout);
};

export const reducer = (state: State, action: Action): State => {
  switch (action.type) {
    case "ADD_TOAST":
      return {
        ...state,
        toasts: [action.toast, ...state.toasts].slice(0, TOAST_LIMIT),
      };

    case "UPDATE_TOAST":
      return {
        ...state,
        toasts: state.toasts.map((t) =>
          t.id === action.toast.id ? { ...t, ...action.toast } : t,
        ),
      };

    case "DISMISS_TOAST": {
      const { toastId } = action;

      // ! Side effects ! - This could be extracted into a dismissToast() action,
      // but I'll keep it here for simplicity
      if (toastId) {
        addToRemoveQueue(toastId);
      } else {
        state.toasts.forEach((toast) => {
          addToRemoveQueue(toast.id);
        });
      }

      return {
        ...state,
        toasts: state.toasts.map((t) =>
          t.id === toastId || toastId === undefined
            ? {
                ...t,
                open: false,
              }
            : t,
        ),
      };
    }
    case "REMOVE_TOAST":
      if (action.toastId === undefined) {
        return {
          ...state,
          toasts: [],
        };
      }
      return {
        ...state,
        toasts: state.toasts.filter((t) => t.id !== action.toastId),
      };
  }
};

const listeners: Array<(state: State) => void> = [];

let memoryState: State = { toasts: [] };

function dispatch(action: Action) {
  memoryState = reducer(memoryState, action);
  listeners.forEach((listener) => {
    listener(memoryState);
  });
}

type Toast = Omit<ToasterToast, "id">;

function toast({ ...props }: Toast) {
  const id = genId();

  const update = (props: ToasterToast) =>
    dispatch({
      type: "UPDATE_TOAST",
      toast: { ...props, id },
    });
  const dismiss = () => dispatch({ type: "DISMISS_TOAST", toastId: id });

  dispatch({
    type: "ADD_TOAST",
    toast: {
      ...props,
      id,
      open: true,
      onOpenChange: (open: boolean) => {
        if (!open) dismiss();
      },
    },
  });

  return {
    id: id,
    dismiss,
    update,
  };
}

function useToast() {
  const [state, setState] = React.useState<State>(memoryState);

  React.useEffect(() => {
    listeners.push(setState);
    return () => {
      const index = listeners.indexOf(setState);
      if (index > -1) {
        listeners.splice(index, 1);
      }
    };
  }, [state]);

  return {
    ...state,
    toast,
    dismiss: (toastId?: string) => dispatch({ type: "DISMISS_TOAST", toastId }),
  };
}

export { useToast, toast };



================================================
FILE: frontend/app/hooks/useRuns.tsx
================================================
import { useCallback } from "react";
import { Feedback } from "langsmith";

export interface FeedbackResponse {
  success: boolean;
  feedback: Feedback;
}

export function useRuns() {
  /**
   * Generates a public shared run ID for the given run ID.
   */
  const shareRun = async (runId: string): Promise<string | undefined> => {
    const res = await fetch("/api/runs/share", {
      method: "POST",
      body: JSON.stringify({ runId }),
      headers: {
        "Content-Type": "application/json",
      },
    });

    if (!res.ok) {
      return;
    }

    const { sharedRunURL } = await res.json();
    return sharedRunURL;
  };

  const sendFeedback = useCallback(
    async (
      runId: string,
      feedbackKey: string,
      score: number,
      comment?: string,
    ): Promise<FeedbackResponse | undefined> => {
      try {
        const res = await fetch("/api/runs/feedback", {
          method: "POST",
          body: JSON.stringify({ runId, feedbackKey, score, comment }),
          headers: {
            "Content-Type": "application/json",
          },
        });

        if (!res.ok) {
          return;
        }

        return (await res.json()) as FeedbackResponse;
      } catch (error) {
        console.error("Error sending feedback:", error);
        return;
      }
    },
    [],
  );

  return {
    shareRun,
    sendFeedback,
  };
}



================================================
FILE: frontend/app/hooks/useThreads.tsx
================================================
"use client";

import { useEffect, useState } from "react";

import { Client, Thread } from "@langchain/langgraph-sdk";
import { useToast } from "./use-toast";
import { useQueryState } from "nuqs";

export const createClient = () => {
  const apiUrl = process.env.NEXT_PUBLIC_API_URL ?? "http://localhost:3000/api";
  return new Client({
    apiUrl,
  });
};

export function useThreads(userId: string | undefined) {
  const { toast } = useToast();
  const [isUserThreadsLoading, setIsUserThreadsLoading] = useState(false);
  const [userThreads, setUserThreads] = useState<Thread[]>([]);
  const [threadId, setThreadId] = useQueryState("threadId");

  useEffect(() => {
    if (typeof window == "undefined" || !userId) return;
    getUserThreads(userId);
  }, [userId]);

  const createThread = async (id: string) => {
    const client = createClient();
    let thread;
    try {
      thread = await client.threads.create({
        metadata: {
          user_id: id,
        },
      });
      if (!thread || !thread.thread_id) {
        throw new Error("Thread creation failed.");
      }
      setThreadId(thread.thread_id);
    } catch (e) {
      console.error("Error creating thread", e);
      toast({
        title: "Error creating thread.",
      });
    }
    return thread;
  };

  const getUserThreads = async (id: string) => {
    setIsUserThreadsLoading(true);
    try {
      const client = createClient();

      const userThreads = (await client.threads.search({
        metadata: {
          user_id: id,
        },
        limit: 100,
      })) as Awaited<Thread[]>;

      if (userThreads.length > 0) {
        const lastInArray = userThreads[0];
        const allButLast = userThreads.slice(1, userThreads.length);
        const filteredThreads = allButLast.filter(
          (thread) => thread.values && Object.keys(thread.values).length > 0,
        );
        setUserThreads([...filteredThreads, lastInArray]);
      }
    } finally {
      setIsUserThreadsLoading(false);
    }
  };

  const getThreadById = async (id: string) => {
    const client = createClient();
    return (await client.threads.get(id)) as Awaited<Thread>;
  };

  const deleteThread = async (id: string, clearMessages: () => void) => {
    if (!userId) {
      throw new Error("User ID not found");
    }
    setUserThreads((prevThreads) => {
      const newThreads = prevThreads.filter(
        (thread) => thread.thread_id !== id,
      );
      return newThreads;
    });
    const client = createClient();
    await client.threads.delete(id);
    if (id === threadId) {
      // Remove the threadID from query params, and refetch threads to
      // update the sidebar UI.
      clearMessages();
      getUserThreads(userId);
      setThreadId(null);
    }
  };

  return {
    isUserThreadsLoading,
    userThreads,
    getThreadById,
    setUserThreads,
    getUserThreads,
    createThread,
    deleteThread,
  };
}



================================================
FILE: frontend/app/hooks/useUser.tsx
================================================
import { useEffect, useState } from "react";
import { v4 as uuidv4 } from "uuid";
import { getCookie, setCookie } from "../utils/cookies";
import { USER_ID_COOKIE_NAME } from "../utils/constants";

export function useUser() {
  const [userId, setUserId] = useState<string>();

  useEffect(() => {
    if (userId) return;

    const userIdCookie = getCookie(USER_ID_COOKIE_NAME);
    if (userIdCookie) {
      setUserId(userIdCookie);
    } else {
      const newUserId = uuidv4();
      setUserId(newUserId);
      setCookie(USER_ID_COOKIE_NAME, newUserId);
    }
  }, []);

  return {
    userId,
  };
}



================================================
FILE: frontend/app/utils/cn.ts
================================================
import { clsx, type ClassValue } from "clsx";
import { twMerge } from "tailwind-merge";

export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs));
}



================================================
FILE: frontend/app/utils/constants.tsx
================================================
export const RESPONSE_FEEDBACK_KEY = "user_score";
export const SOURCE_CLICK_KEY = "user_click";
export const THREAD_ID_COOKIE_NAME = "clc_thread_id_v3";
export const USER_ID_COOKIE_NAME = "clc_user_id_v3";



================================================
FILE: frontend/app/utils/convert_messages.ts
================================================
import {
  useExternalMessageConverter,
  ThreadMessageLike,
  ToolCallContentPart,
} from "@assistant-ui/react";
import { AIMessage, BaseMessage, ToolMessage } from "@langchain/core/messages";

// Not exposed by `@assistant-ui/react` package, but is
// the required return type for this callback function.
type Message =
  | ThreadMessageLike
  | {
      role: "tool";
      toolCallId: string;
      toolName?: string | undefined;
      result: any;
    };

export const convertLangchainMessages: useExternalMessageConverter.Callback<
  BaseMessage
> = (message): Message | Message[] => {
  if (typeof message.content !== "string") {
    throw new Error("Only text messages are supported");
  }

  switch (message._getType()) {
    case "system":
      return {
        role: "system",
        id: message.id,
        content: [{ type: "text", text: message.content }],
      };
    case "human":
      return {
        role: "user",
        id: message.id,
        content: [{ type: "text", text: message.content }],
      };
    case "ai":
      const aiMsg = message as AIMessage;
      const toolCallsContent: ToolCallContentPart[] = aiMsg.tool_calls?.length
        ? aiMsg.tool_calls.map((tc) => ({
            type: "tool-call" as const,
            toolCallId: tc.id ?? "",
            toolName: tc.name,
            args: tc.args,
            argsText: JSON.stringify(tc.args),
          }))
        : [];
      return {
        role: "assistant",
        id: message.id,
        content: [
          ...toolCallsContent,
          {
            type: "text",
            text: message.content,
          },
        ],
      };
    case "tool":
      return {
        role: "tool",
        toolName: message.name,
        toolCallId: (message as ToolMessage).tool_call_id,
        result: message.content,
      };
    default:
      throw new Error(`Unsupported message type: ${message._getType()}`);
  }
};

export function convertToOpenAIFormat(message: BaseMessage) {
  if (typeof message.content !== "string") {
    throw new Error("Only text messages are supported");
  }
  switch (message._getType()) {
    case "system":
      return {
        role: "system",
        content: message.content,
      };
    case "human":
      return {
        role: "user",
        content: message.content,
      };
    case "ai":
      return {
        role: "assistant",
        content: message.content,
      };
    case "tool":
      return {
        role: "tool",
        toolName: message.name,
        result: message.content,
      };
    default:
      throw new Error(`Unsupported message type: ${message._getType()}`);
  }
}



================================================
FILE: frontend/app/utils/cookies.ts
================================================
import Cookies from "js-cookie";

export const getCookie = (name: string): string | undefined => {
  if (typeof window === "undefined") {
    return undefined;
  }
  return Cookies.get(name);
};

export const setCookie = (
  name: string,
  value: string,
  options?: Cookies.CookieAttributes,
): void => {
  if (typeof window === "undefined") {
    return;
  }
  Cookies.set(name, value, {
    expires: 365, // Default to 1 year expiration
    ...(options || {}),
  });
};



================================================
FILE: frontend/app/utils/dummy.ts
================================================
type DummyThread = {
  id: string;
  onClick: (id: string) => void;
  label: string;
  createdAt: Date;
};

export const dummyThreads: DummyThread[] = [
  {
    id: "1",
    label: "How can I use LangChain to build a question-answering system?",
    onClick: () => {},
    createdAt: new Date("2024-10-16T12:00:00.000Z"), // Today
  },
  {
    id: "2",
    label: "What are the different types of chains available in LangChain?",
    onClick: () => {},
    createdAt: new Date("2024-10-16T10:00:00.000Z"), // Today
  },
  {
    id: "3",
    label: "Can I use LangChain to summarize text documents?",
    onClick: () => {},
    createdAt: new Date("2024-10-16T08:00:00.000Z"), // Today
  },
  {
    id: "4",
    label: "How do I integrate LangChain with a database?",
    onClick: () => {},
    createdAt: new Date("2024-10-15T18:00:00.000Z"), // Yesterday
  },
  {
    id: "5",
    label: "What are the best practices for using LangChain?",
    onClick: () => {},
    createdAt: new Date("2024-10-15T16:00:00.000Z"), // Yesterday
  },
  {
    id: "6",
    label: "Can I use LangChain to generate creative content?",
    onClick: () => {},
    createdAt: new Date("2024-10-15T14:00:00.000Z"), // Yesterday
  },
  {
    id: "7",
    label: "How can I fine-tune a LangChain model for a specific task?",
    onClick: () => {},
    createdAt: new Date("2024-10-10T12:00:00.000Z"), // Previous Week
  },
  {
    id: "8",
    label: "What are the limitations of LangChain?",
    onClick: () => {},
    createdAt: new Date("2024-10-09T10:00:00.000Z"), // Previous Week
  },
  {
    id: "9",
    label: "How can I use LangChain to build a chatbot?",
    onClick: () => {},
    createdAt: new Date("2024-10-08T08:00:00.000Z"), // Previous Week
  },
  {
    id: "10",
    label: "What are the different ways to use LangChain with OpenAI models?",
    onClick: () => {},
    createdAt: new Date("2024-10-03T18:00:00.000Z"), // Older
  },
  {
    id: "11",
    label: "How can I use LangChain to translate text?",
    onClick: () => {},
    createdAt: new Date("2024-10-02T16:00:00.000Z"), // Older
  },
  {
    id: "12",
    label: "What are the ethical considerations of using LangChain?",
    onClick: () => {},
    createdAt: new Date("2024-10-01T14:00:00.000Z"), // Older
  },
  {
    id: "13",
    label: "How can I use LangChain to build a knowledge graph?",
    onClick: () => {},
    createdAt: new Date("2024-09-28T12:00:00.000Z"), // Older
  },
  {
    id: "14",
    label:
      "What are the different ways to use LangChain with Google AI models?",
    onClick: () => {},
    createdAt: new Date("2024-09-27T10:00:00.000Z"), // Older
  },
  {
    id: "15",
    label: "How can I use LangChain to build a document summarizer?",
    onClick: () => {},
    createdAt: new Date("2024-09-26T08:00:00.000Z"), // Older
  },
  {
    id: "16",
    label:
      "What are the different ways to use LangChain with Hugging Face models?",
    onClick: () => {},
    createdAt: new Date("2024-09-25T18:00:00.000Z"), // Older
  },
  {
    id: "17",
    label: "How can I use LangChain to build a text classifier?",
    onClick: () => {},
    createdAt: new Date("2024-09-24T16:00:00.000Z"), // Older
  },
  {
    id: "18",
    label: "What are the different ways to use LangChain with custom models?",
    onClick: () => {},
    createdAt: new Date("2024-09-23T14:00:00.000Z"), // Older
  },
  {
    id: "19",
    label: "How can I use LangChain to build a code generator?",
    onClick: () => {},
    createdAt: new Date("2024-09-22T12:00:00.000Z"), // Older
  },
  {
    id: "20",
    label: "What are the different ways to use LangChain with other libraries?",
    onClick: () => {},
    createdAt: new Date("2024-09-21T10:00:00.000Z"), // Older
  },
  {
    id: "21",
    label: "How can I use LangChain to build a data analysis tool?",
    onClick: () => {},
    createdAt: new Date("2024-09-20T08:00:00.000Z"), // Older
  },
  {
    id: "22",
    label: "What are the different ways to use LangChain with cloud services?",
    onClick: () => {},
    createdAt: new Date("2024-09-19T18:00:00.000Z"), // Older
  },
  {
    id: "23",
    label:
      "How can I use LangChain to build a personalized recommendation system?",
    onClick: () => {},
    createdAt: new Date("2024-09-18T16:00:00.000Z"), // Older
  },
  {
    id: "24",
    label: "What are the different ways to use LangChain with web APIs?",
    onClick: () => {},
    createdAt: new Date("2024-09-17T14:00:00.000Z"), // Older
  },
  {
    id: "25",
    label: "How can I use LangChain to build a search engine?",
    onClick: () => {},
    createdAt: new Date("2024-09-16T12:00:00.000Z"), // Older
  },
];



================================================
FILE: terraform/backend.tf
================================================
terraform {
  backend "gcs" {
    bucket = "YOUR BUCKET"
    prefix = "YOUR PREFIX"
  }
}



================================================
FILE: terraform/main.tf
================================================
locals {
  secret_json = jsondecode(data.google_secret_manager_secret_version.chat_langchain_backend_secrets.secret_data)
  region      = "YOUR REGION"
  project_id  = "YOUR PROJECT ID"
}

provider "google" {
  project = local.project_id
  region  = local.region
}

# Load secrets from Secret Manager. You can specify your secrets in anyway you see fit.
data "google_secret_manager_secret_version" "chat_langchain_backend_secrets" {
  secret = "chat-langchain-backend"
}

module "chat_langchain_backend" {
  source = "./modules/chat_langchain_backend"

  project_id                  = local.project_id
  region                      = local.region
  chat_langchain_backend_name = "chat-langchain-backend"
  domain_name                 = "YOUR DOMAIN NAME"
  image_tag                   = "docker.io/langchain/chat-langchain-backend:0.0.1"
  openai_api_key              = local.secret_json["openai_api_key"]
  weaviate_api_key            = local.secret_json["weaviate_api_key"]
  weaviate_url                = local.secret_json["weaviate_url"]
  langsmith_api_key           = local.secret_json["langsmith_api_key"]
}



================================================
FILE: terraform/modules/chat_langchain_backend/main.tf
================================================
# Common environment variables
locals {
  voyager_vars = var.voyage_ai_model != "" && var.voyage_api_key != "" ? {
    VOYAGE_AI_MODEL = var.voyage_ai_model
    VOYAGE_API_KEY  = var.voyage_api_key
  } : {}
  env_vars = merge(local.voyager_vars, {
    OPENAI_API_KEY       = var.openai_api_key
    WEAVIATE_URL         = var.weaviate_url
    WEAVIATE_API_KEY     = var.weaviate_api_key
    LANGCHAIN_TRACING_V2 = true
    LANGCHAIN_API_KEY    = var.langsmith_api_key
    LANGCHAIN_PROJECT    = var.langchain_project
    FIREWORKS_API_KEY    = var.fireworks_api_key
    ANTHROPIC_API_KEY    = var.anthropic_api_key
    }, var.env_vars
  )
}

# No auth policy since auth is handled by Supabase or internal API Key auth
data "google_iam_policy" "noauth" {
  binding {
    role = "roles/run.invoker"

    members = [
      "allUsers",
    ]
  }
}

# Web service
resource "google_cloud_run_v2_service" "chat_langchain_backend" {
  name     = var.chat_langchain_backend_name
  location = var.region
  ingress  = "INGRESS_TRAFFIC_INTERNAL_LOAD_BALANCER"

  template {
    max_instance_request_concurrency = var.max_instance_request_concurrency
    scaling {
      min_instance_count = var.min_instance_count
      max_instance_count = var.max_instance_count
    }
    volumes {
      name = "cloudsql"
      cloud_sql_instance {
        instances = [var.cloudsql_instance_name]
      }
    }
    containers {
      image = var.image_tag

      dynamic "env" {
        for_each = local.env_vars
        content {
          name  = env.key
          value = env.value
        }
      }
      resources {
        limits = {
          cpu    = 2
          memory = "4Gi"
        }
        startup_cpu_boost = true
        cpu_idle          = false
      }
      volume_mounts {
        name       = "cloudsql"
        mount_path = "/cloudsql"
      }
    }
  }

  project = var.project_id
}

resource "google_cloud_run_v2_service_iam_policy" "web_noauth" {
  location    = google_cloud_run_v2_service.chat_langchain_backend.location
  project     = google_cloud_run_v2_service.chat_langchain_backend.project
  name        = google_cloud_run_v2_service.chat_langchain_backend.name
  policy_data = data.google_iam_policy.noauth.policy_data
}

resource "google_compute_region_network_endpoint_group" "web_serverless_neg" {
  provider              = google-beta
  project               = var.project_id
  name                  = "hub-serverless-neg-web"
  network_endpoint_type = "SERVERLESS"
  region                = var.region
  cloud_run {
    service = google_cloud_run_v2_service.chat_langchain_backend.name
  }
}

resource "google_compute_security_policy" "hub_web_noauth_lb_http" {
  name        = "hub-web-noauth-lb-http-authorization-throttle"
  project     = var.project_id
  description = "Web Security Policy"

  rule {
    action      = "throttle"
    description = "IP Address Throttle"
    priority    = "2147483647"

    match {
      config {
        src_ip_ranges = ["*"]
      }
      versioned_expr = "SRC_IPS_V1"
    }

    rate_limit_options {
      conform_action = "allow"
      exceed_action  = "deny(429)"

      enforce_on_key = "IP"

      rate_limit_threshold {
        count        = 5000
        interval_sec = 60
      }
    }

    preview = false
  }
}

module "lb-http-web" {
  source  = "GoogleCloudPlatform/lb-http/google//modules/serverless_negs"
  version = "9.0"
  name    = "hub-web-lb-http"
  project = var.project_id

  ssl                             = true
  managed_ssl_certificate_domains = [var.domain_name]
  https_redirect                  = true
  create_address                  = true

  backends = {
    default = {
      description = null
      groups = [
        {
          group = google_compute_region_network_endpoint_group.web_serverless_neg.id
        }
      ]
      enable_cdn              = false
      edge_security_policy    = null
      security_policy         = google_compute_security_policy.hub_web_noauth_lb_http.id
      custom_request_headers  = null
      custom_response_headers = null

      iap_config = {
        enable               = false
        oauth2_client_id     = ""
        oauth2_client_secret = ""
      }
      log_config = {
        enable      = false
        sample_rate = null
      }
      protocol         = null
      port_name        = null
      compression_mode = null
    }
  }
}



================================================
FILE: terraform/modules/chat_langchain_backend/variables.tf
================================================
variable "chat_langchain_backend_name" {
  description = "Name to use for resources that will be created"
  type        = string
}

variable "project_id" {
  description = "The ID of the project"
  type        = string
}

variable "region" {
  description = "The region to deploy to"
  type        = string
}

variable "image_tag" {
  description = "The tag of the Chat Langchain Docker image to deploy"
  type        = string
}

variable "domain_name" {
  description = "The domain name for the backend"
  type        = string
}

variable "openai_api_key" {
  description = "Openai api key to use for the backend"
  type        = string
}

variable "weaviate_url" {
  description = "Weaviate url to use for the backend"
  type        = string
}

variable "weaviate_api_key" {
  description = "Weaviate api key to use for the backend"
  type        = string
}

variable "voyage_ai_model" {
  description = "Voyage AI model to use for the backend"
  type        = string
  default     = ""
}

variable "voyage_api_key" {
  description = "Voyage API key url to use for the backend"
  type        = string
  default     = ""
}

variable "langsmith_api_key" {
  description = "Langsmith api key to use for the backend"
  type        = string
}

variable "langchain_project" {
  description = "Langchain project to use for the backend"
  type        = string
  default     = "chat-langchain"
}

variable "min_instance_count" {
  description = "Minimum number of instances to run"
  type        = number
  default     = 1
}

variable "max_instance_count" {
  description = "Maximum number of instances to run"
  type        = number
  default     = 50
}

variable "max_instance_request_concurrency" {
  description = "Maximum number of requests to process concurrently per instance"
  type        = number
  default     = 50
}

variable "fireworks_api_key" {
  description = "Fireworks api key to use for the backend"
  type        = string
  default     = ""
}

variable "anthropic_api_key" {
  description = "Anthropic api key to use for the backend"
  type        = string
  default     = ""
}

variable "env_vars" {
  description = "Environment variables to set on the backend"
  type        = map(string)
  default     = {}
}

variable "cloudsql_instance_name" {
  description = "The name of the Cloud SQL instance to connect to"
  type        = string
}



================================================
FILE: .github/dependabot.yml
================================================
# Please see the documentation for all configuration options:
# https://docs.github.com/github/administering-a-repository/configuration-options-for-dependency-updates
# and
# https://docs.github.com/code-security/dependabot/dependabot-version-updates/configuration-options-for-the-dependabot.yml-file

version: 2
updates:
  - package-ecosystem: "github-actions"
    directory: "/"
    schedule:
      interval: "weekly"



================================================
FILE: .github/actions/poetry_setup/action.yml
================================================
# An action for setting up poetry install with caching.
# Using a custom action since the default action does not
# take poetry install groups into account.
# Action code from:
# https://github.com/actions/setup-python/issues/505#issuecomment-1273013236
name: poetry-install-with-caching
description: Poetry install with support for caching of dependency groups.

inputs:
  python-version:
    description: Python version, supporting MAJOR.MINOR only
    required: true

  poetry-version:
    description: Poetry version
    required: true

  cache-key:
    description: Cache key to use for manual handling of caching
    required: true

runs:
  using: composite
  steps:
    - uses: actions/setup-python@v5
      name: Setup python ${{ inputs.python-version }}
      id: setup-python
      with:
        python-version: ${{ inputs.python-version }}

    - uses: actions/cache@v3
      id: cache-bin-poetry
      name: Cache Poetry binary - Python ${{ inputs.python-version }}
      env:
        SEGMENT_DOWNLOAD_TIMEOUT_MIN: "1"
      with:
        path: |
          /opt/pipx/venvs/poetry
        # This step caches the poetry installation, so make sure it's keyed on the poetry version as well.
        key: bin-poetry-${{ runner.os }}-${{ runner.arch }}-py-${{ inputs.python-version }}-${{ inputs.poetry-version }}

    - name: Refresh shell hashtable and fixup softlinks
      if: steps.cache-bin-poetry.outputs.cache-hit == 'true'
      shell: bash
      env:
        POETRY_VERSION: ${{ inputs.poetry-version }}
        PYTHON_VERSION: ${{ inputs.python-version }}
      run: |
        set -eux

        # Refresh the shell hashtable, to ensure correct `which` output.
        hash -r

        # `actions/cache@v3` doesn't always seem able to correctly unpack softlinks.
        # Delete and recreate the softlinks pipx expects to have.
        rm /opt/pipx/venvs/poetry/bin/python
        cd /opt/pipx/venvs/poetry/bin
        ln -s "$(which "python$PYTHON_VERSION")" python
        chmod +x python
        cd /opt/pipx_bin/
        ln -s /opt/pipx/venvs/poetry/bin/poetry poetry
        chmod +x poetry

        # Ensure everything got set up correctly.
        /opt/pipx/venvs/poetry/bin/python --version
        /opt/pipx_bin/poetry --version

    - name: Install poetry
      if: steps.cache-bin-poetry.outputs.cache-hit != 'true'
      shell: bash
      env:
        POETRY_VERSION: ${{ inputs.poetry-version }}
        PYTHON_VERSION: ${{ inputs.python-version }}
      # Install poetry using the python version installed by setup-python step.
      run: pipx install "poetry==$POETRY_VERSION" --python '${{ steps.setup-python.outputs.python-path }}' --verbose

    - name: Restore pip and poetry cached dependencies
      uses: actions/cache@v3
      env:
        SEGMENT_DOWNLOAD_TIMEOUT_MIN: "4"
      with:
        path: |
          ~/.cache/pip
          ~/.cache/pypoetry/virtualenvs
          ~/.cache/pypoetry/cache
          ~/.cache/pypoetry/artifacts
          ./.venv
        key: py-deps-${{ runner.os }}-${{ runner.arch }}-py-${{ inputs.python-version }}-poetry-${{ inputs.poetry-version }}-${{ inputs.cache-key }}-${{ hashFiles('./poetry.lock') }}



================================================
FILE: .github/workflows/clear-and-update-index.yml
================================================
name: Clear and update index

on:
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest
    environment: Indexing
    steps:
      - uses: actions/checkout@v3

      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Poetry
        uses: snok/install-poetry@v1

      - name: Install dependencies
        run: poetry install

      - name: Clear index
        run: poetry run python _scripts/clear_index.py
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          WEAVIATE_URL: ${{ secrets.WEAVIATE_URL }}
          WEAVIATE_API_KEY: ${{ secrets.WEAVIATE_API_KEY }}
          RECORD_MANAGER_DB_URL: ${{ secrets.RECORD_MANAGER_DB_URL }}

      - name: Ingest docs
        run: poetry run python backend/ingest.py
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          WEAVIATE_URL: ${{ secrets.WEAVIATE_URL }}
          WEAVIATE_API_KEY: ${{ secrets.WEAVIATE_API_KEY }}
          RECORD_MANAGER_DB_URL: ${{ secrets.RECORD_MANAGER_DB_URL }}
          VOYAGE_API_KEY: ${{ secrets.VOYAGE_API_KEY }}
          VOYAGE_AI_MODEL: ${{ secrets.VOYAGE_AI_MODEL }}
          VOYAGE_AI_URL: ${{ secrets.VOYAGE_AI_URL }}



================================================
FILE: .github/workflows/deploy-cloud-run.yaml
================================================
name: Build, Push, and Deploy Chat Langchain

on:
  push:
    branches: [master]
  workflow_dispatch:

jobs:
  deploy-frontend:
    name: Deploy Frontend to Vercel
    runs-on: ubuntu-latest
    environment: Production
    env:
      VERCEL_ORG_ID: ${{ secrets.VERCEL_ORG_ID }}
      VERCEL_PROJECT_ID: ${{ secrets.VERCEL_PROJECT_ID }}
    steps:
      - uses: actions/checkout@v2
      - name: Install Vercel CLI
        run: npm install --global vercel@latest
      - name: Pull Vercel Environment Information
        run: vercel pull --yes --environment=production --token=${{ secrets.VERCEL_TOKEN }}
      - name: Build Project Artifacts
        run: vercel build --prod --token=${{ secrets.VERCEL_TOKEN }}
      - name: Deploy Project Artifacts to Vercel
        run: vercel deploy --prebuilt --prod --token=${{ secrets.VERCEL_TOKEN }}



================================================
FILE: .github/workflows/eval.yml
================================================
name: Eval

on:
  push:
    branches:
      - master
  pull_request:
    branches:
      - master
  workflow_dispatch:

concurrency:
  group: eval-${{ github.ref }}
  cancel-in-progress: true

jobs:
  run_eval:
    runs-on: ubuntu-latest
    environment: Evaluation
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python + Poetry
        uses: "./.github/actions/poetry_setup"
        with:
          python-version: "3.11"
          poetry-version: "1.7.1"
          cache-key: eval

      - name: Install dependencies
        run: poetry install --with dev

      - name: Evaluate
        env:
          LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          WEAVIATE_URL: ${{ secrets.WEAVIATE_URL }}
          WEAVIATE_API_KEY: ${{ secrets.WEAVIATE_API_KEY }}
        run: poetry run pytest backend/tests/evals


================================================
FILE: .github/workflows/lint.yml
================================================
name: lint

on:
  push:
    branches: [master]
  pull_request:

env:
  POETRY_VERSION: "1.7.1"

  # This env var allows us to get inline annotations when ruff has complaints.
  RUFF_OUTPUT_FORMAT: github

jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        # Only lint on the min and max supported Python versions.
        # It's extremely unlikely that there's a lint issue on any version in between
        # that doesn't show up on the min or max versions.
        #
        # GitHub rate-limits how many jobs can be running at any one time.
        # Starting new jobs is also relatively slow,
        # so linting on fewer versions makes CI faster.
        python-version:
          - "3.11"
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }} + Poetry ${{ env.POETRY_VERSION }}
        uses: "./.github/actions/poetry_setup"
        with:
          python-version: ${{ matrix.python-version }}
          poetry-version: ${{ env.POETRY_VERSION }}
          cache-key: lint

      - name: Check Poetry File
        shell: bash
        run: poetry check

      - name: Check lock file
        shell: bash
        run: poetry lock --check

      - name: Install dependencies
        run: poetry install --with dev

      - name: Get .mypy_cache to speed up mypy
        uses: actions/cache@v3
        env:
          SEGMENT_DOWNLOAD_TIMEOUT_MIN: "2"
        with:
          path: |
            ./.mypy_cache
          key: mypy-lint-${{ runner.os }}-${{ runner.arch }}-py${{ matrix.python-version }}-${{ hashFiles('./poetry.lock') }}

      - name: Analysing the code with our lint
        run: |
          make lint


================================================
FILE: .github/workflows/update-index.yml
================================================
name: Update index

on:
  workflow_dispatch:
    inputs:
      force_update:
        description: 'Whether to overwrite documents found in the record manager'
        required: false
        default: false
        type: boolean
  schedule:
    - cron:  '0 13 * * *'

jobs:
  build:
    runs-on: ubuntu-latest
    environment: Indexing
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install Poetry
        uses: snok/install-poetry@v1
      - name: Install dependencies
        run: poetry install
      - name: Ingest docs
        run: poetry run python backend/ingest.py
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          WEAVIATE_URL: ${{ secrets.WEAVIATE_URL }}
          WEAVIATE_API_KEY: ${{ secrets.WEAVIATE_API_KEY }}
          RECORD_MANAGER_DB_URL: ${{ secrets.RECORD_MANAGER_DB_URL }}
          VOYAGE_AI_MODEL: ${{ secrets.VOYAGE_AI_MODEL }}
          VOYAGE_AI_URL: ${{ secrets.VOYAGE_AI_URL }}
          VOYAGE_API_KEY: ${{ secrets.VOYAGE_API_KEY }}
          FORCE_UPDATE: ${{ github.event.inputs.force_update }}


